{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUd3jJg_egjk",
        "outputId": "a3588f7e-728a-46a4-9feb-2f16f0ead794"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from itertools import chain\n",
        "import language_tool_python\n",
        "from argparse import Namespace\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (\n",
        "    AutoConfig, \n",
        "    AutoTokenizer, \n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForMaskedLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "import inspect\n",
        "\n",
        "from transformers.generation_utils import BeamSearchScorer\n",
        "from transformers import LogitsProcessorList, StoppingCriteriaList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXWXvwRJjpBA"
      },
      "source": [
        "## Blended_Skill_Talk Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "7170283bda7045e283e036ccaeffdcf2",
            "004738fd62854848a7676e3100693a1b",
            "df68b3c7f37d4821bff89b0e1cd082bb",
            "1be25e0afa9c405fa3b7089fe6aa6ba7",
            "7c8a70a52cb64d8eb765ee4cd5df30ec",
            "d37c18f11be549a8834c745a960807f3",
            "fe9301ea70fc4a6691af00b5d43321f4",
            "ed5f96869232484282b47280a9f0639c",
            "36d9fd106ff240c8b0fb971e5c8efb3b",
            "6d8e725392444308b0c8233f62d9ec64",
            "3c2deb172de24560b57a5a548acfe53b"
          ]
        },
        "id": "qIXJ_oTvGRcB",
        "outputId": "829e202a-f7cc-4849-c194-50ae9afa91e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset blended_skill_talk (/home/monkey/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/8544e13cbbf2fb9b34157f2e2f28c1539e4f36bf0ef2bd96edd138b4000c5ca1)\n",
            "\n",
            "100%|██████████| 3/3 [00:00<00:00, 861.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 4819\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 1009\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 980\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "train_dataset = bst_dataset['train']\n",
        "eval_dataset = bst_dataset['validation']\n",
        "test_dataset = bst_dataset['test']\n",
        "print(bst_dataset)\n",
        "# print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOTNHI1jq5W1",
        "outputId": "88cba86e-ddf9-4eea-ee9b-9a2cdafabdb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#pairs of training dialogues: 27018, validation dialogues: 5651, test dialogues: 5482\n"
          ]
        }
      ],
      "source": [
        "# Get statistics of pair of dialogues \n",
        "train_num, eval_num, test_num = 0, 0, 0\n",
        "for i, instance in enumerate(train_dataset):\n",
        "    train_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(eval_dataset):\n",
        "    eval_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(test_dataset):\n",
        "    test_num += len(instance['free_messages'])\n",
        "\n",
        "print(\"#pairs of training dialogues: {}, validation dialogues: {}, test dialogues: {}\".format(\n",
        "    train_num, eval_num, test_num,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xnYGvfspczn",
        "outputId": "f17f5947-5fb2-4902-96e1-021efe8dfc0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "personas: ['i hate talking to people.', 'i believe dragons are real.']\n",
            "additional_context: Social anxiety\n",
            "previous_utterance: ['Wow, I am never shy. Do you have anxiety?', \"Yes. I end up sweating and blushing and feel like i'm going to throw up.\"]\n",
            "context: wizard_of_wikipedia\n",
            "free_messages: ['and why is that?', 'interesting but I know how you feel especially the whole people telling that it in your head ', \"Dang that's though. But I also understand that. I have people some who talks behind my back because of certain things that I believe in \"]\n",
            "guided_messages: [\"I think it's because in my head, I think everyone is judging me. I just start to sweat and I get sick in my stomach.\", \"I don't really have people telling me in my head, more like behind my back\", 'Me too! What do you believe in? I believe in dragons... Just finished watching Game of Thrones. Man, those things are dope']\n",
            "suggestions: {'convai2': [\"i've no idea i am also very shy\", 'oh i know . i always feel judged and never know what to do .', 'i try to do stuff like that all time but i can never speak up for myself'], 'empathetic_dialogues': ['Probably because I am insecure.', \"Please don't care about those people. Try to be true to yourself. World will recognize you one day.\", 'I am not sure I believe them most of the time though'], 'wizard_of_wikipedia': [\"I think it's because in my head, I think everyone is judging me. I just start to sweat and I get sick in my stomach.\", 'Right! But the difference with me is I just think about myself and no one else really. People consider it a social and cultural problem.', 'Shyness is weird too, that awkwardness breeds more shyness I feel and just makes for a positive feedback']}\n",
            "guided_chosen_suggestions: ['wizard_of_wikipedia', '', '']\n"
          ]
        }
      ],
      "source": [
        "# Show examples\n",
        "for i, instance in enumerate(test_dataset.select(range(1))):\n",
        "    # for (x, y) in zip(instance['free_messages'], instance['guided_messages']):\n",
        "    #     print(\"U--{}\\nG--{}\".format(x, y))\n",
        "    #     print(\"\")\n",
        "    for key, value in instance.items():\n",
        "        if key != 'label_candidates':\n",
        "            print(\"{}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8sPchPi4zW3"
      },
      "outputs": [],
      "source": [
        "data_args = Namespace(\n",
        "    model_name_or_path=\"facebook/bart-base\",\n",
        "    # model_name_or_path=\"results/\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=256,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='results/bart',\n",
        ")\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(data_args.model_name_or_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHvWQLxuhgsM"
      },
      "source": [
        "##### Tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ykvNCj60y0eT"
      },
      "outputs": [],
      "source": [
        "# def preprocess_bst(examples):\n",
        "#     inputs = [sent for ex in examples[\"free_messages\"] for sent in ex]\n",
        "#     targets = [sent for ex in examples[\"guided_messages\"] for sent in ex]\n",
        "#     model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "#     # Setup the tokenizer for targets\n",
        "#     with tokenizer.as_target_tokenizer():\n",
        "#         labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "#     # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "#     # padding in the loss.\n",
        "#     if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "#         labels[\"input_ids\"] = [\n",
        "#             [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "#         ]\n",
        "\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs\n",
        "\n",
        "def preprocess_bst(examples):\n",
        "    num_entries = len(examples[\"free_messages\"])\n",
        "    persona_pieces = [\n",
        "        f\"[PS] {examples['personas'][0]}\",\n",
        "        f\"[PS] {examples['personas'][1]}\",\n",
        "    ]\n",
        "    if examples['context'] == \"wizard_of_wikipedia\":\n",
        "        additional_context_pieces = [f\"[CTX] {examples['additional_context']}. [SEP] \"]\n",
        "    else:\n",
        "        additional_context_pieces = [\"[SEP] \"]\n",
        "\n",
        "    previous_utterance_pieces = examples[\"previous_utterance\"]\n",
        "\n",
        "    inputs, labels = [], []\n",
        "    for entry_idx in range(num_entries):\n",
        "        free_message = examples['free_messages'][entry_idx]\n",
        "        guided_message = examples['guided_messages'][entry_idx]\n",
        "\n",
        "        previous_utterance = ' [SEP] '.join(previous_utterance_pieces)\n",
        "        original_context = ' '.join(\n",
        "            persona_pieces + additional_context_pieces\n",
        "        ) + previous_utterance\n",
        "        # Input & Output\n",
        "        text = original_context + ' ' + tokenizer.eos_token + ' ' + free_message\n",
        "        inputs.append(text)\n",
        "        labels.append(guided_message)\n",
        "\n",
        "        previous_utterance_pieces += [\n",
        "            free_message,\n",
        "            guided_message,\n",
        "        ]\n",
        "        # print(\"history: \", text)\n",
        "        # print(\"label: \", guided_message)\n",
        "\n",
        "    inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(labels, max_length=data_args.max_target_length, padding=padding, truncation=True)\n",
        "    \n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
        "    # when we want to ignore padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # ['input_ids', 'attention_mask', 'labels']\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    # total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # # customize this part to your needs.\n",
        "    # if total_length >= block_size:\n",
        "    #     total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    # # Split by chunks of max_len.\n",
        "    # result = {\n",
        "    #     k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "    #     for k, t in concatenated_examples.items()\n",
        "    # }\n",
        "    # result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return concatenated_examples\n",
        "    # return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNv_GKSVy0a9"
      },
      "outputs": [],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "print(tokenized_train_dataset)\n",
        "print(tokenized_eval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfVI32fzhmV6"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "64cF7tqxy0KF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=30,\n",
        "    predict_with_generate=True, # generation task\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
        "if data_args.pad_to_max_length:\n",
        "    data_collator = default_data_collator\n",
        "else:\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
        "    )\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=tokenized_eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "yLh15B-ny0HZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/monkey/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 27018\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 10\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 200\n",
            "  Gradient Accumulation steps = 20\n",
            "  Total optimization steps = 4050\n",
            "  Number of trainable parameters = 139420416\n",
            "  0%|          | 0/4050 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "  1%|          | 38/4050 [02:25<4:13:34,  3.79s/it]"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Casual Language Model (CLM) e.g., DialoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_args = Namespace(\n",
        "    model_name_or_path=\"microsoft/DialoGPT-small\",\n",
        "    # model_name_or_path=\"results/\",\n",
        "    max_length=1000,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='results/dialogpt',\n",
        "    block_size=None,\n",
        ")\n",
        "\n",
        "max_length = data_args.max_length\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(data_args.model_name_or_path, config=config)\n",
        "\n",
        "print(len(tokenizer))\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(tokenizer.eos_token)\n",
        "\n",
        "# Define new special tokens: [PS], [CTX]\n",
        "num_added_toks = tokenizer.add_tokens(['[PS]'], special_tokens=True) ## this line is updated\n",
        "num_added_toks = tokenizer.add_tokens(['[CTX]'], special_tokens=True) ## this line is updated\n",
        "num_added_toks = tokenizer.add_tokens(['[SEP]'], special_tokens=True) ## this line is updated\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "if data_args.block_size is None:\n",
        "    block_size = tokenizer.model_max_length\n",
        "    if block_size > 1024:\n",
        "        block_size = 1024\n",
        "else:\n",
        "    block_size = min(data_args.block_size, tokenizer.model_max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_bst(examples):\n",
        "    num_entries = len(examples[\"free_messages\"])\n",
        "    persona_pieces = [\n",
        "        f\"[PS] {examples['personas'][0]}\",\n",
        "        f\"[PS] {examples['personas'][1]}\",\n",
        "    ]\n",
        "    if examples['context'] == \"wizard_of_wikipedia\":\n",
        "        additional_context_pieces = [f\"[CTX] {examples['additional_context']}. [SEP] \"]\n",
        "    else:\n",
        "        additional_context_pieces = [\"[SEP] \"]\n",
        "\n",
        "    previous_utterance_pieces = examples[\"previous_utterance\"]\n",
        "\n",
        "    inputs, labels = [], []\n",
        "    for entry_idx in range(num_entries):\n",
        "        free_message = examples['free_messages'][entry_idx]\n",
        "        guided_message = examples['guided_messages'][entry_idx]\n",
        "\n",
        "        previous_utterance = ' [SEP] '.join(previous_utterance_pieces)\n",
        "        original_context = ' '.join(\n",
        "            persona_pieces + additional_context_pieces\n",
        "        ) + previous_utterance\n",
        "        # Input & Output\n",
        "        text = original_context + ' ' + tokenizer.eos_token + ' ' + free_message\n",
        "        inputs.append(text)\n",
        "        labels.append(guided_message)\n",
        "\n",
        "        previous_utterance_pieces += [\n",
        "            free_message,\n",
        "            guided_message,\n",
        "        ]\n",
        "        # print(\"history: \", text)\n",
        "        # print(\"label: \", guided_message)\n",
        "\n",
        "    inputs = tokenizer(inputs, max_length=data_args.max_length, padding=padding, truncation=True)\n",
        "    labels = tokenizer(labels, max_length=data_args.max_length, padding=padding, truncation=True)\n",
        "    \n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
        "    # when we want to ignore padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # ['input_ids', 'attention_mask', 'labels']\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    # total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # # customize this part to your needs.\n",
        "    # if total_length >= block_size:\n",
        "    #     total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    # # Split by chunks of max_len.\n",
        "    # result = {\n",
        "    #     k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "    #     for k, t in concatenated_examples.items()\n",
        "    # }\n",
        "    # result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return concatenated_examples\n",
        "    # return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4819/4819 [00:04<00:00, 1039.56ex/s]\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.63ba/s]\n",
            "100%|██████████| 1009/1009 [00:01<00:00, 778.44ex/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.80ba/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 27018\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 5651\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "batched_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "batched_eval_dataset = tokenized_eval_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "print(batched_train_dataset)\n",
        "print(batched_eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [50257, 1312, 1053, 587, 257, 15169, 1201, 1312, 373, 642, 13, 220, 50257, 1312, 423, 734, 9397, 13, 220, 50258, 3854, 9860, 13, 220, 50259, 1629, 3961, 4301, 1099, 373, 616, 4004, 2426, 13, 775, 750, 423, 257, 10457, 6240, 508, 925, 340, 477, 523, 3499, 13, 843, 922, 329, 345, 532, 2506, 14071, 10552, 13, 220, 50259, 1081, 257, 4301, 3761, 6136, 11, 616, 22847, 318, 11749, 5371, 12954, 290, 10192, 511, 2489, 13, 220, 706, 477, 11, 2506, 318, 10218, 1566, 9157, 6717, 13, 220, 50256, 314, 4236, 351, 345, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [72, 1842, 9326, 1165, 2592, 262, 2726, 3392, 355, 484, 760, 703, 284, 2222, 736, 5316]}\n"
          ]
        }
      ],
      "source": [
        "print(batched_train_dataset[2843])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=30,\n",
        ")\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    if isinstance(logits, tuple):\n",
        "        # Depending on the model and config, logits may contain extra tensors,\n",
        "        # like past_key_values, but logits always come first\n",
        "        logits = logits[0]\n",
        "    return logits.argmax(dim=-1)\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics if training_args.do_eval else None,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: suggestions, context, previous_utterance, personas, free_messages, guided_chosen_suggestions, additional_context, guided_messages, label_candidates. If suggestions, context, previous_utterance, personas, free_messages, guided_chosen_suggestions, additional_context, guided_messages, label_candidates are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
            "/home/monkey/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 0\n",
            "  Num Epochs = 30\n",
            "  Instantaneous batch size per device = 10\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 200\n",
            "  Gradient Accumulation steps = 20\n",
            "  Total optimization steps = 720\n",
            "  Number of trainable parameters = 124442112\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "Invalid key: 2843 is out of bounds for size 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[81], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m checkpoint \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49mcheckpoint)\n\u001b[1;32m      4\u001b[0m trainer\u001b[39m.\u001b[39msave_model()  \u001b[39m# Saves the tokenizer too for easy upload\u001b[39;00m\n\u001b[1;32m      6\u001b[0m metrics \u001b[39m=\u001b[39m train_result\u001b[39m.\u001b[39mmetrics\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py:1723\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1722\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1723\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1724\u001b[0m \n\u001b[1;32m   1725\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1726\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1727\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:2356\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2355\u001b[0m     \u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\n\u001b[1;32m   2357\u001b[0m         key,\n\u001b[1;32m   2358\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py:2340\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   2338\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2339\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, decoded\u001b[39m=\u001b[39mdecoded, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2340\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2341\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2342\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2343\u001b[0m )\n\u001b[1;32m   2344\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/formatting/formatting.py:463\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n\u001b[0;32m--> 463\u001b[0m     _check_valid_index_key(key, size)\n\u001b[1;32m    464\u001b[0m \u001b[39m# Query the main table\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/formatting/formatting.py:406\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[1;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m (key \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m key \u001b[39m+\u001b[39m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m size):\n\u001b[0;32m--> 406\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid key: \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m is out of bounds for size \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    407\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n",
            "\u001b[0;31mIndexError\u001b[0m: Invalid key: 2843 is out of bounds for size 0"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 2, 3],\n",
              " [9, 1, 2, 6, 33, 15, 25, 90],\n",
              " [39, 0, 511, 76, 24, 8, 11],\n",
              " [11, 0, 6, 54, 7, 2],\n",
              " [76, 43, 8, 33, 28, 0, 22],\n",
              " [21, 45, 98, 3, 43, 34, 12, 4],\n",
              " [122, 2, 3, 87, 3, 2],\n",
              " [9, 1, 2, 6, 33]]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a1 = [[1, 2, 3], [9, 1, 2, 6, 33, 15, 25, 90], [39, 0, 511, 76, 24, 8, 11]]\n",
        "a2 = [[11, 0, 6, 54, 7, 2], [76, 43, 8, 33, 28, 0, 22], [21, 45, 98, 3, 43, 34, 12, 4]]\n",
        "a3 = [[122, 2, 3, 87, 3, 2], [9, 1, 2, 6, 33]]\n",
        "\n",
        "a = [a1, a2, a3]\n",
        "list(chain(*a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kyn42UoVJ5"
      },
      "source": [
        "### Attack methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0GNLfbkioZwO"
      },
      "outputs": [],
      "source": [
        "class BaseAttacker:\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "      \n",
        "        self.device = device\n",
        "        self.model = model.to(self.device)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.embedding = self.model.get_input_embeddings().weight\n",
        "        self.specical_token = self.tokenizer.all_special_tokens\n",
        "        self.specical_id = self.tokenizer.all_special_ids\n",
        "        self.eos_token_id = self.model.config.eos_token_id\n",
        "        self.pad_token_id = self.model.config.pad_token_id\n",
        "        self.num_beams = self.model.config.num_beams\n",
        "        self.num_beam_groups = self.model.config.num_beam_groups\n",
        "        self.max_len = max_len\n",
        "        self.max_per = max_per\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    @classmethod\n",
        "    def _get_hparam(cls, namespace: Namespace, key: str, default=None):\n",
        "        if hasattr(namespace, key):\n",
        "            return getattr(namespace, key)\n",
        "        print('Using default argument for \"{}\"'.format(key))\n",
        "\n",
        "        return default\n",
        "\n",
        "    def run_attack(self, x):\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, x):\n",
        "        pass\n",
        "\n",
        "    def compute_seq_len(self, seq):\n",
        "        if seq[0].eq(self.pad_token_id):\n",
        "            return int(len(seq) - sum(seq.eq(self.pad_token_id)))\n",
        "        else:\n",
        "            return int(len(seq) - sum(seq.eq(self.pad_token_id))) - 1\n",
        "\n",
        "    def get_prediction(self, sentence):\n",
        "        def remove_pad(s):\n",
        "            for i, tk in enumerate(s):\n",
        "                if tk == self.eos_token_id and i != 0:\n",
        "                    return s[:i + 1]\n",
        "            return s\n",
        "\n",
        "        input_ids = self.tokenizer(sentence, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "        \n",
        "        # ['sequences', 'sequences_scores', 'scores', 'beam_indices'] if num_beams != 1\n",
        "        # ['sequences', 'scores'] if num_beams = 1\n",
        "        outputs = dialogue(\n",
        "            self.model, \n",
        "            input_ids,\n",
        "            early_stopping=False, \n",
        "            num_beams=self.num_beams,\n",
        "            num_beam_groups=self.num_beam_groups, \n",
        "            use_cache=True,\n",
        "            max_length=self.max_len,\n",
        "        )\n",
        "        \n",
        "        seqs = outputs['sequences']\n",
        "        seqs = [remove_pad(seq) for seq in seqs]\n",
        "        out_scores = outputs['scores']\n",
        "        pred_len = [self.compute_seq_len(seq) for seq in seqs]\n",
        "        return pred_len, seqs, out_scores\n",
        "\n",
        "    def get_trans_string_len(self, text):\n",
        "        pred_len, seqs, _ = self.get_prediction(text)\n",
        "        return seqs[0], pred_len[0]\n",
        "\n",
        "    def get_trans_len(self, text):\n",
        "        pred_len, _, _ = self.get_prediction(text)\n",
        "        return pred_len\n",
        "\n",
        "    def get_trans_strings(self, text):\n",
        "        pred_len, seqs, _ = self.get_prediction(text)\n",
        "        out_res = [self.tokenizer.decode(seq, skip_special_tokens=True) for seq in seqs]\n",
        "        return out_res, pred_len\n",
        "    \n",
        "    def compute_score(self, text):\n",
        "        batch_size = len(text)\n",
        "        index_list = [i * self.num_beams for i in range(batch_size + 1)]\n",
        "        pred_len, seqs, out_scores = self.get_prediction(text)\n",
        "\n",
        "\n",
        "        scores = [[] for _ in range(batch_size)]\n",
        "        for out_s in out_scores:\n",
        "            for i in range(batch_size):\n",
        "                current_index = index_list[i]\n",
        "                scores[i].append(out_s[current_index: current_index + 1])\n",
        "        scores = [torch.cat(s) for s in scores]\n",
        "        scores = [s[:pred_len[i]] for i, s in enumerate(scores)]\n",
        "        return scores, seqs, pred_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RzfF1r-CoZtg"
      },
      "outputs": [],
      "source": [
        "class SlowAttacker(BaseAttacker):\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "        super(SlowAttacker, self).__init__(device, tokenizer, model, max_len, max_per)\n",
        "\n",
        "    def leave_eos_loss(self, scores, pred_len):\n",
        "        loss = []\n",
        "        for i, s in enumerate(scores):\n",
        "            s[:, self.pad_token_id] = 1e-12 # T X V\n",
        "            eos_p = self.softmax(s)[:pred_len[i], self.eos_token_id]\n",
        "            loss.append(self.bce_loss(eos_p, torch.zeros_like(eos_p)))\n",
        "        return loss\n",
        "\n",
        "    def leave_eos_target_loss(self, scores, seqs, pred_len):\n",
        "        loss = []\n",
        "        for i, s in enumerate(scores): # s: T X V\n",
        "            # if self.pad_token_id != self.eos_token_id:\n",
        "            s[:, self.pad_token_id] = 1e-12\n",
        "            softmax_v = self.softmax(s)\n",
        "            eos_p = softmax_v[:pred_len[i], self.eos_token_id]\n",
        "            target_p = torch.stack([softmax_v[idx, s] for idx, s in enumerate(seqs[i][1:])])\n",
        "            target_p = target_p[:pred_len[i]]\n",
        "            pred = eos_p + target_p\n",
        "            pred[-1] = pred[-1] / 2\n",
        "            loss.append(self.bce_loss(pred, torch.zeros_like(pred)))\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def select_best(self, new_strings, batch_size=30):\n",
        "        \"\"\"\n",
        "        Select generated strings which induce longest output sentences.\n",
        "        \"\"\"\n",
        "        pred_len = []\n",
        "        # seqs = []\n",
        "        batch_num = len(new_strings) // batch_size\n",
        "        if batch_size * batch_num != len(new_strings):\n",
        "            batch_num += 1\n",
        "\n",
        "        for i in range(batch_num):\n",
        "            st, ed = i * batch_size, min(i * batch_size + batch_size, len(new_strings))\n",
        "            input_ids = self.tokenizer(new_strings[st:ed], return_tensors=\"pt\", padding=True).input_ids\n",
        "            input_ids = input_ids.to(self.device)\n",
        "            outputs = self.model.generate(\n",
        "                input_ids, \n",
        "                num_beams=self.num_beams, \n",
        "                max_length=self.max_len,\n",
        "                return_dict_in_generate=True,\n",
        "            )\n",
        "            lengths = [self.compute_seq_len(seq) for seq in outputs['sequences']]\n",
        "            # pdb.set_trace()\n",
        "            pred_len.extend(lengths)\n",
        "            \n",
        "        # pred_len = np.array([self.compute_seq_len(torch.tensor(seq)) for seq in seqs])\n",
        "        pred_len = np.array(pred_len)\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        assert len(new_strings) == len(pred_len)\n",
        "        return new_strings[pred_len.argmax()], max(pred_len)\n",
        "\n",
        "    def prepare_attack(self, text):\n",
        "        ori_len = self.get_trans_len(text)[0] # original sentence length\n",
        "        best_adv_text, best_len = deepcopy(text), ori_len\n",
        "        current_adv_text, current_len = deepcopy(text), ori_len  # current_adv_text: List[str]\n",
        "        return ori_len, (best_adv_text, best_len), (current_adv_text, current_len)\n",
        "\n",
        "    def compute_loss(self, text):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def mutation(self, current_adv_text, grad, modified_pos):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run_attack(self, text):\n",
        "        \"\"\"\n",
        "        (1) Using gradient ascent to generate adversarial sentences -- mutation();\n",
        "        (2) Select the best samples which induce longest output sentences -- select_best();\n",
        "        (3) Save the adversarial samples -- adv_his.\n",
        "        \"\"\"\n",
        "        assert len(text) != 1\n",
        "        # torch.autograd.set_detect_anomaly(True)\n",
        "        ori_len, (best_adv_text, best_len), (current_adv_text, current_len) = self.prepare_attack(text)\n",
        "        # adv_his = [(deepcopy(current_adv_text), deepcopy(current_len), 0.0)]\n",
        "        adv_his = []\n",
        "        modify_pos = []\n",
        "        pbar = tqdm(range(self.max_per))\n",
        "        t1 = time.time()\n",
        "\n",
        "        for it in pbar:\n",
        "            loss_list = self.compute_loss([current_adv_text])\n",
        "            loss = sum(loss_list)\n",
        "            self.model.zero_grad()\n",
        "            loss.backward()\n",
        "            grad = self.embedding.grad\n",
        "            new_strings = self.mutation(current_adv_text, grad, modify_pos)\n",
        "\n",
        "            if new_strings:\n",
        "                current_adv_text, current_len = self.select_best(new_strings)\n",
        "                log_str = \"%d, %d, %.2f\" % (it, len(new_strings), best_len / ori_len)\n",
        "                pbar.set_description(log_str)\n",
        "\n",
        "                if current_len > best_len:\n",
        "                    best_adv_text = deepcopy(current_adv_text)\n",
        "                    best_len = current_len\n",
        "                t2 = time.time()\n",
        "                adv_his.append((best_adv_text, int(best_len), t2 - t1))\n",
        "\n",
        "        if adv_his:\n",
        "            return True, adv_his\n",
        "        else:\n",
        "            return False, [(deepcopy(current_adv_text), deepcopy(current_len), 0.0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZkV1HKfloZmK"
      },
      "outputs": [],
      "source": [
        "class WordAttacker(SlowAttacker):\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "        super(WordAttacker, self).__init__(device, tokenizer, model, max_len, max_per)\n",
        "\n",
        "    def compute_loss(self, text):\n",
        "        scores, seqs, pred_len = self.compute_score(text) # [T X V], [T], [1]\n",
        "        loss_list = self.leave_eos_target_loss(scores, seqs, pred_len)\n",
        "        # loss_list = self.leave_eos_loss(scores, pred_len)\n",
        "        return loss_list\n",
        "    \n",
        "\n",
        "    def token_replace_mutation(self, current_adv_text, grad, modified_pos):\n",
        "        new_strings = []\n",
        "        current_ids = self.tokenizer(current_adv_text, return_tensors=\"pt\", padding=True).input_ids[0]\n",
        "        base_ids = current_ids.clone()\n",
        "        for pos in modified_pos:\n",
        "            t = current_ids[0][pos]\n",
        "            grad_t = grad[t]\n",
        "            score = (self.embedding - self.embedding[t]).mm(grad_t.reshape([-1, 1])).reshape([-1])\n",
        "            index = score.argsort()\n",
        "            for tgt_t in index:\n",
        "                if tgt_t not in self.specical_token:\n",
        "                    base_ids[pos] = tgt_t\n",
        "                    break\n",
        "\n",
        "        for pos, t in enumerate(current_ids):\n",
        "            if t not in self.specical_id:\n",
        "                cnt, grad_t = 0, grad[t]\n",
        "                score = (self.embedding - self.embedding[t]).mm(grad_t.reshape([-1, 1])).reshape([-1])\n",
        "                index = score.argsort()\n",
        "                for tgt_t in index:\n",
        "                    if tgt_t not in self.specical_token:\n",
        "                        new_base_ids = base_ids.clone()\n",
        "                        new_base_ids[pos] = tgt_t\n",
        "                        candidate_s = self.tokenizer.decode(new_base_ids, skip_special_tokens=True)\n",
        "                        new_strings.append(candidate_s)\n",
        "                        cnt += 1\n",
        "                        if cnt >= 50:\n",
        "                            break\n",
        "\n",
        "        return new_strings\n",
        "\n",
        "\n",
        "    def mutation(self, current_adv_text, grad, modify_pos):\n",
        "        new_strings = self.token_replace_mutation(current_adv_text, grad, modify_pos)\n",
        "        return new_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GnXnWV4o4e2"
      },
      "source": [
        "### Inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o-edv1Ybo3-D"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(preds, labels, metric, tokenizer):\n",
        "    if not isinstance(preds, list):\n",
        "        preds = [preds]\n",
        "    if not isinstance(labels, list):\n",
        "        labels = [labels]\n",
        "    preds, labels = postprocess_text(preds, labels)\n",
        "    result = metric.compute(predictions=preds, references=labels)\n",
        "    return result['score']\n",
        "\n",
        "\n",
        "def inference(sentence, label, model, tokenizer, metric, device):\n",
        "    input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "    t1 = time.time()\n",
        "    outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    t2 = time.time()\n",
        "    prediction_len = len(output.split())\n",
        "    eval_score = compute_metrics(output, label, metric, tokenizer)\n",
        "\n",
        "    print(\"\")\n",
        "    success, adv_his = attacker.run_attack(sentence)\n",
        "    print(\"\\nU--{}\".format(sentence))\n",
        "    print(\"G--{}\".format(output))\n",
        "    print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(\n",
        "        prediction_len, t2-t1, eval_score,\n",
        "    ))\n",
        "\n",
        "    if success:\n",
        "        print(\"U'--{}\".format(adv_his[-1][0]))\n",
        "    else:\n",
        "        print(\"Attack failed!\")\n",
        "\n",
        "    input_ids = tokenizer(adv_his[-1][0], return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "    t1 = time.time()\n",
        "    outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    t2 = time.time()\n",
        "    prediction_len = len(output.split())\n",
        "    print(\"G'--{}\".format(output))\n",
        "    eval_score = compute_metrics(output, label, metric, tokenizer)\n",
        "    print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(\n",
        "        prediction_len, t2-t1, eval_score,\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGhuwycqpe6V"
      },
      "source": [
        "### Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAVngC49o36c"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "attacker = WordAttacker(\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        max_len=64,\n",
        "        max_per=1,\n",
        "    )\n",
        "\n",
        "metric = load_metric(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OF2USA2kXUS",
        "outputId": "50058530-ce29-4dd5-8f67-a2cbbfb86fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 500, 1.00: 100%|██████████| 1/1 [00:12<00:00, 12.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--Can't believe the kid grew up so quick.\n",
            "G--I can't believe it. He's only a few months old.\n",
            "(length: 10, latency: 0.189, BLEU: 3.386)\n",
            "U'--Can Directions believe the kid grew up so quick.\n",
            "G'--I can't believe it. My dad was an Army brat and he always said he wanted to be a pilot.\n",
            "(length: 20, latency: 0.277, BLEU: 1.727)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 350, 1.00: 100%|██████████| 1/1 [00:11<00:00, 11.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--How would I start rock climbing?\n",
            "G--If you know how to climb, you can google it.\n",
            "(length: 10, latency: 0.211, BLEU: 12.046)\n",
            "U'--How would I start rock waterfall?\n",
            "G'--There are many ways to start a rock waterfall. There are many different types of water based on rock factors such as salinity, temperate, etc.\n",
            "(length: 25, latency: 0.352, BLEU: 1.820)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 350, 1.00: 100%|██████████| 1/1 [00:09<00:00,  9.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--How often do you use computers?\n",
            "G--Almost every week ends.\n",
            "(length: 4, latency: 0.113, BLEU: 8.627)\n",
            "U'--How sort do you use computers?\n",
            "G'--I have a couple of computers. One is a gaming rig and the other is a storage unit.\n",
            "(length: 18, latency: 0.235, BLEU: 2.708)\n"
          ]
        }
      ],
      "source": [
        "# Demo 1\n",
        "input_text = \"Can't believe the kid grew up so quick.\"\n",
        "output_text = \"Yeah, kids grow up so quickly.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)\n",
        "\n",
        "# Demo 2\n",
        "input_text = \"How would I start rock climbing?\"\n",
        "output_text = \"You can google it. But I suggest you to find a local climbing gym and take a class.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)\n",
        "\n",
        "# Demo 3\n",
        "input_text = \"How often do you use computers?\"\n",
        "output_text = \"Almost every week. I use them for work and personal use.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YEeglOfRpzba"
      },
      "outputs": [],
      "source": [
        "# Demo on BST test set\n",
        "import random\n",
        "\n",
        "def test_demo(device, model, tokenizer, attacker, max_num_samples=100, max_per=3):\n",
        "    random.seed(2019)\n",
        "    bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "    test_dataset = bst_dataset['test']\n",
        "    ids = random.sample(range(len(test_dataset)), max_num_samples)\n",
        "\n",
        "    sampled_test_dataset = test_dataset.select(ids)\n",
        "\n",
        "    metric = load_metric(\"sacrebleu\")\n",
        "    ori_lens, adv_lens = [], []\n",
        "    ori_bleus, adv_bleus = [], []\n",
        "    ori_time, adv_time = [], []\n",
        "    att_success = 0\n",
        "    total_pairs = 0\n",
        "\n",
        "    for i, instance in tqdm(enumerate(sampled_test_dataset)):\n",
        "        if total_pairs >= max_num_samples:\n",
        "            break\n",
        "\n",
        "        for (sentence, label) in zip(instance['free_messages'], instance['guided_messages']):\n",
        "\n",
        "            input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "            input_ids = input_ids.to(device)\n",
        "            t1 = time.time()\n",
        "            \n",
        "            outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "            output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            t2 = time.time()\n",
        "            pred_len = np.count_nonzero(outputs[0].cpu() != tokenizer.pad_token_id)\n",
        "            eval_scores = compute_metrics(output, label, metric, tokenizer)\n",
        "            \n",
        "            ori_lens.append(pred_len)\n",
        "            ori_bleus.append(eval_scores)\n",
        "            ori_time.append(t2-t1)\n",
        "            \n",
        "            # Attack\n",
        "            print(\"\")\n",
        "            success, adv_his = attacker.run_attack(sentence)\n",
        "            print('\\n')\n",
        "            print(\"U--{}\".format(sentence))\n",
        "            print(\"G--{}\".format(output))\n",
        "            print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(pred_len, t2-t1, eval_scores))\n",
        "\n",
        "            if success:\n",
        "                # print(\"Attack Succeed!\")\n",
        "                print(\"U'--{}\".format(adv_his[-1][0]))\n",
        "            else:\n",
        "                print(\"Attack failed!\")\n",
        "\n",
        "            input_ids = tokenizer(adv_his[-1][0], return_tensors=\"pt\").input_ids\n",
        "            input_ids = input_ids.to(device)\n",
        "            t1 = time.time()\n",
        "            outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "            output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            t2 = time.time()\n",
        "            adv_pred_len = np.count_nonzero(outputs[0].cpu() != tokenizer.pad_token_id)\n",
        "            print(\"G'--{}\".format(output))\n",
        "            eval_scores = compute_metrics(output, label, metric, tokenizer)\n",
        "            print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(adv_pred_len, t2-t1, eval_scores))\n",
        "\n",
        "            adv_lens.append(adv_pred_len)\n",
        "            adv_bleus.append(eval_scores)\n",
        "            adv_time.append(t2-t1)\n",
        "\n",
        "            att_success += (adv_pred_len > pred_len)\n",
        "            total_pairs += 1\n",
        "\n",
        "            if total_pairs >= max_num_samples:\n",
        "                break\n",
        "\n",
        "\n",
        "    # Summarize eval results\n",
        "    ori_len = np.mean(ori_lens)\n",
        "    adv_len = np.mean(adv_lens)\n",
        "    ori_bleu = np.mean(ori_bleus)\n",
        "    adv_bleu = np.mean(adv_bleus)\n",
        "    ori_t = np.mean(ori_time)\n",
        "    adv_t = np.mean(adv_time)\n",
        "    print(\"Original output length: {:.3f}, latency: {:.3f}, BLEU: {:.3f}\".format(ori_len, ori_t, ori_bleu))\n",
        "    print(\"Adversarial output length: {:.3f}, latency: {:.3f}, BLEU: {:.3f}\".format(adv_len, adv_t, adv_bleu))\n",
        "    print(\"Attack success rate: {:.2f}%\".format(100*att_success/total_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "84315fa7f06c46278ea553e836ce60e9",
            "213f6810504c44b3ba261a5eaea26b82",
            "f985e336fb6b46c094cadb43bdbc0e4a",
            "2a90ce1d47e243e1a574e923098867f3",
            "26a6adaecf3b462186fa09311f32a4d1",
            "9d8ec86383e14aa7ad58304e64546741",
            "428f603e389347229be629f3c16b29d9",
            "a40f1faa4ea44197be87ef6f6bc63c4e",
            "91ecbb6978664e289607aac1db2e4166",
            "01bdfd23d64049638a2e629d093ee7bb",
            "40b233347a504c5699577a4f94b91db1"
          ]
        },
        "id": "wQkZmzYStHZL",
        "outputId": "2eb827f3-7f0f-4dee-9315-9d681e941ff2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset blended_skill_talk (/root/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/8544e13cbbf2fb9b34157f2e2f28c1539e4f36bf0ef2bd96edd138b4000c5ca1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84315fa7f06c46278ea553e836ce60e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 550, 1.00:   0%|          | 0/1 [00:15<?, ?it/s]\u001b[A\n",
            "0, 550, 1.00: 100%|██████████| 1/1 [00:15<00:00, 15.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--I have two daughters. Do you have any children?\n",
            "G--Yes, I have a son and daughter. How about you?\n",
            "(length: 16, latency: 0.161, BLEU: 30.266)\n",
            "U'--I have twoifix. Do you have any children?\n",
            "G'--No, I don't. But I wish someday after getting married I will have my own. Do you like kids?\n",
            "(length: 27, latency: 0.275, BLEU: 1.958)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 600, 1.00:   0%|          | 0/1 [00:14<?, ?it/s]\u001b[A\n",
            "0, 600, 1.00: 100%|██████████| 1/1 [00:14<00:00, 14.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--Congratulations! That's great. Do you have any hobbies?\n",
            "G--Yes, I love to read. What about you?\n",
            "(length: 14, latency: 0.162, BLEU: 3.629)\n",
            "U'--Congratulations! That's great. Do you have any averages?\n",
            "G'--I don't have any averages, but I do know that if I work hard enough I will be good at something.\n",
            "(length: 27, latency: 0.287, BLEU: 2.167)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 950, 1.00:   0%|          | 0/1 [00:34<?, ?it/s]\u001b[A\n",
            "0, 950, 1.00: 100%|██████████| 1/1 [00:34<00:00, 34.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--Keeping active will make you feel younger and healthy too. Will you teach your grandchild soccer?\n",
            "G--Yes, I will. It's a fun sport to play. How old is he?\n",
            "(length: 21, latency: 0.207, BLEU: 3.152)\n",
            "U'--Keeping activeELS make you feel younger and healthy too. Will you teach your grandchild soccer?\n",
            "G'--I think it would be a good way to keep them active and healthy. I'm not sure yet, but I hope to one day.\n",
            "(length: 31, latency: 0.323, BLEU: 1.891)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 1050, 1.00:   0%|          | 0/1 [00:28<?, ?it/s]\u001b[A\n",
            "0, 1050, 1.00: 100%|██████████| 1/1 [00:28<00:00, 28.62s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--Wonderful. I am sure they will appreciate that! I like to photograph nature in my free time.\n",
            "G--I love taking pictures of nature in my free time.\n",
            "(length: 14, latency: 0.152, BLEU: 1.815)\n",
            "U'--Wonderful. I am sure they will appreciate that! I took to photograph nature in my free time.\n",
            "G'--It's nice to have a hobby, especially when you have free time. What kinds of things do you do?\n",
            "(length: 26, latency: 0.282, BLEU: 2.443)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 1300, 1.00:   0%|          | 0/1 [00:45<?, ?it/s]\u001b[A\n",
            "0, 1300, 1.00: 100%|██████████| 1/1 [00:45<00:00, 45.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--That's okay! Yes, I mostly photograph nature, but I like to photograph other things as well, like people and landmarks.\n",
            "G--That's interesting. I like to photograph nature in general. People and landmarks are fascinating to me.\n",
            "(length: 23, latency: 0.233, BLEU: 2.167)\n",
            "U'--That's okay! Yes, I mostly photograph nature, but I like pip photograph other things as well, like people and landmarks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [02:21, 141.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "G'--That's interesting. I like to photograph nature in general, but I also like being able to photograph other things, like people and landmarks.\n",
            "(length: 31, latency: 0.335, BLEU: 1.650)\n",
            "Original output length: 17.600, latency: 0.183, BLEU: 8.206\n",
            "Adversarial output length: 28.400, latency: 0.301, BLEU: 2.022\n",
            "Attack success rate: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "max_num_samples = 5\n",
        "max_per = 1\n",
        "test_demo(device, model, tokenizer, attacker, max_num_samples, max_per)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q3MYRHwtOah"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wHvWQLxuhgsM",
        "SfVI32fzhmV6"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('py3.10')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ed07da37284b78ddf1027a77eacdaaa03fe44cd85ed96169aaea4ffb3c093db"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "004738fd62854848a7676e3100693a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37c18f11be549a8834c745a960807f3",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9301ea70fc4a6691af00b5d43321f4",
            "value": "100%"
          }
        },
        "01bdfd23d64049638a2e629d093ee7bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be25e0afa9c405fa3b7089fe6aa6ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d8e725392444308b0c8233f62d9ec64",
            "placeholder": "​",
            "style": "IPY_MODEL_3c2deb172de24560b57a5a548acfe53b",
            "value": " 3/3 [00:00&lt;00:00, 82.94it/s]"
          }
        },
        "213f6810504c44b3ba261a5eaea26b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8ec86383e14aa7ad58304e64546741",
            "placeholder": "​",
            "style": "IPY_MODEL_428f603e389347229be629f3c16b29d9",
            "value": "100%"
          }
        },
        "26a6adaecf3b462186fa09311f32a4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a90ce1d47e243e1a574e923098867f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01bdfd23d64049638a2e629d093ee7bb",
            "placeholder": "​",
            "style": "IPY_MODEL_40b233347a504c5699577a4f94b91db1",
            "value": " 3/3 [00:00&lt;00:00, 89.30it/s]"
          }
        },
        "36d9fd106ff240c8b0fb971e5c8efb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c2deb172de24560b57a5a548acfe53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b233347a504c5699577a4f94b91db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "428f603e389347229be629f3c16b29d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d8e725392444308b0c8233f62d9ec64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7170283bda7045e283e036ccaeffdcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_004738fd62854848a7676e3100693a1b",
              "IPY_MODEL_df68b3c7f37d4821bff89b0e1cd082bb",
              "IPY_MODEL_1be25e0afa9c405fa3b7089fe6aa6ba7"
            ],
            "layout": "IPY_MODEL_7c8a70a52cb64d8eb765ee4cd5df30ec"
          }
        },
        "7c8a70a52cb64d8eb765ee4cd5df30ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84315fa7f06c46278ea553e836ce60e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_213f6810504c44b3ba261a5eaea26b82",
              "IPY_MODEL_f985e336fb6b46c094cadb43bdbc0e4a",
              "IPY_MODEL_2a90ce1d47e243e1a574e923098867f3"
            ],
            "layout": "IPY_MODEL_26a6adaecf3b462186fa09311f32a4d1"
          }
        },
        "91ecbb6978664e289607aac1db2e4166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d8ec86383e14aa7ad58304e64546741": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40f1faa4ea44197be87ef6f6bc63c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37c18f11be549a8834c745a960807f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df68b3c7f37d4821bff89b0e1cd082bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5f96869232484282b47280a9f0639c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d9fd106ff240c8b0fb971e5c8efb3b",
            "value": 3
          }
        },
        "ed5f96869232484282b47280a9f0639c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f985e336fb6b46c094cadb43bdbc0e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40f1faa4ea44197be87ef6f6bc63c4e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ecbb6978664e289607aac1db2e4166",
            "value": 3
          }
        },
        "fe9301ea70fc4a6691af00b5d43321f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
