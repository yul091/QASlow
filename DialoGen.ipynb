{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUd3jJg_egjk",
        "outputId": "a3588f7e-728a-46a4-9feb-2f16f0ead794"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-08 12:52:11.267859: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-08 12:52:11.810885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-08 12:52:11.810928: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-08 12:52:11.810933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.dont_write_bytecode = True\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from itertools import chain\n",
        "import language_tool_python\n",
        "from argparse import Namespace\n",
        "from datasets import load_dataset, load_metric, DatasetDict, Dataset\n",
        "from transformers import (\n",
        "    AutoConfig, \n",
        "    AutoTokenizer, \n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from typing import *\n",
        "from DialogueAPI import dialogue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXWXvwRJjpBA"
      },
      "source": [
        "## Blended_Skill_Talk Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "7170283bda7045e283e036ccaeffdcf2",
            "004738fd62854848a7676e3100693a1b",
            "df68b3c7f37d4821bff89b0e1cd082bb",
            "1be25e0afa9c405fa3b7089fe6aa6ba7",
            "7c8a70a52cb64d8eb765ee4cd5df30ec",
            "d37c18f11be549a8834c745a960807f3",
            "fe9301ea70fc4a6691af00b5d43321f4",
            "ed5f96869232484282b47280a9f0639c",
            "36d9fd106ff240c8b0fb971e5c8efb3b",
            "6d8e725392444308b0c8233f62d9ec64",
            "3c2deb172de24560b57a5a548acfe53b"
          ]
        },
        "id": "qIXJ_oTvGRcB",
        "outputId": "829e202a-f7cc-4849-c194-50ae9afa91e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset blended_skill_talk (/home/monkey/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/8544e13cbbf2fb9b34157f2e2f28c1539e4f36bf0ef2bd96edd138b4000c5ca1)\n",
            "100%|██████████| 3/3 [00:00<00:00, 774.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 4819\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 1009\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 980\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "train_dataset = bst_dataset['train']\n",
        "eval_dataset = bst_dataset['validation']\n",
        "test_dataset = bst_dataset['test']\n",
        "print(bst_dataset)\n",
        "# print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOTNHI1jq5W1",
        "outputId": "88cba86e-ddf9-4eea-ee9b-9a2cdafabdb0"
      },
      "outputs": [],
      "source": [
        "# Get statistics of pair of dialogues \n",
        "train_num, eval_num, test_num = 0, 0, 0\n",
        "for i, instance in enumerate(train_dataset):\n",
        "    train_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(eval_dataset):\n",
        "    eval_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(test_dataset):\n",
        "    test_num += len(instance['free_messages'])\n",
        "\n",
        "print(\"#pairs of training dialogues: {}, validation dialogues: {}, test dialogues: {}\".format(\n",
        "    train_num, eval_num, test_num,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xnYGvfspczn",
        "outputId": "f17f5947-5fb2-4902-96e1-021efe8dfc0f"
      },
      "outputs": [],
      "source": [
        "# Show examples\n",
        "# for i, instance in enumerate(test_dataset.select(range(1))):\n",
        "#     for key, value in instance.items():\n",
        "#         if key != 'label_candidates':\n",
        "#             print(\"{} ({}): {}\".format(key, len(value), value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8sPchPi4zW3"
      },
      "outputs": [],
      "source": [
        "data_args = Namespace(\n",
        "    model_name_or_path=\"facebook/bart-base\",\n",
        "    # model_name_or_path=\"results/\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=256,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='results/bart',\n",
        ")\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "# config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(data_args.model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_added_toks = tokenizer.add_tokens(['<PS>'], special_tokens=True) ## this line is updated\n",
        "num_added_toks = tokenizer.add_tokens(['<CTX>'], special_tokens=True) ## this line is updated\n",
        "num_added_toks = tokenizer.add_tokens(['<SEP>'], special_tokens=True) ## this line is updated\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHvWQLxuhgsM"
      },
      "source": [
        "##### Tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykvNCj60y0eT"
      },
      "outputs": [],
      "source": [
        "def preprocess_bst(examples):\n",
        "    num_entries = len(examples[\"free_messages\"])\n",
        "    persona_pieces = [\n",
        "        f\"<PS> {examples['personas'][0]}\",\n",
        "        f\"<PS> {examples['personas'][1]}\",\n",
        "    ]\n",
        "    if examples['context'] == \"wizard_of_wikipedia\":\n",
        "        additional_context_pieces = [f\"[<CTX> {examples['additional_context']}. <SEP> \"]\n",
        "    else:\n",
        "        additional_context_pieces = [\"<SEP> \"]\n",
        "\n",
        "    previous_utterance_pieces = examples[\"previous_utterance\"]\n",
        "    inputs, labels = [], []\n",
        "    for entry_idx in range(num_entries):\n",
        "        free_message = examples['free_messages'][entry_idx]\n",
        "        guided_message = examples['guided_messages'][entry_idx]\n",
        "\n",
        "        previous_utterance = ' <SEP> '.join(previous_utterance_pieces)\n",
        "        original_context = ' '.join(\n",
        "            persona_pieces + additional_context_pieces\n",
        "        ) + previous_utterance\n",
        "        # Input & Output\n",
        "        text = original_context + ' ' + tokenizer.eos_token + ' ' + free_message\n",
        "        inputs.append(text)\n",
        "        labels.append(guided_message)\n",
        "\n",
        "        previous_utterance_pieces += [\n",
        "            free_message,\n",
        "            guided_message,\n",
        "        ]\n",
        "        # print(\"history: \", text)\n",
        "        # print(\"label: \", guided_message)\n",
        "\n",
        "    inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(labels, max_length=data_args.max_target_length, padding=padding, truncation=True)\n",
        "    \n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
        "    # when we want to ignore padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # ['input_ids', 'attention_mask', 'labels']\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    return concatenated_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNv_GKSVy0a9"
      },
      "outputs": [],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "print(tokenized_train_dataset)\n",
        "print(tokenized_eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfVI32fzhmV6"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64cF7tqxy0KF"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=30,\n",
        "    predict_with_generate=True, # generation task\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
        "if data_args.pad_to_max_length:\n",
        "    data_collator = default_data_collator\n",
        "else:\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
        "    )\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=tokenized_eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLh15B-ny0HZ"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Casual Language Model (CLM) e.g., DialoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_args = Namespace(\n",
        "    # model_name_or_path=\"microsoft/DialoGPT-small\",\n",
        "    model_name_or_path=\"results/personagpt\",\n",
        "    # model_name_or_path=\"gpt2\",\n",
        "    max_length=1000,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='results/dialogpt',\n",
        "    block_size=None,\n",
        ")\n",
        "\n",
        "max_length = data_args.max_length\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(data_args.model_name_or_path, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_bst(examples):\n",
        "    num_entries = len(examples[\"free_messages\"])\n",
        "    persona_pieces = [\n",
        "        f\"<PS> {examples['personas'][0]}\",\n",
        "        f\"<PS> {examples['personas'][1]}\",\n",
        "    ]\n",
        "    if examples['context'] == \"wizard_of_wikipedia\":\n",
        "        additional_context_pieces = [f\"[<CTX> {examples['additional_context']}. <SEP> \"]\n",
        "    else:\n",
        "        additional_context_pieces = [\"<SEP> \"]\n",
        "\n",
        "    previous_utterance_pieces = examples[\"previous_utterance\"]\n",
        "    inputs, labels = [], []\n",
        "    for entry_idx in range(num_entries):\n",
        "        free_message = examples['free_messages'][entry_idx]\n",
        "        guided_message = examples['guided_messages'][entry_idx]\n",
        "\n",
        "        previous_utterance = ' <SEP> '.join(previous_utterance_pieces)\n",
        "        original_context = ' '.join(\n",
        "            persona_pieces + additional_context_pieces\n",
        "        ) + previous_utterance\n",
        "        # Input & Output\n",
        "        text = original_context + ' ' + tokenizer.eos_token + ' ' + free_message\n",
        "        inputs.append(text)\n",
        "        labels.append(guided_message)\n",
        "\n",
        "        previous_utterance_pieces += [\n",
        "            free_message,\n",
        "            guided_message,\n",
        "        ]\n",
        "        # print(\"history: \", text)\n",
        "        # print(\"label: \", guided_message)\n",
        "\n",
        "    inputs = tokenizer(inputs, max_length=data_args.max_length, padding=padding, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(labels, max_length=data_args.max_length, padding=padding, truncation=True)\n",
        "    \n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
        "    # when we want to ignore padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # ['input_ids', 'attention_mask', 'labels']\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    return concatenated_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "batched_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "batched_eval_dataset = tokenized_eval_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "print(batched_train_dataset)\n",
        "print(batched_eval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=30,\n",
        ")\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    if isinstance(logits, tuple):\n",
        "        # Depending on the model and config, logits may contain extra tensors,\n",
        "        # like past_key_values, but logits always come first\n",
        "        logits = logits[0]\n",
        "    return logits.argmax(dim=-1)\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics if training_args.do_eval else None,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGhuwycqpe6V"
      },
      "source": [
        "### Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from DG_dataset import DGDataset\n",
        "from attacker.DGSlow import StructureAttacker\n",
        "\n",
        "device = torch.device('cpu')\n",
        "task = \"clm\"\n",
        "sp_token = '<SEP>' # for clm\n",
        "data_name = 'blended_skill_talk'\n",
        "max_length = 128\n",
        "num_beams = 4\n",
        "num_beam_groups = 1\n",
        "max_per = 1 # number of perturbations\n",
        "model_path = 'results/dialogpt'\n",
        "config = AutoConfig.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, config=config)\n",
        "\n",
        "dg = DGDataset(\n",
        "    dataset=data_name,\n",
        "    task=task,\n",
        "    tokenizer=tokenizer,\n",
        "    max_source_length=max_length,\n",
        "    max_target_length=max_length,\n",
        ")\n",
        "attacker = StructureAttacker(\n",
        "    device=device,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    max_len=max_length,\n",
        "    max_per=max_per,\n",
        "    task=task,\n",
        "    use_combined_loss=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAVngC49o36c"
      },
      "outputs": [],
      "source": [
        "instance = test_dataset[1]\n",
        "entry_idx = 0\n",
        "# Original generation\n",
        "num_entries, total_entries, context, prev_utt_pc = dg.prepare_context(instance)\n",
        "free_msg, guided_msg, orig_context, references = dg.prepare_entry(\n",
        "    instance, \n",
        "    entry_idx, \n",
        "    context, \n",
        "    prev_utt_pc,\n",
        "    total_entries,\n",
        ")\n",
        "# orig_context = \"You seem to know a lot about it. I chose the topic because I don't know anything about it.  Yeah it's the chat process that works on a client/server model. It's a network chat. It's very interesting. Do you want to now more? Not really. Let's talk about food. What do you like to eat? I LOVE fish  I like fish too.  But really I am not picky.  Do you eat a lot of fish?\"\n",
        "# free_msg = \"I eat pretty much only fish. My parents do too, and they're both over 6 feet. Probably cause of the fish LOL\"\n",
        "# Original generation\n",
        "text = orig_context + sp_token + free_msg\n",
        "print(\"C--{}\".format(orig_context))\n",
        "print(\"U--{}\".format(free_msg))\n",
        "effective_text = text + tokenizer.eos_token # for clm\n",
        "# effective_text = text\n",
        "inputs = tokenizer(\n",
        "    effective_text,  \n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    max_length=max_length-1,\n",
        ")\n",
        "input_ids = inputs.input_ids\n",
        "print(\"input_ids\", input_ids.shape)\n",
        "with torch.no_grad():\n",
        "    outputs = dialogue(\n",
        "        model, \n",
        "        input_ids,\n",
        "        early_stopping=False, \n",
        "        num_beams=num_beams,\n",
        "        num_beam_groups=num_beam_groups, \n",
        "        use_cache=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "output = tokenizer.batch_decode(\n",
        "    outputs['sequences'][:, input_ids.shape[-1]:], \n",
        "    skip_special_tokens=True,\n",
        ")[0]\n",
        "print(\"G--{}\".format(output))\n",
        "\n",
        "# # Attack\n",
        "# success, adv_his = attacker.run_attack(text, guided_msg)\n",
        "# new_text = adv_his[-1][0]\n",
        "# new_free_msg= new_text.split(sp_token)[1].strip()\n",
        "# print(\"U'--{}\".format(new_free_msg))\n",
        "# cos_sim = attacker.sent_encoder.get_sim(new_free_msg, free_msg)\n",
        "\n",
        "# effective_text = new_text + tokenizer.eos_token # for clm\n",
        "# inputs = tokenizer(\n",
        "#     effective_text,  \n",
        "#     return_tensors=\"pt\",\n",
        "#     truncation=True,\n",
        "#     max_length=max_length,\n",
        "# )\n",
        "# input_ids = inputs.input_ids\n",
        "# with torch.no_grad():\n",
        "#     outputs = dialogue(\n",
        "#         model, \n",
        "#         input_ids,\n",
        "#         early_stopping=False, \n",
        "#         num_beams=num_beams,\n",
        "#         num_beam_groups=num_beam_groups, \n",
        "#         use_cache=True,\n",
        "#         max_length=max_length,\n",
        "#     )\n",
        "\n",
        "# new_output = tokenizer.batch_decode(\n",
        "#     outputs['sequences'][:, input_ids.shape[-1]:], \n",
        "#     skip_special_tokens=True,\n",
        "# )[0]\n",
        "# print(\"G'--{}\".format(output))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ConvAI2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset conv_ai_2 (/home/monkey/.cache/huggingface/datasets/conv_ai_2/conv_ai_2/1.0.0/11d600ddce66bb9d07ca50d1b55b488145ef0d5d0206168c32f1043677875865)\n",
            "100%|██████████| 1/1 [00:00<00:00, 594.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialog_id', 'dialog', 'bot_profile', 'user_profile', 'eval_score', 'profile_match'],\n",
            "        num_rows: 3495\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"conv_ai_2\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for i in range(1):\n",
        "#     print(dataset['train'][i])\n",
        "#     instance = dataset['train'][i]\n",
        "#     user_profile = ' '.join([''.join(x) for x in instance['user_profile']])\n",
        "#     print('user profile: ', user_profile)\n",
        "\n",
        "#     persona_pieces = f\"<PS> {user_profile}\"\n",
        "#     num_entries = len([x for x in instance['dialog'] if x['sender_class'] == 'Human'])\n",
        "#     previous_utterance_pieces = [persona_pieces]\n",
        "\n",
        "#     for entry_idx in range(num_entries):\n",
        "#         bot_msg = instance['dialog'][entry_idx*2]['text']\n",
        "#         human_msg = instance['dialog'][entry_idx*2+1]['text']\n",
        "#         original_context = ' '.join(previous_utterance_pieces)\n",
        "#         previous_utterance_pieces += [\n",
        "#             bot_msg,\n",
        "#             human_msg,\n",
        "#         ]\n",
        "\n",
        "#         text = original_context + '<EOS>' + bot_msg + '<EOS>' + human_msg\n",
        "#         print(\"text: \", text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Empathetic Dialogues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset empathetic_dialogues (/home/monkey/.cache/huggingface/datasets/empathetic_dialogues/default/0.1.0/09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf)\n",
            "100%|██████████| 3/3 [00:00<00:00, 727.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 76673\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 12030\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 10943\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"empathetic_dialogues\")\n",
        "print(dataset)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'conv_id': 'hit:0_conv:1',\n",
              " 'utterance_idx': 1,\n",
              " 'context': 'sentimental',\n",
              " 'prompt': 'I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.',\n",
              " 'speaker_idx': 1,\n",
              " 'utterance': 'I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people_comma_ we felt like the only people in the world.',\n",
              " 'selfeval': '5|5|5_2|2|5',\n",
              " 'tags': ''}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def group_texts(dataset):\n",
        "    results = {\n",
        "        'conv_id': [], \n",
        "        'prompt': [],\n",
        "        'dialog': [], \n",
        "        'context': [],\n",
        "    }\n",
        "    for i, instance in enumerate(dataset):\n",
        "        if instance['utterance_idx'] == 1:\n",
        "            results['conv_id'].append(instance['conv_id'])\n",
        "            results['dialog'].append([])\n",
        "            results['prompt'].append(instance['prompt'])\n",
        "            results['context'].append(instance['context'])\n",
        "\n",
        "        response = {'text': instance['utterance'], 'speaker_idx': instance['speaker_idx']}\n",
        "        results['dialog'][-1].append(response)\n",
        "\n",
        "    return Dataset.from_dict(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grouped_test_dataset = group_texts(test_dataset)\n",
        "print(grouped_test_dataset[0])\n",
        "grouped_test_dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PersonaChat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"AlekseyKorshuk/persona-chat\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(dataset['train'].column_names)\n",
        "# print(dataset['train'][0]['personality'])\n",
        "# print(dataset['train'][0]['utterances'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "bleu = evaluate.load('bleu')\n",
        "predictions = ['Yes, I have two daughters. I am a grandparent at 44.'.lower()]\n",
        "references = [[\"yes i have a son and just recently i became a grandpa\".lower()]]\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "print(results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the example tips dataset\n",
        "df = pd.DataFrame({\n",
        "    'Score': np.array([0.705, 0.789, 0.753, 0.736, 0.728, 0.762]), \n",
        "    'Metric': ['Prec.', 'Prec.', 'Rec.', 'Rec.', 'F1', 'F1'],\n",
        "    'method': ['w/o ES', 'w/ ES', 'w/o ES', 'w/ ES','w/o ES', 'w/ ES'],\n",
        "})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the total bill amount for each day of the week\n",
        "plt.figure(figsize=(5, 2))\n",
        "a = sns.barplot(\n",
        "    x='Score', \n",
        "    y='Metric', \n",
        "    data=df, \n",
        "    hue='method', \n",
        "    orient=\"h\",\n",
        "    width=0.55,\n",
        ")\n",
        "a.legend(ncol=1)\n",
        "a.set_xlim(0.65, 0.8)\n",
        "a.set_xlabel('')\n",
        "a.set_ylabel('')\n",
        "# Show the plot\n",
        "plt.show()\n",
        "a.figure.savefig('es.pdf', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wHvWQLxuhgsM",
        "SfVI32fzhmV6"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ed07da37284b78ddf1027a77eacdaaa03fe44cd85ed96169aaea4ffb3c093db"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "004738fd62854848a7676e3100693a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37c18f11be549a8834c745a960807f3",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9301ea70fc4a6691af00b5d43321f4",
            "value": "100%"
          }
        },
        "01bdfd23d64049638a2e629d093ee7bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be25e0afa9c405fa3b7089fe6aa6ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d8e725392444308b0c8233f62d9ec64",
            "placeholder": "​",
            "style": "IPY_MODEL_3c2deb172de24560b57a5a548acfe53b",
            "value": " 3/3 [00:00&lt;00:00, 82.94it/s]"
          }
        },
        "213f6810504c44b3ba261a5eaea26b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8ec86383e14aa7ad58304e64546741",
            "placeholder": "​",
            "style": "IPY_MODEL_428f603e389347229be629f3c16b29d9",
            "value": "100%"
          }
        },
        "26a6adaecf3b462186fa09311f32a4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a90ce1d47e243e1a574e923098867f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01bdfd23d64049638a2e629d093ee7bb",
            "placeholder": "​",
            "style": "IPY_MODEL_40b233347a504c5699577a4f94b91db1",
            "value": " 3/3 [00:00&lt;00:00, 89.30it/s]"
          }
        },
        "36d9fd106ff240c8b0fb971e5c8efb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c2deb172de24560b57a5a548acfe53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b233347a504c5699577a4f94b91db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "428f603e389347229be629f3c16b29d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d8e725392444308b0c8233f62d9ec64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7170283bda7045e283e036ccaeffdcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_004738fd62854848a7676e3100693a1b",
              "IPY_MODEL_df68b3c7f37d4821bff89b0e1cd082bb",
              "IPY_MODEL_1be25e0afa9c405fa3b7089fe6aa6ba7"
            ],
            "layout": "IPY_MODEL_7c8a70a52cb64d8eb765ee4cd5df30ec"
          }
        },
        "7c8a70a52cb64d8eb765ee4cd5df30ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84315fa7f06c46278ea553e836ce60e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_213f6810504c44b3ba261a5eaea26b82",
              "IPY_MODEL_f985e336fb6b46c094cadb43bdbc0e4a",
              "IPY_MODEL_2a90ce1d47e243e1a574e923098867f3"
            ],
            "layout": "IPY_MODEL_26a6adaecf3b462186fa09311f32a4d1"
          }
        },
        "91ecbb6978664e289607aac1db2e4166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d8ec86383e14aa7ad58304e64546741": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40f1faa4ea44197be87ef6f6bc63c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37c18f11be549a8834c745a960807f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df68b3c7f37d4821bff89b0e1cd082bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5f96869232484282b47280a9f0639c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d9fd106ff240c8b0fb971e5c8efb3b",
            "value": 3
          }
        },
        "ed5f96869232484282b47280a9f0639c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f985e336fb6b46c094cadb43bdbc0e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40f1faa4ea44197be87ef6f6bc63c4e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ecbb6978664e289607aac1db2e4166",
            "value": 3
          }
        },
        "fe9301ea70fc4a6691af00b5d43321f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
