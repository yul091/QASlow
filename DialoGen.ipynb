{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wHvWQLxuhgsM",
        "SfVI32fzhmV6"
      ],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers==4.24.0\n",
        "! pip install datasets \n",
        "! pip install sacrebleu"
      ],
      "metadata": {
        "id": "Aqs6Cy4veuWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUd3jJg_egjk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import language_tool_python\n",
        "from argparse import Namespace\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (\n",
        "    AutoConfig, \n",
        "    AutoTokenizer, \n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForMaskedLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "import os\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "import inspect\n",
        "\n",
        "from transformers.generation_utils import BeamSearchScorer\n",
        "from transformers import LogitsProcessorList, StoppingCriteriaList\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blended_Skill_Talk Dataset"
      ],
      "metadata": {
        "id": "mXWXvwRJjpBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "train_dataset = bst_dataset['train']\n",
        "eval_dataset = bst_dataset['validation']\n",
        "test_dataset = bst_dataset['test']"
      ],
      "metadata": {
        "id": "qIXJ_oTvGRcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_args = Namespace(\n",
        "    # model_name_or_path=\"facebook/bart-base\",\n",
        "    model_name_or_path=\"/content/drive/My Drive/results/\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=256,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='.',\n",
        ")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True, # generation task\n",
        ")\n",
        "\n",
        "max_target_length = data_args.max_target_length\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False"
      ],
      "metadata": {
        "id": "M8sPchPi4zW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(data_args.model_name_or_path)"
      ],
      "metadata": {
        "id": "_5bCeARMzxEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize dataset"
      ],
      "metadata": {
        "id": "wHvWQLxuhgsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_bst(examples):\n",
        "    inputs = [sent for ex in examples[\"free_messages\"] for sent in ex]\n",
        "    targets = [sent for ex in examples[\"guided_messages\"] for sent in ex]\n",
        "    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "ykvNCj60y0eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")"
      ],
      "metadata": {
        "id": "WNv_GKSVy0a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "SfVI32fzhmV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
        "if data_args.pad_to_max_length:\n",
        "    data_collator = default_data_collator\n",
        "else:\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
        "    )\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
        ")"
      ],
      "metadata": {
        "id": "64cF7tqxy0KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "yLh15B-ny0HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dialogue API"
      ],
      "metadata": {
        "id": "9obPrvznFjL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dialogue(\n",
        "        module,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        max_length: Optional[int] = None,\n",
        "        min_length: Optional[int] = None,\n",
        "        do_sample: Optional[bool] = None,\n",
        "        early_stopping: Optional[bool] = None,\n",
        "        num_beams: Optional[int] = None,\n",
        "        temperature: Optional[float] = None,\n",
        "        top_k: Optional[int] = None,\n",
        "        top_p: Optional[float] = None,\n",
        "        repetition_penalty: Optional[float] = None,\n",
        "        bad_words_ids: Optional[Iterable[int]] = None,\n",
        "        bos_token_id: Optional[int] = None,\n",
        "        pad_token_id: Optional[int] = None,\n",
        "        eos_token_id: Optional[int] = None,\n",
        "        length_penalty: Optional[float] = None,\n",
        "        no_repeat_ngram_size: Optional[int] = None,\n",
        "        encoder_no_repeat_ngram_size: Optional[int] = None,\n",
        "        num_return_sequences: Optional[int] = None,\n",
        "        max_time: Optional[float] = None,\n",
        "        max_new_tokens: Optional[int] = None,\n",
        "        decoder_start_token_id: Optional[int] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        num_beam_groups: Optional[int] = None,\n",
        "        diversity_penalty: Optional[float] = None,\n",
        "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
        "        logits_processor: Optional[LogitsProcessorList] = LogitsProcessorList(),\n",
        "        stopping_criteria: Optional[StoppingCriteriaList] = StoppingCriteriaList(),\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        output_scores: Optional[bool] = None,\n",
        "        return_dict_in_generate: Optional[bool] = None,\n",
        "        forced_bos_token_id: Optional[int] = None,\n",
        "        forced_eos_token_id: Optional[int] = None,\n",
        "        remove_invalid_values: Optional[bool] = None,\n",
        "        synced_gpus: Optional[bool] = None,\n",
        "        **model_kwargs):\n",
        "    # 1. Set generation parameters if not already defined\n",
        "    bos_token_id = bos_token_id if bos_token_id is not None else module.config.bos_token_id\n",
        "    num_beams = num_beams if num_beams is not None else module.config.num_beams\n",
        "    length_penalty = length_penalty if length_penalty is not None else module.config.length_penalty\n",
        "    early_stopping = early_stopping if early_stopping is not None else module.config.early_stopping\n",
        "    num_beam_groups = num_beam_groups if num_beam_groups is not None else module.config.num_beam_groups\n",
        "    do_sample = do_sample if do_sample is not None else module.config.do_sample\n",
        "    num_return_sequences = (\n",
        "        num_return_sequences if num_return_sequences is not None else module.config.num_return_sequences\n",
        "    )\n",
        "\n",
        "    pad_token_id = pad_token_id if pad_token_id is not None else module.config.pad_token_id\n",
        "    eos_token_id = eos_token_id if eos_token_id is not None else module.config.eos_token_id\n",
        "\n",
        "    # output_scores = output_scores if output_scores is not None else module.config.output_scores\n",
        "    output_scores = True\n",
        "    output_attentions = output_attentions if output_attentions is not None else module.config.output_attentions\n",
        "    output_hidden_states = (\n",
        "        output_hidden_states if output_hidden_states is not None else module.config.output_hidden_states\n",
        "    )\n",
        "    # return_dict_in_generate = (\n",
        "    #     return_dict_in_generate if return_dict_in_generate is not None else module.config.return_dict_in_generate\n",
        "    # )\n",
        "    return_dict_in_generate = True\n",
        "\n",
        "    if pad_token_id is None and eos_token_id is not None:\n",
        "        # special case if pad_token_id is not defined\n",
        "        # logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
        "        pad_token_id = eos_token_id\n",
        "\n",
        "    # 2. Define model inputs\n",
        "    # inputs_tensor has to be defined\n",
        "    # model_input_name is defined if model-specific keyword input is passed\n",
        "    # otherwise model_input_name is None\n",
        "    # all model-specific keyword inputs are removed from `model_kwargs`\n",
        "    inputs_tensor, model_input_name, model_kwargs = module._prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n",
        "    batch_size = inputs_tensor.shape[0]\n",
        "\n",
        "    # 3. Define other model kwargs\n",
        "    model_kwargs[\"output_attentions\"] = output_attentions\n",
        "    model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
        "    model_kwargs[\"use_cache\"] = use_cache\n",
        "\n",
        "    accepts_attention_mask = \"attention_mask\" in set(inspect.signature(module.forward).parameters.keys())\n",
        "    requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
        "\n",
        "    if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n",
        "        model_kwargs[\"attention_mask\"] = module._prepare_attention_mask_for_generation(\n",
        "            inputs_tensor, pad_token_id, eos_token_id\n",
        "        )\n",
        "\n",
        "    if module.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
        "        # if model is encoder decoder encoder_outputs are created\n",
        "        # and added to `model_kwargs`\n",
        "        model_kwargs = module._prepare_encoder_decoder_kwargs_for_generation(\n",
        "            inputs_tensor, model_kwargs, model_input_name\n",
        "        )\n",
        "\n",
        "    # 4. Prepare `input_ids` which will be used for auto-regressive generation\n",
        "    if module.config.is_encoder_decoder:\n",
        "        input_ids = module._prepare_decoder_input_ids_for_generation(\n",
        "            batch_size,\n",
        "            decoder_start_token_id=decoder_start_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            model_kwargs=model_kwargs,\n",
        "        )\n",
        "    else:\n",
        "        # if decoder-only then inputs_tensor has to be `input_ids`\n",
        "        input_ids = inputs_tensor\n",
        "\n",
        "    # 5. Prepare `max_length` depending on other stopping criteria\n",
        "    # if `max_new_tokens` is passed, but not `max_length` -> set `max_length = max_new_tokens`\n",
        "    if max_length is None and max_new_tokens is not None:\n",
        "        max_length = max_new_tokens + input_ids.shape[-1]\n",
        "    elif max_length is not None and max_new_tokens is not None:\n",
        "        pass\n",
        "        # Both are set, this is odd, raise a warning\n",
        "        # warnings.warn(\n",
        "        #     \"Both `max_length` and `max_new_tokens` have been set \"\n",
        "        #     f\"but they serve the same purpose. `max_length` {max_length} \"\n",
        "        #     f\"will take priority over `max_new_tokens` {max_new_tokens}.\",\n",
        "        #     UserWarning,\n",
        "        # )\n",
        "    # default to config if still None\n",
        "    max_length = max_length if max_length is not None else module.config.max_length\n",
        "\n",
        "    if input_ids.shape[-1] >= max_length:\n",
        "        input_ids_string = \"decoder_input_ids\" if module.config.is_encoder_decoder else \"input_ids\"\n",
        "        # logger.warning(\n",
        "        #     f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}. \"\n",
        "        #     \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
        "        # )\n",
        "\n",
        "    # 6. determine generation mode\n",
        "    is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False\n",
        "    is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True\n",
        "    is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False\n",
        "    is_beam_sample_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is True\n",
        "    is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1)\n",
        "\n",
        "    if num_beam_groups > num_beams:\n",
        "        raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
        "    if is_group_beam_gen_mode and do_sample is True:\n",
        "        raise ValueError(\n",
        "            \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
        "        )\n",
        "\n",
        "    # # 7. prepare distribution pre_processing samplers\n",
        "    # logits_processor = module._get_logits_processor(\n",
        "    #     repetition_penalty=repetition_penalty,\n",
        "    #     no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "    #     encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
        "    #     encoder_input_ids=inputs_tensor,\n",
        "    #     bad_words_ids=bad_words_ids,\n",
        "    #     min_length=min_length,\n",
        "    #     max_length=max_length,\n",
        "    #     eos_token_id=eos_token_id,\n",
        "    #     forced_bos_token_id=forced_bos_token_id,\n",
        "    #     forced_eos_token_id=forced_eos_token_id,\n",
        "    #     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
        "    #     num_beams=num_beams,\n",
        "    #     num_beam_groups=num_beam_groups,\n",
        "    #     diversity_penalty=diversity_penalty,\n",
        "    #     remove_invalid_values=remove_invalid_values,\n",
        "    #     logits_processor=logits_processor,\n",
        "    # )\n",
        "\n",
        "    # 8. prepare stopping criteria\n",
        "    stopping_criteria = module._get_stopping_criteria(\n",
        "        max_length=max_length, max_time=max_time, stopping_criteria=stopping_criteria\n",
        "    )\n",
        "\n",
        "    # 9. go into different generation modes\n",
        "    if is_greedy_gen_mode:\n",
        "        if num_return_sequences > 1:\n",
        "            raise ValueError(\n",
        "                f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
        "            )\n",
        "\n",
        "        # 10. run greedy search\n",
        "        return module.greedy_search(\n",
        "            input_ids,\n",
        "            logits_processor=logits_processor,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "    elif is_sample_gen_mode:\n",
        "        # 10. prepare logits warper\n",
        "        logits_warper = module._get_logits_warper(\n",
        "            top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
        "        )\n",
        "\n",
        "        # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n",
        "        input_ids, model_kwargs = module._expand_inputs_for_generation(\n",
        "            input_ids,\n",
        "            expand_size=num_return_sequences,\n",
        "            is_encoder_decoder=module.config.is_encoder_decoder,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "        # 12. run sample\n",
        "        return module.sample(\n",
        "            input_ids,\n",
        "            logits_processor=logits_processor,\n",
        "            logits_warper=logits_warper,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "    elif is_beam_gen_mode:\n",
        "        if num_return_sequences > num_beams:\n",
        "            raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
        "\n",
        "        if stopping_criteria.max_length is None:\n",
        "            raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
        "\n",
        "        # 10. prepare beam search scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size,\n",
        "            num_beams=num_beams,\n",
        "            device=module.device,\n",
        "            length_penalty=length_penalty,\n",
        "            do_early_stopping=early_stopping,\n",
        "            num_beam_hyps_to_keep=num_return_sequences,\n",
        "        )\n",
        "        # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
        "        input_ids, model_kwargs = module._expand_inputs_for_generation(\n",
        "            input_ids, expand_size=num_beams, is_encoder_decoder=module.config.is_encoder_decoder, **model_kwargs\n",
        "        )\n",
        "        # 12. run beam search\n",
        "        return module.beam_search(\n",
        "            input_ids,\n",
        "            beam_scorer,\n",
        "            logits_processor=logits_processor,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "    elif is_beam_sample_gen_mode:\n",
        "        # 10. prepare logits warper\n",
        "        logits_warper = module._get_logits_warper(\n",
        "            top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
        "        )\n",
        "\n",
        "        if stopping_criteria.max_length is None:\n",
        "            raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
        "        # 11. prepare beam search scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size * num_return_sequences,\n",
        "            num_beams=num_beams,\n",
        "            device=module.device,\n",
        "            length_penalty=length_penalty,\n",
        "            do_early_stopping=early_stopping,\n",
        "        )\n",
        "\n",
        "        # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
        "        input_ids, model_kwargs = module._expand_inputs_for_generation(\n",
        "            input_ids,\n",
        "            expand_size=num_beams * num_return_sequences,\n",
        "            is_encoder_decoder=module.config.is_encoder_decoder,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "        # 13. run beam sample\n",
        "        return module.beam_sample(\n",
        "            input_ids,\n",
        "            beam_scorer,\n",
        "            logits_processor=logits_processor,\n",
        "            logits_warper=logits_warper,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "    elif is_group_beam_gen_mode:\n",
        "        if num_return_sequences > num_beams:\n",
        "            raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
        "\n",
        "        if num_beams % num_beam_groups != 0:\n",
        "            raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
        "\n",
        "        if stopping_criteria.max_length is None:\n",
        "            raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
        "\n",
        "        # 10. prepare beam search scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size,\n",
        "            num_beams=num_beams,\n",
        "            max_length=stopping_criteria.max_length,\n",
        "            device=module.device,\n",
        "            length_penalty=length_penalty,\n",
        "            do_early_stopping=early_stopping,\n",
        "            num_beam_hyps_to_keep=num_return_sequences,\n",
        "            num_beam_groups=num_beam_groups,\n",
        "        )\n",
        "        # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
        "        input_ids, model_kwargs = module._expand_inputs_for_generation(\n",
        "            input_ids, expand_size=num_beams, is_encoder_decoder=module.config.is_encoder_decoder, **model_kwargs\n",
        "        )\n",
        "        # 12. run beam search\n",
        "        return module.group_beam_search(\n",
        "            input_ids,\n",
        "            beam_scorer,\n",
        "            logits_processor=logits_processor,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )"
      ],
      "metadata": {
        "id": "C2MrymsRFUDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attack methods"
      ],
      "metadata": {
        "id": "h-kyn42UoVJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttacker:\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "      \n",
        "        self.device = device\n",
        "        self.model = model.to(self.device)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.embedding = self.model.get_input_embeddings().weight\n",
        "        self.specical_token = self.tokenizer.all_special_tokens\n",
        "        self.specical_id = self.tokenizer.all_special_ids\n",
        "        self.eos_token_id = self.model.config.eos_token_id\n",
        "        self.pad_token_id = self.model.config.pad_token_id\n",
        "        self.num_beams = self.model.config.num_beams\n",
        "        self.num_beam_groups = self.model.config.num_beam_groups\n",
        "        self.max_len = max_len\n",
        "        self.max_per = max_per\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    @classmethod\n",
        "    def _get_hparam(cls, namespace: Namespace, key: str, default=None):\n",
        "        if hasattr(namespace, key):\n",
        "            return getattr(namespace, key)\n",
        "        print('Using default argument for \"{}\"'.format(key))\n",
        "\n",
        "        return default\n",
        "\n",
        "    def run_attack(self, x):\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, x):\n",
        "        pass\n",
        "\n",
        "    def compute_seq_len(self, seq):\n",
        "        if seq[0].eq(self.pad_token_id):\n",
        "            return int(len(seq) - sum(seq.eq(self.pad_token_id)))\n",
        "        else:\n",
        "            return int(len(seq) - sum(seq.eq(self.pad_token_id))) - 1\n",
        "\n",
        "    def get_prediction(self, sentence):\n",
        "        def remove_pad(s):\n",
        "            for i, tk in enumerate(s):\n",
        "                if tk == self.eos_token_id and i != 0:\n",
        "                    return s[:i + 1]\n",
        "            return s\n",
        "\n",
        "        input_ids = self.tokenizer(sentence, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "        \n",
        "        # ['sequences', 'sequences_scores', 'scores', 'beam_indices'] if num_beams != 1\n",
        "        # ['sequences', 'scores'] if num_beams = 1\n",
        "        outputs = dialogue(\n",
        "            self.model, \n",
        "            input_ids,\n",
        "            early_stopping=False, \n",
        "            num_beams=self.num_beams,\n",
        "            num_beam_groups=self.num_beam_groups, \n",
        "            use_cache=True,\n",
        "            max_length=self.max_len,\n",
        "        )\n",
        "        \n",
        "        seqs = outputs['sequences']\n",
        "        seqs = [remove_pad(seq) for seq in seqs]\n",
        "        out_scores = outputs['scores']\n",
        "        pred_len = [self.compute_seq_len(seq) for seq in seqs]\n",
        "        return pred_len, seqs, out_scores\n",
        "\n",
        "    def get_trans_string_len(self, text):\n",
        "        pred_len, seqs, _ = self.get_prediction(text)\n",
        "        return seqs[0], pred_len[0]\n",
        "\n",
        "    def get_trans_len(self, text):\n",
        "        pred_len, _, _ = self.get_prediction(text)\n",
        "        return pred_len\n",
        "\n",
        "    def get_trans_strings(self, text):\n",
        "        pred_len, seqs, _ = self.get_prediction(text)\n",
        "        out_res = [self.tokenizer.decode(seq, skip_special_tokens=True) for seq in seqs]\n",
        "        return out_res, pred_len\n",
        "    \n",
        "    def compute_score(self, text):\n",
        "        batch_size = len(text)\n",
        "        index_list = [i * self.num_beams for i in range(batch_size + 1)]\n",
        "        pred_len, seqs, out_scores = self.get_prediction(text)\n",
        "\n",
        "\n",
        "        scores = [[] for _ in range(batch_size)]\n",
        "        for out_s in out_scores:\n",
        "            for i in range(batch_size):\n",
        "                current_index = index_list[i]\n",
        "                scores[i].append(out_s[current_index: current_index + 1])\n",
        "        scores = [torch.cat(s) for s in scores]\n",
        "        scores = [s[:pred_len[i]] for i, s in enumerate(scores)]\n",
        "        return scores, seqs, pred_len"
      ],
      "metadata": {
        "id": "0GNLfbkioZwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SlowAttacker(BaseAttacker):\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "        super(SlowAttacker, self).__init__(device, tokenizer, model, max_len, max_per)\n",
        "\n",
        "    def leave_eos_loss(self, scores, pred_len):\n",
        "        loss = []\n",
        "        for i, s in enumerate(scores):\n",
        "            s[:, self.pad_token_id] = 1e-12 # T X V\n",
        "            eos_p = self.softmax(s)[:pred_len[i], self.eos_token_id]\n",
        "            loss.append(self.bce_loss(eos_p, torch.zeros_like(eos_p)))\n",
        "        return loss\n",
        "\n",
        "    def leave_eos_target_loss(self, scores, seqs, pred_len):\n",
        "        loss = []\n",
        "        for i, s in enumerate(scores): # s: T X V\n",
        "            # if self.pad_token_id != self.eos_token_id:\n",
        "            s[:, self.pad_token_id] = 1e-12\n",
        "            softmax_v = self.softmax(s)\n",
        "            eos_p = softmax_v[:pred_len[i], self.eos_token_id]\n",
        "            target_p = torch.stack([softmax_v[idx, s] for idx, s in enumerate(seqs[i][1:])])\n",
        "            target_p = target_p[:pred_len[i]]\n",
        "            pred = eos_p + target_p\n",
        "            pred[-1] = pred[-1] / 2\n",
        "            loss.append(self.bce_loss(pred, torch.zeros_like(pred)))\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def select_best(self, new_strings, batch_size=30):\n",
        "        \"\"\"\n",
        "        Select generated strings which induce longest output sentences.\n",
        "        \"\"\"\n",
        "        pred_len = []\n",
        "        # seqs = []\n",
        "        batch_num = len(new_strings) // batch_size\n",
        "        if batch_size * batch_num != len(new_strings):\n",
        "            batch_num += 1\n",
        "\n",
        "        for i in range(batch_num):\n",
        "            st, ed = i * batch_size, min(i * batch_size + batch_size, len(new_strings))\n",
        "            input_ids = self.tokenizer(new_strings[st:ed], return_tensors=\"pt\", padding=True).input_ids\n",
        "            input_ids = input_ids.to(self.device)\n",
        "            outputs = self.model.generate(\n",
        "                input_ids, \n",
        "                num_beams=self.num_beams, \n",
        "                max_length=self.max_len,\n",
        "                return_dict_in_generate=True,\n",
        "            )\n",
        "            lengths = [self.compute_seq_len(seq) for seq in outputs['sequences']]\n",
        "            # pdb.set_trace()\n",
        "            pred_len.extend(lengths)\n",
        "            \n",
        "        # pred_len = np.array([self.compute_seq_len(torch.tensor(seq)) for seq in seqs])\n",
        "        pred_len = np.array(pred_len)\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        assert len(new_strings) == len(pred_len)\n",
        "        return new_strings[pred_len.argmax()], max(pred_len)\n",
        "\n",
        "    def prepare_attack(self, text):\n",
        "        ori_len = self.get_trans_len(text)[0] # original sentence length\n",
        "        best_adv_text, best_len = deepcopy(text), ori_len\n",
        "        current_adv_text, current_len = deepcopy(text), ori_len  # current_adv_text: List[str]\n",
        "        return ori_len, (best_adv_text, best_len), (current_adv_text, current_len)\n",
        "\n",
        "    def compute_loss(self, text):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def mutation(self, current_adv_text, grad, modified_pos):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run_attack(self, text):\n",
        "        \"\"\"\n",
        "        (1) Using gradient ascent to generate adversarial sentences -- mutation();\n",
        "        (2) Select the best samples which induce longest output sentences -- select_best();\n",
        "        (3) Save the adversarial samples -- adv_his.\n",
        "        \"\"\"\n",
        "        assert len(text) != 1\n",
        "        # torch.autograd.set_detect_anomaly(True)\n",
        "        ori_len, (best_adv_text, best_len), (current_adv_text, current_len) = self.prepare_attack(text)\n",
        "        # adv_his = [(deepcopy(current_adv_text), deepcopy(current_len), 0.0)]\n",
        "        adv_his = []\n",
        "        modify_pos = []\n",
        "        pbar = tqdm(range(self.max_per))\n",
        "        t1 = time.time()\n",
        "\n",
        "        for it in pbar:\n",
        "            loss_list = self.compute_loss([current_adv_text])\n",
        "            loss = sum(loss_list)\n",
        "            self.model.zero_grad()\n",
        "            loss.backward()\n",
        "            grad = self.embedding.grad\n",
        "            new_strings = self.mutation(current_adv_text, grad, modify_pos)\n",
        "\n",
        "            if new_strings:\n",
        "                current_adv_text, current_len = self.select_best(new_strings)\n",
        "                log_str = \"%d, %d, %.2f\" % (it, len(new_strings), best_len / ori_len)\n",
        "                pbar.set_description(log_str)\n",
        "\n",
        "                if current_len > best_len:\n",
        "                    best_adv_text = deepcopy(current_adv_text)\n",
        "                    best_len = current_len\n",
        "                t2 = time.time()\n",
        "                adv_his.append((best_adv_text, int(best_len), t2 - t1))\n",
        "\n",
        "        if adv_his:\n",
        "            return True, adv_his\n",
        "        else:\n",
        "            return False, [(deepcopy(current_adv_text), deepcopy(current_len), 0.0)]"
      ],
      "metadata": {
        "id": "RzfF1r-CoZtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordAttacker(SlowAttacker):\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "        super(WordAttacker, self).__init__(device, tokenizer, model, max_len, max_per)\n",
        "\n",
        "    def compute_loss(self, text):\n",
        "        scores, seqs, pred_len = self.compute_score(text) # [T X V], [T], [1]\n",
        "        loss_list = self.leave_eos_target_loss(scores, seqs, pred_len)\n",
        "        # loss_list = self.leave_eos_loss(scores, pred_len)\n",
        "        return loss_list\n",
        "    \n",
        "\n",
        "    def token_replace_mutation(self, current_adv_text, grad, modified_pos):\n",
        "        new_strings = []\n",
        "        current_ids = self.tokenizer(current_adv_text, return_tensors=\"pt\", padding=True).input_ids[0]\n",
        "        base_ids = current_ids.clone()\n",
        "        for pos in modified_pos:\n",
        "            t = current_ids[0][pos]\n",
        "            grad_t = grad[t]\n",
        "            score = (self.embedding - self.embedding[t]).mm(grad_t.reshape([-1, 1])).reshape([-1])\n",
        "            index = score.argsort()\n",
        "            for tgt_t in index:\n",
        "                if tgt_t not in self.specical_token:\n",
        "                    base_ids[pos] = tgt_t\n",
        "                    break\n",
        "\n",
        "        for pos, t in enumerate(current_ids):\n",
        "            if t not in self.specical_id:\n",
        "                cnt, grad_t = 0, grad[t]\n",
        "                score = (self.embedding - self.embedding[t]).mm(grad_t.reshape([-1, 1])).reshape([-1])\n",
        "                index = score.argsort()\n",
        "                for tgt_t in index:\n",
        "                    if tgt_t not in self.specical_token:\n",
        "                        new_base_ids = base_ids.clone()\n",
        "                        new_base_ids[pos] = tgt_t\n",
        "                        candidate_s = self.tokenizer.decode(new_base_ids, skip_special_tokens=True)\n",
        "                        new_strings.append(candidate_s)\n",
        "                        cnt += 1\n",
        "                        if cnt >= 50:\n",
        "                            break\n",
        "\n",
        "        return new_strings\n",
        "\n",
        "\n",
        "    def mutation(self, current_adv_text, grad, modify_pos):\n",
        "        new_strings = self.token_replace_mutation(current_adv_text, grad, modify_pos)\n",
        "        return new_strings"
      ],
      "metadata": {
        "id": "ZkV1HKfloZmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference pipeline"
      ],
      "metadata": {
        "id": "-GnXnWV4o4e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(preds, labels, metric, tokenizer):\n",
        "    if not isinstance(preds, list):\n",
        "        preds = [preds]\n",
        "    if not isinstance(labels, list):\n",
        "        labels = [labels]\n",
        "    preds, labels = postprocess_text(preds, labels)\n",
        "    result = metric.compute(predictions=preds, references=labels)\n",
        "    return result['score']\n",
        "\n",
        "\n",
        "def inference(sentence, label, model, tokenizer, metric, device):\n",
        "    input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "    t1 = time.time()\n",
        "    outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    t2 = time.time()\n",
        "    prediction_len = len(output.split())\n",
        "    eval_score = compute_metrics(output, label, metric, tokenizer)\n",
        "\n",
        "    print(\"\")\n",
        "    success, adv_his = attacker.run_attack(sentence)\n",
        "    print(\"\\nU--{}\".format(sentence))\n",
        "    print(\"G--{}\".format(output))\n",
        "    print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(\n",
        "        prediction_len, t2-t1, eval_score,\n",
        "    ))\n",
        "\n",
        "    if success:\n",
        "        print(\"U'--{}\".format(adv_his[-1][0]))\n",
        "    else:\n",
        "        print(\"Attack failed!\")\n",
        "\n",
        "    input_ids = tokenizer(adv_his[-1][0], return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "    t1 = time.time()\n",
        "    outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    t2 = time.time()\n",
        "    prediction_len = len(output.split())\n",
        "    print(\"G'--{}\".format(output))\n",
        "    eval_score = compute_metrics(output, label, metric, tokenizer)\n",
        "    print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(\n",
        "        prediction_len, t2-t1, eval_score,\n",
        "    ))"
      ],
      "metadata": {
        "id": "o-edv1Ybo3-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo"
      ],
      "metadata": {
        "id": "EGhuwycqpe6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "attacker = WordAttacker(\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        max_len=64,\n",
        "        max_per=1,\n",
        "    )\n",
        "\n",
        "metric = load_metric(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "jAVngC49o36c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo 1\n",
        "input_text = \"Can't believe the kid grew up so quick.\"\n",
        "output_text = \"Yeah, kids grow up so quickly.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)\n",
        "\n",
        "# Demo 2\n",
        "input_text = \"How would I start rock climbing?\"\n",
        "output_text = \"You can google it. But I suggest you to find a local climbing gym and take a class.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)\n",
        "\n",
        "# Demo 3\n",
        "input_text = \"How often do you use computers?\"\n",
        "output_text = \"Almost every week. I use them for work and personal use.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)"
      ],
      "metadata": {
        "id": "5OF2USA2kXUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo on BST test set\n",
        "\n",
        "import random\n",
        "\n",
        "def test_demo(device, model, tokenizer, attacker, max_num_samples=100, max_per=3):\n",
        "    random.seed(2019)\n",
        "    bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "    test_dataset = bst_dataset['test']\n",
        "    ids = random.sample(range(len(test_dataset)), max_num_samples)\n",
        "\n",
        "    sampled_test_dataset = test_dataset.select(ids)\n",
        "\n",
        "    metric = load_metric(\"sacrebleu\")\n",
        "    ori_lens, adv_lens = [], []\n",
        "    ori_bleus, adv_bleus = [], []\n",
        "    ori_time, adv_time = [], []\n",
        "    att_success = 0\n",
        "    total_pairs = 0\n",
        "\n",
        "    for i, instance in tqdm(enumerate(sampled_test_dataset)):\n",
        "        if total_pairs >= max_num_samples:\n",
        "            break\n",
        "\n",
        "        for (sentence, label) in zip(instance['free_messages'], instance['guided_messages']):\n",
        "\n",
        "            input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "            input_ids = input_ids.to(device)\n",
        "            t1 = time.time()\n",
        "            \n",
        "            outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "            output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            t2 = time.time()\n",
        "            pred_len = np.count_nonzero(outputs[0].cpu() != tokenizer.pad_token_id)\n",
        "            eval_scores = compute_metrics(output, label, metric, tokenizer)\n",
        "            \n",
        "            ori_lens.append(pred_len)\n",
        "            ori_bleus.append(eval_scores)\n",
        "            ori_time.append(t2-t1)\n",
        "            \n",
        "            # Attack\n",
        "            print(\"\")\n",
        "            success, adv_his = attacker.run_attack(sentence)\n",
        "            print('\\n')\n",
        "            print(\"U--{}\".format(sentence))\n",
        "            print(\"G--{}\".format(output))\n",
        "            print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(pred_len, t2-t1, eval_scores))\n",
        "\n",
        "            if success:\n",
        "                # print(\"Attack Succeed!\")\n",
        "                print(\"U'--{}\".format(adv_his[-1][0]))\n",
        "            else:\n",
        "                print(\"Attack failed!\")\n",
        "\n",
        "            input_ids = tokenizer(adv_his[-1][0], return_tensors=\"pt\").input_ids\n",
        "            input_ids = input_ids.to(device)\n",
        "            t1 = time.time()\n",
        "            outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "            output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            t2 = time.time()\n",
        "            adv_pred_len = np.count_nonzero(outputs[0].cpu() != tokenizer.pad_token_id)\n",
        "            print(\"G'--{}\".format(output))\n",
        "            eval_scores = compute_metrics(output, label, metric, tokenizer)\n",
        "            print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(adv_pred_len, t2-t1, eval_scores))\n",
        "\n",
        "            adv_lens.append(adv_pred_len)\n",
        "            adv_bleus.append(eval_scores)\n",
        "            adv_time.append(t2-t1)\n",
        "\n",
        "            att_success += (adv_pred_len > pred_len)\n",
        "            total_pairs += 1\n",
        "\n",
        "            if total_pairs >= max_num_samples:\n",
        "                break\n",
        "\n",
        "\n",
        "    # Summarize eval results\n",
        "    ori_len = np.mean(ori_lens)\n",
        "    adv_len = np.mean(adv_lens)\n",
        "    ori_bleu = np.mean(ori_bleus)\n",
        "    adv_bleu = np.mean(adv_bleus)\n",
        "    ori_t = np.mean(ori_time)\n",
        "    adv_t = np.mean(adv_time)\n",
        "    print(\"Original output length: {:.3f}, latency: {:.3f}, BLEU: {:.3f}\".format(ori_len, ori_t, ori_bleu))\n",
        "    print(\"Adversarial output length: {:.3f}, latency: {:.3f}, BLEU: {:.3f}\".format(adv_len, adv_t, adv_bleu))\n",
        "    print(\"Attack success rate: {:.2f}%\".format(100*att_success/total_pairs))"
      ],
      "metadata": {
        "id": "YEeglOfRpzba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_num_samples = 5\n",
        "max_per = 1\n",
        "test_demo(device, model, tokenizer, attacker, max_num_samples, max_per)"
      ],
      "metadata": {
        "id": "wQkZmzYStHZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1q3MYRHwtOah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}