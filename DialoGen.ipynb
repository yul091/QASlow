{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUd3jJg_egjk",
        "outputId": "a3588f7e-728a-46a4-9feb-2f16f0ead794"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-06 13:23:12.897708: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-06 13:23:13.765212: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-06 13:23:13.765404: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-01-06 13:23:13.765410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.dont_write_bytecode = True\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from itertools import chain\n",
        "import language_tool_python\n",
        "from argparse import Namespace\n",
        "from datasets import load_dataset, load_metric, DatasetDict, Dataset\n",
        "from transformers import (\n",
        "    AutoConfig, \n",
        "    AutoTokenizer, \n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from typing import *\n",
        "from DialogueAPI import dialogue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXWXvwRJjpBA"
      },
      "source": [
        "## Blended_Skill_Talk Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "7170283bda7045e283e036ccaeffdcf2",
            "004738fd62854848a7676e3100693a1b",
            "df68b3c7f37d4821bff89b0e1cd082bb",
            "1be25e0afa9c405fa3b7089fe6aa6ba7",
            "7c8a70a52cb64d8eb765ee4cd5df30ec",
            "d37c18f11be549a8834c745a960807f3",
            "fe9301ea70fc4a6691af00b5d43321f4",
            "ed5f96869232484282b47280a9f0639c",
            "36d9fd106ff240c8b0fb971e5c8efb3b",
            "6d8e725392444308b0c8233f62d9ec64",
            "3c2deb172de24560b57a5a548acfe53b"
          ]
        },
        "id": "qIXJ_oTvGRcB",
        "outputId": "829e202a-f7cc-4849-c194-50ae9afa91e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset blended_skill_talk (/home/monkey/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/bded69fdeee98ed8bba2ef088ac9dfd74e9ad0b95b1de5d51e333cee6f6261aa)\n",
            "100%|██████████| 3/3 [00:00<00:00, 123.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions'],\n",
            "        num_rows: 4819\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions'],\n",
            "        num_rows: 1009\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions'],\n",
            "        num_rows: 980\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "train_dataset = bst_dataset['train']\n",
        "eval_dataset = bst_dataset['validation']\n",
        "test_dataset = bst_dataset['test']\n",
        "print(bst_dataset)\n",
        "# print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOTNHI1jq5W1",
        "outputId": "88cba86e-ddf9-4eea-ee9b-9a2cdafabdb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#pairs of training dialogues: 27018, validation dialogues: 5651, test dialogues: 5482\n"
          ]
        }
      ],
      "source": [
        "# Get statistics of pair of dialogues \n",
        "train_num, eval_num, test_num = 0, 0, 0\n",
        "for i, instance in enumerate(train_dataset):\n",
        "    train_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(eval_dataset):\n",
        "    eval_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(test_dataset):\n",
        "    test_num += len(instance['free_messages'])\n",
        "\n",
        "print(\"#pairs of training dialogues: {}, validation dialogues: {}, test dialogues: {}\".format(\n",
        "    train_num, eval_num, test_num,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xnYGvfspczn",
        "outputId": "f17f5947-5fb2-4902-96e1-021efe8dfc0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "personas (2): ['i hate talking to people.', 'i believe dragons are real.']\n",
            "additional_context (14): Social anxiety\n",
            "previous_utterance (2): ['Wow, I am never shy. Do you have anxiety?', \"Yes. I end up sweating and blushing and feel like i'm going to throw up.\"]\n",
            "context (19): wizard_of_wikipedia\n",
            "free_messages (3): ['and why is that?', 'interesting but I know how you feel especially the whole people telling that it in your head ', \"Dang that's though. But I also understand that. I have people some who talks behind my back because of certain things that I believe in \"]\n",
            "guided_messages (3): [\"I think it's because in my head, I think everyone is judging me. I just start to sweat and I get sick in my stomach.\", \"I don't really have people telling me in my head, more like behind my back\", 'Me too! What do you believe in? I believe in dragons... Just finished watching Game of Thrones. Man, those things are dope']\n",
            "suggestions (3): {'convai2': [\"i've no idea i am also very shy\", 'oh i know . i always feel judged and never know what to do .', 'i try to do stuff like that all time but i can never speak up for myself'], 'empathetic_dialogues': ['Probably because I am insecure.', \"Please don't care about those people. Try to be true to yourself. World will recognize you one day.\", 'I am not sure I believe them most of the time though'], 'wizard_of_wikipedia': [\"I think it's because in my head, I think everyone is judging me. I just start to sweat and I get sick in my stomach.\", 'Right! But the difference with me is I just think about myself and no one else really. People consider it a social and cultural problem.', 'Shyness is weird too, that awkwardness breeds more shyness I feel and just makes for a positive feedback']}\n",
            "guided_chosen_suggestions (3): ['wizard_of_wikipedia', '', '']\n"
          ]
        }
      ],
      "source": [
        "# Show examples\n",
        "for i, instance in enumerate(test_dataset.select(range(1))):\n",
        "    for key, value in instance.items():\n",
        "        if key != 'label_candidates':\n",
        "            print(\"{} ({}): {}\".format(key, len(value), value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M8sPchPi4zW3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 1.72k/1.72k [00:00<00:00, 612kB/s]\n",
            "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 16.3MB/s]\n",
            "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 12.3MB/s]\n",
            "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 19.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "data_args = Namespace(\n",
        "    model_name_or_path=\"facebook/bart-base\",\n",
        "    # model_name_or_path=\"results/\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=256,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='results/bart',\n",
        ")\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "# config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(data_args.model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50268, 768)"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_added_toks = tokenizer.add_tokens(['<PS>'], special_tokens=True) ## this line is updated\n",
        "num_added_toks = tokenizer.add_tokens(['<CTX>'], special_tokens=True) ## this line is updated\n",
        "num_added_toks = tokenizer.add_tokens(['<SEP>'], special_tokens=True) ## this line is updated\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHvWQLxuhgsM"
      },
      "source": [
        "##### Tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ykvNCj60y0eT"
      },
      "outputs": [],
      "source": [
        "def preprocess_bst(examples):\n",
        "    num_entries = len(examples[\"free_messages\"])\n",
        "    persona_pieces = [\n",
        "        f\"<PS> {examples['personas'][0]}\",\n",
        "        f\"<PS> {examples['personas'][1]}\",\n",
        "    ]\n",
        "    if examples['context'] == \"wizard_of_wikipedia\":\n",
        "        additional_context_pieces = [f\"[<CTX> {examples['additional_context']}. <SEP> \"]\n",
        "    else:\n",
        "        additional_context_pieces = [\"<SEP> \"]\n",
        "\n",
        "    previous_utterance_pieces = examples[\"previous_utterance\"]\n",
        "    inputs, labels = [], []\n",
        "    for entry_idx in range(num_entries):\n",
        "        free_message = examples['free_messages'][entry_idx]\n",
        "        guided_message = examples['guided_messages'][entry_idx]\n",
        "\n",
        "        previous_utterance = ' <SEP> '.join(previous_utterance_pieces)\n",
        "        original_context = ' '.join(\n",
        "            persona_pieces + additional_context_pieces\n",
        "        ) + previous_utterance\n",
        "        # Input & Output\n",
        "        text = original_context + ' ' + tokenizer.eos_token + ' ' + free_message\n",
        "        inputs.append(text)\n",
        "        labels.append(guided_message)\n",
        "\n",
        "        previous_utterance_pieces += [\n",
        "            free_message,\n",
        "            guided_message,\n",
        "        ]\n",
        "        # print(\"history: \", text)\n",
        "        # print(\"label: \", guided_message)\n",
        "\n",
        "    inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(labels, max_length=data_args.max_target_length, padding=padding, truncation=True)\n",
        "    \n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
        "    # when we want to ignore padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # ['input_ids', 'attention_mask', 'labels']\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    return concatenated_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WNv_GKSVy0a9"
      },
      "outputs": [],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "print(tokenized_train_dataset)\n",
        "print(tokenized_eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfVI32fzhmV6"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "64cF7tqxy0KF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=30,\n",
        "    predict_with_generate=True, # generation task\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
        "if data_args.pad_to_max_length:\n",
        "    data_collator = default_data_collator\n",
        "else:\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
        "    )\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=tokenized_eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "yLh15B-ny0HZ"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Casual Language Model (CLM) e.g., DialoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_args = Namespace(\n",
        "    # model_name_or_path=\"microsoft/DialoGPT-small\",\n",
        "    model_name_or_path=\"results/personagpt\",\n",
        "    # model_name_or_path=\"gpt2\",\n",
        "    max_length=1000,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='results/dialogpt',\n",
        "    block_size=None,\n",
        ")\n",
        "\n",
        "max_length = data_args.max_length\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(data_args.model_name_or_path, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<|endoftext|>', '<PAD>', '<MASK>']\n",
            "<|endoftext|> 50256\n",
            "<PAD> 50257\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.eos_token, tokenizer.eos_token_id)\n",
        "print(tokenizer.pad_token, tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_bst(examples):\n",
        "    num_entries = len(examples[\"free_messages\"])\n",
        "    persona_pieces = [\n",
        "        f\"<PS> {examples['personas'][0]}\",\n",
        "        f\"<PS> {examples['personas'][1]}\",\n",
        "    ]\n",
        "    if examples['context'] == \"wizard_of_wikipedia\":\n",
        "        additional_context_pieces = [f\"[<CTX> {examples['additional_context']}. <SEP> \"]\n",
        "    else:\n",
        "        additional_context_pieces = [\"<SEP> \"]\n",
        "\n",
        "    previous_utterance_pieces = examples[\"previous_utterance\"]\n",
        "    inputs, labels = [], []\n",
        "    for entry_idx in range(num_entries):\n",
        "        free_message = examples['free_messages'][entry_idx]\n",
        "        guided_message = examples['guided_messages'][entry_idx]\n",
        "\n",
        "        previous_utterance = ' <SEP> '.join(previous_utterance_pieces)\n",
        "        original_context = ' '.join(\n",
        "            persona_pieces + additional_context_pieces\n",
        "        ) + previous_utterance\n",
        "        # Input & Output\n",
        "        text = original_context + ' ' + tokenizer.eos_token + ' ' + free_message\n",
        "        inputs.append(text)\n",
        "        labels.append(guided_message)\n",
        "\n",
        "        previous_utterance_pieces += [\n",
        "            free_message,\n",
        "            guided_message,\n",
        "        ]\n",
        "        # print(\"history: \", text)\n",
        "        # print(\"label: \", guided_message)\n",
        "\n",
        "    inputs = tokenizer(inputs, max_length=data_args.max_length, padding=padding, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(labels, max_length=data_args.max_length, padding=padding, truncation=True)\n",
        "    \n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
        "    # when we want to ignore padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # ['input_ids', 'attention_mask', 'labels']\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    return concatenated_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4819 [00:00<?, ?ex/s]/home/monkey/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "100%|██████████| 4819/4819 [00:04<00:00, 1077.63ex/s]\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.70ba/s]\n",
            "100%|██████████| 1009/1009 [00:01<00:00, 811.27ex/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.91ba/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 27018\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 5651\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "batched_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "batched_eval_dataset = tokenized_eval_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "print(batched_train_dataset)\n",
        "print(batched_eval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=30,\n",
        ")\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    if isinstance(logits, tuple):\n",
        "        # Depending on the model and config, logits may contain extra tensors,\n",
        "        # like past_key_values, but logits always come first\n",
        "        logits = logits[0]\n",
        "    return logits.argmax(dim=-1)\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics if training_args.do_eval else None,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGhuwycqpe6V"
      },
      "source": [
        "### Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 27.1MB/s]                    \n",
            "Can not find mwt: default from official model list. Ignoring it.\n",
            "Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "| pos       | combined |\n",
            "| lemma     | combined |\n",
            "========================\n",
            "\n",
            "Use device: gpu\n",
            "Loading: tokenize\n",
            "Loading: pos\n",
            "Loading: lemma\n",
            "Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "from DG_dataset import DGDataset\n",
        "from attacker.DGSlow import StructureAttacker\n",
        "\n",
        "device = torch.device('cpu')\n",
        "instance = test_dataset[0]\n",
        "entry_idx = 0\n",
        "task = \"clm\"\n",
        "sp_token = '<SEP>' # for clm\n",
        "data_name = 'blended_skill_talk'\n",
        "max_length = 128\n",
        "num_beams = 4\n",
        "num_beam_groups = 1\n",
        "max_per = 1 # number of perturbations\n",
        "model_path = 'results/dialogpt'\n",
        "config = AutoConfig.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, config=config)\n",
        "\n",
        "dg = DGDataset(\n",
        "    dataset=data_name,\n",
        "    task=task,\n",
        "    tokenizer=tokenizer,\n",
        "    max_source_length=max_length,\n",
        "    max_target_length=max_length,\n",
        ")\n",
        "attacker = StructureAttacker(\n",
        "    device=device,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    max_len=max_length,\n",
        "    max_per=max_per,\n",
        "    task=task,\n",
        "    use_combined_loss=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jAVngC49o36c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C-- Wow, I am never shy. Do you have anxiety? Yes. I end up sweating and blushing and feel like i'm going to throw up.\n",
            "U--and why is that?\n",
            "G--I feel like I'm going to vomit. thar is a good reason to not drink!  is a good reason to not drink!  is a good reason to not drink!\n"
          ]
        }
      ],
      "source": [
        "# Original generation\n",
        "num_entries, total_entries, context, prev_utt_pc = dg.prepare_context(instance)\n",
        "free_msg, guided_msg, orig_context, references = dg.prepare_entry(\n",
        "    instance, \n",
        "    entry_idx, \n",
        "    context, \n",
        "    prev_utt_pc,\n",
        "    total_entries,\n",
        ")\n",
        "\n",
        "# Original generation\n",
        "text = orig_context + sp_token + free_msg\n",
        "print(\"C--{}\".format(orig_context))\n",
        "print(\"U--{}\".format(free_msg))\n",
        "effective_text = text + tokenizer.eos_token # for clm\n",
        "# effective_text = text\n",
        "inputs = tokenizer(\n",
        "    effective_text,  \n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        ")\n",
        "input_ids = inputs.input_ids\n",
        "with torch.no_grad():\n",
        "    outputs = dialogue(\n",
        "        model, \n",
        "        input_ids,\n",
        "        early_stopping=False, \n",
        "        num_beams=num_beams,\n",
        "        num_beam_groups=num_beam_groups, \n",
        "        use_cache=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "output = tokenizer.batch_decode(\n",
        "    outputs['sequences'][:, input_ids.shape[-1]:], \n",
        "    skip_special_tokens=True,\n",
        ")[0]\n",
        "print(\"G--{}\".format(output))\n",
        "\n",
        "# # Attack\n",
        "# success, adv_his = attacker.run_attack(text, guided_msg)\n",
        "# new_text = adv_his[-1][0]\n",
        "# new_free_msg= new_text.split(sp_token)[1].strip()\n",
        "# print(\"U'--{}\".format(new_free_msg))\n",
        "# cos_sim = attacker.sent_encoder.get_sim(new_free_msg, free_msg)\n",
        "\n",
        "# effective_text = new_text + tokenizer.eos_token # for clm\n",
        "# inputs = tokenizer(\n",
        "#     effective_text,  \n",
        "#     return_tensors=\"pt\",\n",
        "#     truncation=True,\n",
        "#     max_length=max_length,\n",
        "# )\n",
        "# input_ids = inputs.input_ids\n",
        "# with torch.no_grad():\n",
        "#     outputs = dialogue(\n",
        "#         model, \n",
        "#         input_ids,\n",
        "#         early_stopping=False, \n",
        "#         num_beams=num_beams,\n",
        "#         num_beam_groups=num_beam_groups, \n",
        "#         use_cache=True,\n",
        "#         max_length=max_length,\n",
        "#     )\n",
        "\n",
        "# new_output = tokenizer.batch_decode(\n",
        "#     outputs['sequences'][:, input_ids.shape[-1]:], \n",
        "#     skip_special_tokens=True,\n",
        "# )[0]\n",
        "# print(\"G'--{}\".format(output))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ConvAI2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset conv_ai_2 (/home/monkey/.cache/huggingface/datasets/conv_ai_2/conv_ai_2/1.0.0/11d600ddce66bb9d07ca50d1b55b488145ef0d5d0206168c32f1043677875865)\n",
            "100%|██████████| 1/1 [00:00<00:00, 280.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialog_id', 'dialog', 'bot_profile', 'user_profile', 'eval_score', 'profile_match'],\n",
            "        num_rows: 3495\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"conv_ai_2\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for i in range(1):\n",
        "#     print(dataset['train'][i])\n",
        "#     instance = dataset['train'][i]\n",
        "#     user_profile = ' '.join([''.join(x) for x in instance['user_profile']])\n",
        "#     print('user profile: ', user_profile)\n",
        "\n",
        "#     persona_pieces = f\"<PS> {user_profile}\"\n",
        "#     num_entries = len([x for x in instance['dialog'] if x['sender_class'] == 'Human'])\n",
        "#     previous_utterance_pieces = [persona_pieces]\n",
        "\n",
        "#     for entry_idx in range(num_entries):\n",
        "#         bot_msg = instance['dialog'][entry_idx*2]['text']\n",
        "#         human_msg = instance['dialog'][entry_idx*2+1]['text']\n",
        "#         original_context = ' '.join(previous_utterance_pieces)\n",
        "#         previous_utterance_pieces += [\n",
        "#             bot_msg,\n",
        "#             human_msg,\n",
        "#         ]\n",
        "\n",
        "#         text = original_context + '<EOS>' + bot_msg + '<EOS>' + human_msg\n",
        "#         print(\"text: \", text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Empathetic Dialogues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset empathetic_dialogues (/home/monkey/.cache/huggingface/datasets/empathetic_dialogues/default/0.1.0/09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf)\n",
            "100%|██████████| 3/3 [00:00<00:00, 47.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 76673\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 12030\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 10943\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"empathetic_dialogues\")\n",
        "print(dataset)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def group_texts(dataset):\n",
        "    results = {\n",
        "        'conv_id': [], \n",
        "        'prompt': [],\n",
        "        'dialog': [], \n",
        "        'context': [],\n",
        "    }\n",
        "    for i, instance in enumerate(dataset):\n",
        "        if instance['utterance_idx'] == 1:\n",
        "            results['conv_id'].append(instance['conv_id'])\n",
        "            results['dialog'].append([])\n",
        "            results['prompt'].append(instance['prompt'])\n",
        "            results['context'].append(instance['context'])\n",
        "\n",
        "        response = {'text': instance['utterance'], 'speaker_idx': instance['speaker_idx']}\n",
        "        results['dialog'][-1].append(response)\n",
        "\n",
        "    return Dataset.from_dict(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conv_id', 'prompt', 'dialog', 'context'],\n",
              "    num_rows: 2541\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_test_dataset = group_texts(test_dataset)\n",
        "print(grouped_test_dataset[0])\n",
        "grouped_test_dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PersonaChat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration AlekseyKorshuk--persona-chat-2e840579d6707f7b\n",
            "Found cached dataset parquet (/home/monkey/.cache/huggingface/datasets/AlekseyKorshuk___parquet/AlekseyKorshuk--persona-chat-2e840579d6707f7b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|██████████| 2/2 [00:00<00:00, 225.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    validation: Dataset({\n",
            "        features: ['personality', 'utterances'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['personality', 'utterances'],\n",
            "        num_rows: 17878\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"AlekseyKorshuk/persona-chat\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(dataset['train'].column_names)\n",
        "# print(dataset['train'][0]['personality'])\n",
        "# print(dataset['train'][0]['utterances'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bleu': 0.0, 'precisions': [0.35714285714285715, 0.07692307692307693, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 14, 'reference_length': 12}\n"
          ]
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "bleu = evaluate.load('bleu')\n",
        "predictions = ['Yes, I have two daughters. I am a grandparent at 44.'.lower()]\n",
        "references = [[\"yes i have a son and just recently i became a grandpa\".lower()]]\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wHvWQLxuhgsM",
        "SfVI32fzhmV6"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ed07da37284b78ddf1027a77eacdaaa03fe44cd85ed96169aaea4ffb3c093db"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "004738fd62854848a7676e3100693a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37c18f11be549a8834c745a960807f3",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9301ea70fc4a6691af00b5d43321f4",
            "value": "100%"
          }
        },
        "01bdfd23d64049638a2e629d093ee7bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be25e0afa9c405fa3b7089fe6aa6ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d8e725392444308b0c8233f62d9ec64",
            "placeholder": "​",
            "style": "IPY_MODEL_3c2deb172de24560b57a5a548acfe53b",
            "value": " 3/3 [00:00&lt;00:00, 82.94it/s]"
          }
        },
        "213f6810504c44b3ba261a5eaea26b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8ec86383e14aa7ad58304e64546741",
            "placeholder": "​",
            "style": "IPY_MODEL_428f603e389347229be629f3c16b29d9",
            "value": "100%"
          }
        },
        "26a6adaecf3b462186fa09311f32a4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a90ce1d47e243e1a574e923098867f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01bdfd23d64049638a2e629d093ee7bb",
            "placeholder": "​",
            "style": "IPY_MODEL_40b233347a504c5699577a4f94b91db1",
            "value": " 3/3 [00:00&lt;00:00, 89.30it/s]"
          }
        },
        "36d9fd106ff240c8b0fb971e5c8efb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c2deb172de24560b57a5a548acfe53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b233347a504c5699577a4f94b91db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "428f603e389347229be629f3c16b29d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d8e725392444308b0c8233f62d9ec64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7170283bda7045e283e036ccaeffdcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_004738fd62854848a7676e3100693a1b",
              "IPY_MODEL_df68b3c7f37d4821bff89b0e1cd082bb",
              "IPY_MODEL_1be25e0afa9c405fa3b7089fe6aa6ba7"
            ],
            "layout": "IPY_MODEL_7c8a70a52cb64d8eb765ee4cd5df30ec"
          }
        },
        "7c8a70a52cb64d8eb765ee4cd5df30ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84315fa7f06c46278ea553e836ce60e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_213f6810504c44b3ba261a5eaea26b82",
              "IPY_MODEL_f985e336fb6b46c094cadb43bdbc0e4a",
              "IPY_MODEL_2a90ce1d47e243e1a574e923098867f3"
            ],
            "layout": "IPY_MODEL_26a6adaecf3b462186fa09311f32a4d1"
          }
        },
        "91ecbb6978664e289607aac1db2e4166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d8ec86383e14aa7ad58304e64546741": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40f1faa4ea44197be87ef6f6bc63c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37c18f11be549a8834c745a960807f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df68b3c7f37d4821bff89b0e1cd082bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5f96869232484282b47280a9f0639c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d9fd106ff240c8b0fb971e5c8efb3b",
            "value": 3
          }
        },
        "ed5f96869232484282b47280a9f0639c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f985e336fb6b46c094cadb43bdbc0e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40f1faa4ea44197be87ef6f6bc63c4e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ecbb6978664e289607aac1db2e4166",
            "value": 3
          }
        },
        "fe9301ea70fc4a6691af00b5d43321f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
