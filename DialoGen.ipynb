{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUd3jJg_egjk",
        "outputId": "a3588f7e-728a-46a4-9feb-2f16f0ead794"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.dont_write_bytecode = True\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from itertools import chain\n",
        "import language_tool_python\n",
        "from argparse import Namespace\n",
        "from datasets import load_dataset, load_metric, DatasetDict, Dataset\n",
        "from transformers import (\n",
        "    AutoConfig, \n",
        "    AutoTokenizer, \n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from typing import *\n",
        "from DialogueAPI import dialogue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXWXvwRJjpBA"
      },
      "source": [
        "## Blended_Skill_Talk Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "7170283bda7045e283e036ccaeffdcf2",
            "004738fd62854848a7676e3100693a1b",
            "df68b3c7f37d4821bff89b0e1cd082bb",
            "1be25e0afa9c405fa3b7089fe6aa6ba7",
            "7c8a70a52cb64d8eb765ee4cd5df30ec",
            "d37c18f11be549a8834c745a960807f3",
            "fe9301ea70fc4a6691af00b5d43321f4",
            "ed5f96869232484282b47280a9f0639c",
            "36d9fd106ff240c8b0fb971e5c8efb3b",
            "6d8e725392444308b0c8233f62d9ec64",
            "3c2deb172de24560b57a5a548acfe53b"
          ]
        },
        "id": "qIXJ_oTvGRcB",
        "outputId": "829e202a-f7cc-4849-c194-50ae9afa91e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset blended_skill_talk (/home/monkey/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/8544e13cbbf2fb9b34157f2e2f28c1539e4f36bf0ef2bd96edd138b4000c5ca1)\n",
            "100%|██████████| 3/3 [00:00<00:00, 153.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 4819\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 1009\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 980\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "train_dataset = bst_dataset['train']\n",
        "eval_dataset = bst_dataset['validation']\n",
        "test_dataset = bst_dataset['test']\n",
        "print(bst_dataset)\n",
        "# print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOTNHI1jq5W1",
        "outputId": "88cba86e-ddf9-4eea-ee9b-9a2cdafabdb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#pairs of training dialogues: 27018, validation dialogues: 5651, test dialogues: 5482\n"
          ]
        }
      ],
      "source": [
        "# Get statistics of pair of dialogues \n",
        "train_num, eval_num, test_num = 0, 0, 0\n",
        "for i, instance in enumerate(train_dataset):\n",
        "    train_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(eval_dataset):\n",
        "    eval_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(test_dataset):\n",
        "    test_num += len(instance['free_messages'])\n",
        "\n",
        "print(\"#pairs of training dialogues: {}, validation dialogues: {}, test dialogues: {}\".format(\n",
        "    train_num, eval_num, test_num,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xnYGvfspczn",
        "outputId": "f17f5947-5fb2-4902-96e1-021efe8dfc0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "personas: ['i hate talking to people.', 'i believe dragons are real.']\n",
            "additional_context: Social anxiety\n",
            "previous_utterance: ['Wow, I am never shy. Do you have anxiety?', \"Yes. I end up sweating and blushing and feel like i'm going to throw up.\"]\n",
            "context: wizard_of_wikipedia\n",
            "free_messages: ['and why is that?', 'interesting but I know how you feel especially the whole people telling that it in your head ', \"Dang that's though. But I also understand that. I have people some who talks behind my back because of certain things that I believe in \"]\n",
            "guided_messages: [\"I think it's because in my head, I think everyone is judging me. I just start to sweat and I get sick in my stomach.\", \"I don't really have people telling me in my head, more like behind my back\", 'Me too! What do you believe in? I believe in dragons... Just finished watching Game of Thrones. Man, those things are dope']\n",
            "suggestions: {'convai2': [\"i've no idea i am also very shy\", 'oh i know . i always feel judged and never know what to do .', 'i try to do stuff like that all time but i can never speak up for myself'], 'empathetic_dialogues': ['Probably because I am insecure.', \"Please don't care about those people. Try to be true to yourself. World will recognize you one day.\", 'I am not sure I believe them most of the time though'], 'wizard_of_wikipedia': [\"I think it's because in my head, I think everyone is judging me. I just start to sweat and I get sick in my stomach.\", 'Right! But the difference with me is I just think about myself and no one else really. People consider it a social and cultural problem.', 'Shyness is weird too, that awkwardness breeds more shyness I feel and just makes for a positive feedback']}\n",
            "guided_chosen_suggestions: ['wizard_of_wikipedia', '', '']\n"
          ]
        }
      ],
      "source": [
        "# Show examples\n",
        "for i, instance in enumerate(test_dataset.select(range(1))):\n",
        "    for key, value in instance.items():\n",
        "        if key != 'label_candidates':\n",
        "            print(\"{}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M8sPchPi4zW3"
      },
      "outputs": [],
      "source": [
        "data_args = Namespace(\n",
        "    model_name_or_path=\"facebook/bart-base\",\n",
        "    # model_name_or_path=\"results/\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=256,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='results/bart',\n",
        ")\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(data_args.model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50268, 768)"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_added_toks = tokenizer.add_tokens(['<PS>'], special_tokens=True) ## this line is updated\n",
        "num_added_toks = tokenizer.add_tokens(['<CTX>'], special_tokens=True) ## this line is updated\n",
        "num_added_toks = tokenizer.add_tokens(['<SEP>'], special_tokens=True) ## this line is updated\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHvWQLxuhgsM"
      },
      "source": [
        "##### Tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ykvNCj60y0eT"
      },
      "outputs": [],
      "source": [
        "def preprocess_bst(examples):\n",
        "    num_entries = len(examples[\"free_messages\"])\n",
        "    persona_pieces = [\n",
        "        f\"<PS> {examples['personas'][0]}\",\n",
        "        f\"<PS> {examples['personas'][1]}\",\n",
        "    ]\n",
        "    if examples['context'] == \"wizard_of_wikipedia\":\n",
        "        additional_context_pieces = [f\"[<CTX> {examples['additional_context']}. <SEP> \"]\n",
        "    else:\n",
        "        additional_context_pieces = [\"<SEP> \"]\n",
        "\n",
        "    previous_utterance_pieces = examples[\"previous_utterance\"]\n",
        "    inputs, labels = [], []\n",
        "    for entry_idx in range(num_entries):\n",
        "        free_message = examples['free_messages'][entry_idx]\n",
        "        guided_message = examples['guided_messages'][entry_idx]\n",
        "\n",
        "        previous_utterance = ' <SEP> '.join(previous_utterance_pieces)\n",
        "        original_context = ' '.join(\n",
        "            persona_pieces + additional_context_pieces\n",
        "        ) + previous_utterance\n",
        "        # Input & Output\n",
        "        text = original_context + ' ' + tokenizer.eos_token + ' ' + free_message\n",
        "        inputs.append(text)\n",
        "        labels.append(guided_message)\n",
        "\n",
        "        previous_utterance_pieces += [\n",
        "            free_message,\n",
        "            guided_message,\n",
        "        ]\n",
        "        # print(\"history: \", text)\n",
        "        # print(\"label: \", guided_message)\n",
        "\n",
        "    inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(labels, max_length=data_args.max_target_length, padding=padding, truncation=True)\n",
        "    \n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
        "    # when we want to ignore padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # ['input_ids', 'attention_mask', 'labels']\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    return concatenated_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WNv_GKSVy0a9"
      },
      "outputs": [],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "print(tokenized_train_dataset)\n",
        "print(tokenized_eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfVI32fzhmV6"
      },
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "64cF7tqxy0KF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=30,\n",
        "    predict_with_generate=True, # generation task\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
        "if data_args.pad_to_max_length:\n",
        "    data_collator = default_data_collator\n",
        "else:\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
        "    )\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=tokenized_eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "yLh15B-ny0HZ"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Casual Language Model (CLM) e.g., DialoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_args = Namespace(\n",
        "    # model_name_or_path=\"microsoft/DialoGPT-small\",\n",
        "    model_name_or_path=\"results/dialogpt\",\n",
        "    max_length=1000,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='results/dialogpt',\n",
        "    block_size=None,\n",
        ")\n",
        "\n",
        "max_length = data_args.max_length\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(data_args.model_name_or_path, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_bst(examples):\n",
        "    num_entries = len(examples[\"free_messages\"])\n",
        "    persona_pieces = [\n",
        "        f\"<PS> {examples['personas'][0]}\",\n",
        "        f\"<PS> {examples['personas'][1]}\",\n",
        "    ]\n",
        "    if examples['context'] == \"wizard_of_wikipedia\":\n",
        "        additional_context_pieces = [f\"[<CTX> {examples['additional_context']}. <SEP> \"]\n",
        "    else:\n",
        "        additional_context_pieces = [\"<SEP> \"]\n",
        "\n",
        "    previous_utterance_pieces = examples[\"previous_utterance\"]\n",
        "    inputs, labels = [], []\n",
        "    for entry_idx in range(num_entries):\n",
        "        free_message = examples['free_messages'][entry_idx]\n",
        "        guided_message = examples['guided_messages'][entry_idx]\n",
        "\n",
        "        previous_utterance = ' <SEP> '.join(previous_utterance_pieces)\n",
        "        original_context = ' '.join(\n",
        "            persona_pieces + additional_context_pieces\n",
        "        ) + previous_utterance\n",
        "        # Input & Output\n",
        "        text = original_context + ' ' + tokenizer.eos_token + ' ' + free_message\n",
        "        inputs.append(text)\n",
        "        labels.append(guided_message)\n",
        "\n",
        "        previous_utterance_pieces += [\n",
        "            free_message,\n",
        "            guided_message,\n",
        "        ]\n",
        "        # print(\"history: \", text)\n",
        "        # print(\"label: \", guided_message)\n",
        "\n",
        "    inputs = tokenizer(inputs, max_length=data_args.max_length, padding=padding, truncation=True)\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(labels, max_length=data_args.max_length, padding=padding, truncation=True)\n",
        "    \n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 \n",
        "    # when we want to ignore padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    # ['input_ids', 'attention_mask', 'labels']\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    return concatenated_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4819 [00:00<?, ?ex/s]/home/monkey/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "100%|██████████| 4819/4819 [00:04<00:00, 1077.63ex/s]\n",
            "100%|██████████| 5/5 [00:02<00:00,  1.70ba/s]\n",
            "100%|██████████| 1009/1009 [00:01<00:00, 811.27ex/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.91ba/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 27018\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 5651\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "batched_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "tokenized_eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=False,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "batched_eval_dataset = tokenized_eval_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "print(batched_train_dataset)\n",
        "print(batched_eval_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=data_args.output_dir,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True, # smaller eval loss is better\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    gradient_accumulation_steps=20,\n",
        "    num_train_epochs=30,\n",
        ")\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    if isinstance(logits, tuple):\n",
        "        # Depending on the model and config, logits may contain extra tensors,\n",
        "        # like past_key_values, but logits always come first\n",
        "        logits = logits[0]\n",
        "    return logits.argmax(dim=-1)\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=default_data_collator,\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics if training_args.do_eval else None,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kyn42UoVJ5"
      },
      "source": [
        "### Attack methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0GNLfbkioZwO"
      },
      "outputs": [],
      "source": [
        "class BaseAttacker:\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "      \n",
        "        self.device = device\n",
        "        self.model = model.to(self.device)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.embedding = self.model.get_input_embeddings().weight\n",
        "        self.specical_token = self.tokenizer.all_special_tokens\n",
        "        self.specical_id = self.tokenizer.all_special_ids\n",
        "        self.eos_token_id = self.model.config.eos_token_id\n",
        "        self.pad_token_id = self.model.config.pad_token_id\n",
        "        self.num_beams = self.model.config.num_beams\n",
        "        self.num_beam_groups = self.model.config.num_beam_groups\n",
        "        self.max_len = max_len\n",
        "        self.max_per = max_per\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    @classmethod\n",
        "    def _get_hparam(cls, namespace: Namespace, key: str, default=None):\n",
        "        if hasattr(namespace, key):\n",
        "            return getattr(namespace, key)\n",
        "        print('Using default argument for \"{}\"'.format(key))\n",
        "\n",
        "        return default\n",
        "\n",
        "    def run_attack(self, x):\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, x):\n",
        "        pass\n",
        "\n",
        "    def compute_seq_len(self, seq):\n",
        "        if seq[0].eq(self.pad_token_id):\n",
        "            return int(len(seq) - sum(seq.eq(self.pad_token_id)))\n",
        "        else:\n",
        "            return int(len(seq) - sum(seq.eq(self.pad_token_id))) - 1\n",
        "\n",
        "    def get_prediction(self, sentence):\n",
        "        def remove_pad(s):\n",
        "            for i, tk in enumerate(s):\n",
        "                if tk == self.eos_token_id and i != 0:\n",
        "                    return s[:i + 1]\n",
        "            return s\n",
        "\n",
        "        input_ids = self.tokenizer(sentence, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "        \n",
        "        # ['sequences', 'sequences_scores', 'scores', 'beam_indices'] if num_beams != 1\n",
        "        # ['sequences', 'scores'] if num_beams = 1\n",
        "        outputs = dialogue(\n",
        "            self.model, \n",
        "            input_ids,\n",
        "            early_stopping=False, \n",
        "            num_beams=self.num_beams,\n",
        "            num_beam_groups=self.num_beam_groups, \n",
        "            use_cache=True,\n",
        "            max_length=self.max_len,\n",
        "        )\n",
        "        \n",
        "        seqs = outputs['sequences']\n",
        "        seqs = [remove_pad(seq) for seq in seqs]\n",
        "        out_scores = outputs['scores']\n",
        "        pred_len = [self.compute_seq_len(seq) for seq in seqs]\n",
        "        return pred_len, seqs, out_scores\n",
        "\n",
        "    def get_trans_string_len(self, text):\n",
        "        pred_len, seqs, _ = self.get_prediction(text)\n",
        "        return seqs[0], pred_len[0]\n",
        "\n",
        "    def get_trans_len(self, text):\n",
        "        pred_len, _, _ = self.get_prediction(text)\n",
        "        return pred_len\n",
        "\n",
        "    def get_trans_strings(self, text):\n",
        "        pred_len, seqs, _ = self.get_prediction(text)\n",
        "        out_res = [self.tokenizer.decode(seq, skip_special_tokens=True) for seq in seqs]\n",
        "        return out_res, pred_len\n",
        "    \n",
        "    def compute_score(self, text):\n",
        "        batch_size = len(text)\n",
        "        index_list = [i * self.num_beams for i in range(batch_size + 1)]\n",
        "        pred_len, seqs, out_scores = self.get_prediction(text)\n",
        "\n",
        "\n",
        "        scores = [[] for _ in range(batch_size)]\n",
        "        for out_s in out_scores:\n",
        "            for i in range(batch_size):\n",
        "                current_index = index_list[i]\n",
        "                scores[i].append(out_s[current_index: current_index + 1])\n",
        "        scores = [torch.cat(s) for s in scores]\n",
        "        scores = [s[:pred_len[i]] for i, s in enumerate(scores)]\n",
        "        return scores, seqs, pred_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RzfF1r-CoZtg"
      },
      "outputs": [],
      "source": [
        "class SlowAttacker(BaseAttacker):\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "        super(SlowAttacker, self).__init__(device, tokenizer, model, max_len, max_per)\n",
        "\n",
        "    def leave_eos_loss(self, scores, pred_len):\n",
        "        loss = []\n",
        "        for i, s in enumerate(scores):\n",
        "            s[:, self.pad_token_id] = 1e-12 # T X V\n",
        "            eos_p = self.softmax(s)[:pred_len[i], self.eos_token_id]\n",
        "            loss.append(self.bce_loss(eos_p, torch.zeros_like(eos_p)))\n",
        "        return loss\n",
        "\n",
        "    def leave_eos_target_loss(self, scores, seqs, pred_len):\n",
        "        loss = []\n",
        "        for i, s in enumerate(scores): # s: T X V\n",
        "            # if self.pad_token_id != self.eos_token_id:\n",
        "            s[:, self.pad_token_id] = 1e-12\n",
        "            softmax_v = self.softmax(s)\n",
        "            eos_p = softmax_v[:pred_len[i], self.eos_token_id]\n",
        "            target_p = torch.stack([softmax_v[idx, s] for idx, s in enumerate(seqs[i][1:])])\n",
        "            target_p = target_p[:pred_len[i]]\n",
        "            pred = eos_p + target_p\n",
        "            pred[-1] = pred[-1] / 2\n",
        "            loss.append(self.bce_loss(pred, torch.zeros_like(pred)))\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def select_best(self, new_strings, batch_size=30):\n",
        "        \"\"\"\n",
        "        Select generated strings which induce longest output sentences.\n",
        "        \"\"\"\n",
        "        pred_len = []\n",
        "        # seqs = []\n",
        "        batch_num = len(new_strings) // batch_size\n",
        "        if batch_size * batch_num != len(new_strings):\n",
        "            batch_num += 1\n",
        "\n",
        "        for i in range(batch_num):\n",
        "            st, ed = i * batch_size, min(i * batch_size + batch_size, len(new_strings))\n",
        "            input_ids = self.tokenizer(new_strings[st:ed], return_tensors=\"pt\", padding=True).input_ids\n",
        "            input_ids = input_ids.to(self.device)\n",
        "            outputs = self.model.generate(\n",
        "                input_ids, \n",
        "                num_beams=self.num_beams, \n",
        "                max_length=self.max_len,\n",
        "                return_dict_in_generate=True,\n",
        "            )\n",
        "            lengths = [self.compute_seq_len(seq) for seq in outputs['sequences']]\n",
        "            # pdb.set_trace()\n",
        "            pred_len.extend(lengths)\n",
        "            \n",
        "        # pred_len = np.array([self.compute_seq_len(torch.tensor(seq)) for seq in seqs])\n",
        "        pred_len = np.array(pred_len)\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        assert len(new_strings) == len(pred_len)\n",
        "        return new_strings[pred_len.argmax()], max(pred_len)\n",
        "\n",
        "    def prepare_attack(self, text):\n",
        "        ori_len = self.get_trans_len(text)[0] # original sentence length\n",
        "        best_adv_text, best_len = deepcopy(text), ori_len\n",
        "        current_adv_text, current_len = deepcopy(text), ori_len  # current_adv_text: List[str]\n",
        "        return ori_len, (best_adv_text, best_len), (current_adv_text, current_len)\n",
        "\n",
        "    def compute_loss(self, text):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def mutation(self, current_adv_text, grad, modified_pos):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run_attack(self, text):\n",
        "        \"\"\"\n",
        "        (1) Using gradient ascent to generate adversarial sentences -- mutation();\n",
        "        (2) Select the best samples which induce longest output sentences -- select_best();\n",
        "        (3) Save the adversarial samples -- adv_his.\n",
        "        \"\"\"\n",
        "        assert len(text) != 1\n",
        "        # torch.autograd.set_detect_anomaly(True)\n",
        "        ori_len, (best_adv_text, best_len), (current_adv_text, current_len) = self.prepare_attack(text)\n",
        "        # adv_his = [(deepcopy(current_adv_text), deepcopy(current_len), 0.0)]\n",
        "        adv_his = []\n",
        "        modify_pos = []\n",
        "        pbar = tqdm(range(self.max_per))\n",
        "        t1 = time.time()\n",
        "\n",
        "        for it in pbar:\n",
        "            loss_list = self.compute_loss([current_adv_text])\n",
        "            loss = sum(loss_list)\n",
        "            self.model.zero_grad()\n",
        "            loss.backward()\n",
        "            grad = self.embedding.grad\n",
        "            new_strings = self.mutation(current_adv_text, grad, modify_pos)\n",
        "\n",
        "            if new_strings:\n",
        "                current_adv_text, current_len = self.select_best(new_strings)\n",
        "                log_str = \"%d, %d, %.2f\" % (it, len(new_strings), best_len / ori_len)\n",
        "                pbar.set_description(log_str)\n",
        "\n",
        "                if current_len > best_len:\n",
        "                    best_adv_text = deepcopy(current_adv_text)\n",
        "                    best_len = current_len\n",
        "                t2 = time.time()\n",
        "                adv_his.append((best_adv_text, int(best_len), t2 - t1))\n",
        "\n",
        "        if adv_his:\n",
        "            return True, adv_his\n",
        "        else:\n",
        "            return False, [(deepcopy(current_adv_text), deepcopy(current_len), 0.0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZkV1HKfloZmK"
      },
      "outputs": [],
      "source": [
        "class WordAttacker(SlowAttacker):\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "        super(WordAttacker, self).__init__(device, tokenizer, model, max_len, max_per)\n",
        "\n",
        "    def compute_loss(self, text):\n",
        "        scores, seqs, pred_len = self.compute_score(text) # [T X V], [T], [1]\n",
        "        loss_list = self.leave_eos_target_loss(scores, seqs, pred_len)\n",
        "        # loss_list = self.leave_eos_loss(scores, pred_len)\n",
        "        return loss_list\n",
        "    \n",
        "\n",
        "    def token_replace_mutation(self, current_adv_text, grad, modified_pos):\n",
        "        new_strings = []\n",
        "        current_ids = self.tokenizer(current_adv_text, return_tensors=\"pt\", padding=True).input_ids[0]\n",
        "        base_ids = current_ids.clone()\n",
        "        for pos in modified_pos:\n",
        "            t = current_ids[0][pos]\n",
        "            grad_t = grad[t]\n",
        "            score = (self.embedding - self.embedding[t]).mm(grad_t.reshape([-1, 1])).reshape([-1])\n",
        "            index = score.argsort()\n",
        "            for tgt_t in index:\n",
        "                if tgt_t not in self.specical_token:\n",
        "                    base_ids[pos] = tgt_t\n",
        "                    break\n",
        "\n",
        "        for pos, t in enumerate(current_ids):\n",
        "            if t not in self.specical_id:\n",
        "                cnt, grad_t = 0, grad[t]\n",
        "                score = (self.embedding - self.embedding[t]).mm(grad_t.reshape([-1, 1])).reshape([-1])\n",
        "                index = score.argsort()\n",
        "                for tgt_t in index:\n",
        "                    if tgt_t not in self.specical_token:\n",
        "                        new_base_ids = base_ids.clone()\n",
        "                        new_base_ids[pos] = tgt_t\n",
        "                        candidate_s = self.tokenizer.decode(new_base_ids, skip_special_tokens=True)\n",
        "                        new_strings.append(candidate_s)\n",
        "                        cnt += 1\n",
        "                        if cnt >= 50:\n",
        "                            break\n",
        "\n",
        "        return new_strings\n",
        "\n",
        "\n",
        "    def mutation(self, current_adv_text, grad, modify_pos):\n",
        "        new_strings = self.token_replace_mutation(current_adv_text, grad, modify_pos)\n",
        "        return new_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GnXnWV4o4e2"
      },
      "source": [
        "### Inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o-edv1Ybo3-D"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(preds, labels, metric, tokenizer):\n",
        "    if not isinstance(preds, list):\n",
        "        preds = [preds]\n",
        "    if not isinstance(labels, list):\n",
        "        labels = [labels]\n",
        "    preds, labels = postprocess_text(preds, labels)\n",
        "    result = metric.compute(predictions=preds, references=labels)\n",
        "    return result['score']\n",
        "\n",
        "\n",
        "def inference(sentence, label, model, tokenizer, metric, device):\n",
        "    input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "    t1 = time.time()\n",
        "    outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    t2 = time.time()\n",
        "    prediction_len = len(output.split())\n",
        "    eval_score = compute_metrics(output, label, metric, tokenizer)\n",
        "\n",
        "    print(\"\")\n",
        "    success, adv_his = attacker.run_attack(sentence)\n",
        "    print(\"\\nU--{}\".format(sentence))\n",
        "    print(\"G--{}\".format(output))\n",
        "    print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(\n",
        "        prediction_len, t2-t1, eval_score,\n",
        "    ))\n",
        "\n",
        "    if success:\n",
        "        print(\"U'--{}\".format(adv_his[-1][0]))\n",
        "    else:\n",
        "        print(\"Attack failed!\")\n",
        "\n",
        "    input_ids = tokenizer(adv_his[-1][0], return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "    t1 = time.time()\n",
        "    outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    t2 = time.time()\n",
        "    prediction_len = len(output.split())\n",
        "    print(\"G'--{}\".format(output))\n",
        "    eval_score = compute_metrics(output, label, metric, tokenizer)\n",
        "    print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(\n",
        "        prediction_len, t2-t1, eval_score,\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGhuwycqpe6V"
      },
      "source": [
        "### Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAVngC49o36c"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "attacker = WordAttacker(\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        max_len=64,\n",
        "        max_per=1,\n",
        "    )\n",
        "\n",
        "metric = load_metric(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OF2USA2kXUS",
        "outputId": "50058530-ce29-4dd5-8f67-a2cbbfb86fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 500, 1.00: 100%|██████████| 1/1 [00:12<00:00, 12.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--Can't believe the kid grew up so quick.\n",
            "G--I can't believe it. He's only a few months old.\n",
            "(length: 10, latency: 0.189, BLEU: 3.386)\n",
            "U'--Can Directions believe the kid grew up so quick.\n",
            "G'--I can't believe it. My dad was an Army brat and he always said he wanted to be a pilot.\n",
            "(length: 20, latency: 0.277, BLEU: 1.727)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 350, 1.00: 100%|██████████| 1/1 [00:11<00:00, 11.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--How would I start rock climbing?\n",
            "G--If you know how to climb, you can google it.\n",
            "(length: 10, latency: 0.211, BLEU: 12.046)\n",
            "U'--How would I start rock waterfall?\n",
            "G'--There are many ways to start a rock waterfall. There are many different types of water based on rock factors such as salinity, temperate, etc.\n",
            "(length: 25, latency: 0.352, BLEU: 1.820)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 350, 1.00: 100%|██████████| 1/1 [00:09<00:00,  9.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--How often do you use computers?\n",
            "G--Almost every week ends.\n",
            "(length: 4, latency: 0.113, BLEU: 8.627)\n",
            "U'--How sort do you use computers?\n",
            "G'--I have a couple of computers. One is a gaming rig and the other is a storage unit.\n",
            "(length: 18, latency: 0.235, BLEU: 2.708)\n"
          ]
        }
      ],
      "source": [
        "# Demo 1\n",
        "input_text = \"Can't believe the kid grew up so quick.\"\n",
        "output_text = \"Yeah, kids grow up so quickly.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)\n",
        "\n",
        "# Demo 2\n",
        "input_text = \"How would I start rock climbing?\"\n",
        "output_text = \"You can google it. But I suggest you to find a local climbing gym and take a class.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)\n",
        "\n",
        "# Demo 3\n",
        "input_text = \"How often do you use computers?\"\n",
        "output_text = \"Almost every week. I use them for work and personal use.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YEeglOfRpzba"
      },
      "outputs": [],
      "source": [
        "# Demo on BST test set\n",
        "import random\n",
        "\n",
        "def test_demo(device, model, tokenizer, attacker, max_num_samples=100, max_per=3):\n",
        "    random.seed(2019)\n",
        "    bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "    test_dataset = bst_dataset['test']\n",
        "    ids = random.sample(range(len(test_dataset)), max_num_samples)\n",
        "\n",
        "    sampled_test_dataset = test_dataset.select(ids)\n",
        "\n",
        "    metric = load_metric(\"sacrebleu\")\n",
        "    ori_lens, adv_lens = [], []\n",
        "    ori_bleus, adv_bleus = [], []\n",
        "    ori_time, adv_time = [], []\n",
        "    att_success = 0\n",
        "    total_pairs = 0\n",
        "\n",
        "    for i, instance in tqdm(enumerate(sampled_test_dataset)):\n",
        "        if total_pairs >= max_num_samples:\n",
        "            break\n",
        "\n",
        "        for (sentence, label) in zip(instance['free_messages'], instance['guided_messages']):\n",
        "\n",
        "            input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "            input_ids = input_ids.to(device)\n",
        "            t1 = time.time()\n",
        "            \n",
        "            outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "            output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            t2 = time.time()\n",
        "            pred_len = np.count_nonzero(outputs[0].cpu() != tokenizer.pad_token_id)\n",
        "            eval_scores = compute_metrics(output, label, metric, tokenizer)\n",
        "            \n",
        "            ori_lens.append(pred_len)\n",
        "            ori_bleus.append(eval_scores)\n",
        "            ori_time.append(t2-t1)\n",
        "            \n",
        "            # Attack\n",
        "            print(\"\")\n",
        "            success, adv_his = attacker.run_attack(sentence)\n",
        "            print('\\n')\n",
        "            print(\"U--{}\".format(sentence))\n",
        "            print(\"G--{}\".format(output))\n",
        "            print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(pred_len, t2-t1, eval_scores))\n",
        "\n",
        "            if success:\n",
        "                # print(\"Attack Succeed!\")\n",
        "                print(\"U'--{}\".format(adv_his[-1][0]))\n",
        "            else:\n",
        "                print(\"Attack failed!\")\n",
        "\n",
        "            input_ids = tokenizer(adv_his[-1][0], return_tensors=\"pt\").input_ids\n",
        "            input_ids = input_ids.to(device)\n",
        "            t1 = time.time()\n",
        "            outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "            output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            t2 = time.time()\n",
        "            adv_pred_len = np.count_nonzero(outputs[0].cpu() != tokenizer.pad_token_id)\n",
        "            print(\"G'--{}\".format(output))\n",
        "            eval_scores = compute_metrics(output, label, metric, tokenizer)\n",
        "            print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(adv_pred_len, t2-t1, eval_scores))\n",
        "\n",
        "            adv_lens.append(adv_pred_len)\n",
        "            adv_bleus.append(eval_scores)\n",
        "            adv_time.append(t2-t1)\n",
        "\n",
        "            att_success += (adv_pred_len > pred_len)\n",
        "            total_pairs += 1\n",
        "\n",
        "            if total_pairs >= max_num_samples:\n",
        "                break\n",
        "\n",
        "\n",
        "    # Summarize eval results\n",
        "    ori_len = np.mean(ori_lens)\n",
        "    adv_len = np.mean(adv_lens)\n",
        "    ori_bleu = np.mean(ori_bleus)\n",
        "    adv_bleu = np.mean(adv_bleus)\n",
        "    ori_t = np.mean(ori_time)\n",
        "    adv_t = np.mean(adv_time)\n",
        "    print(\"Original output length: {:.3f}, latency: {:.3f}, BLEU: {:.3f}\".format(ori_len, ori_t, ori_bleu))\n",
        "    print(\"Adversarial output length: {:.3f}, latency: {:.3f}, BLEU: {:.3f}\".format(adv_len, adv_t, adv_bleu))\n",
        "    print(\"Attack success rate: {:.2f}%\".format(100*att_success/total_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "84315fa7f06c46278ea553e836ce60e9",
            "213f6810504c44b3ba261a5eaea26b82",
            "f985e336fb6b46c094cadb43bdbc0e4a",
            "2a90ce1d47e243e1a574e923098867f3",
            "26a6adaecf3b462186fa09311f32a4d1",
            "9d8ec86383e14aa7ad58304e64546741",
            "428f603e389347229be629f3c16b29d9",
            "a40f1faa4ea44197be87ef6f6bc63c4e",
            "91ecbb6978664e289607aac1db2e4166",
            "01bdfd23d64049638a2e629d093ee7bb",
            "40b233347a504c5699577a4f94b91db1"
          ]
        },
        "id": "wQkZmzYStHZL",
        "outputId": "2eb827f3-7f0f-4dee-9315-9d681e941ff2"
      },
      "outputs": [],
      "source": [
        "# max_num_samples = 5\n",
        "# max_per = 1\n",
        "# test_demo(device, model, tokenizer, attacker, max_num_samples, max_per)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ConvAI2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset conv_ai_2 (/home/monkey/.cache/huggingface/datasets/conv_ai_2/conv_ai_2/1.0.0/11d600ddce66bb9d07ca50d1b55b488145ef0d5d0206168c32f1043677875865)\n",
            "100%|██████████| 1/1 [00:00<00:00, 521.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialog_id', 'dialog', 'bot_profile', 'user_profile', 'eval_score', 'profile_match'],\n",
            "        num_rows: 3495\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"conv_ai_2\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': '0xab38710', 'dialog_id': '0xab38710', 'dialog': [{'id': 0, 'sender': 'participant2', 'text': 'I love iphone! i just bought new iphone!', 'sender_class': 'Bot'}, {'id': 1, 'sender': 'participant1', 'text': \"Thats good for you, i'm not very into new tech\", 'sender_class': 'Human'}, {'id': 2, 'sender': 'participant2', 'text': 'I am a college student and i am a college student', 'sender_class': 'Bot'}, {'id': 3, 'sender': 'participant1', 'text': 'I am go to gym and live on donations', 'sender_class': 'Human'}, {'id': 4, 'sender': 'participant2', 'text': 'I am a vegan and i am in the midwest', 'sender_class': 'Bot'}, {'id': 5, 'sender': 'participant1', 'text': 'So vegan... i have dogs maybe i should told then that they may eat cheap salads insted of meat', 'sender_class': 'Human'}, {'id': 6, 'sender': 'participant2', 'text': 'I would not mind having them in the office that would be hard for me', 'sender_class': 'Bot'}, {'id': 7, 'sender': 'participant1', 'text': 'Dogs or vegan in office?', 'sender_class': 'Human'}, {'id': 8, 'sender': 'participant2', 'text': 'I am a vegetarian so i am vegan', 'sender_class': 'Bot'}, {'id': 9, 'sender': 'participant1', 'text': 'Strange answer', 'sender_class': 'Human'}, {'id': 10, 'sender': 'participant2', 'text': 'I guess i just do not know what to ps4 experience with.', 'sender_class': 'Bot'}], 'bot_profile': [['i', ' ', 'h', 'a', 'v', 'e', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g', ' ', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n', ' ', 'a', 'n', 'd', ' ', 'g', 'r', 'a', 'n', 'd', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n', '.'], ['i', ' ', 'c', 'a', 'n', ' ', 's', 'e', 'w', ' ', 'm', 'y', ' ', 'o', 'w', 'n', ' ', 'c', 'l', 'o', 't', 'h', 'e', 's', '.'], ['i', ' ', 'h', 'a', 'd', ' ', 'c', 'a', 'n', 'c', 'e', 'r', ' ', 'b', 'u', 't', ' ', 'i', 't', 's', ' ', 'g', 'o', 'n', 'e', ' ', 'n', 'o', 'w', '.'], ['i', ' ', 'a', 'm', ' ', 'r', 'e', 't', 'i', 'r', 'e', 'd', ' ', 'a', 'n', 'd', ' ', 'l', 'i', 'v', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'g', 'r', 'e', 'a', 't', ' ', 'l', 'i', 'f', 'e', '.'], ['i', ' ', 'd', 'o', ' ', 'n', 'o', 't', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 's', 'm', 'a', 'r', 't', 'p', 'h', 'o', 'n', 'e', '.']], 'user_profile': [['t', 'w', 'o', ' ', 'd', 'o', 'g', 's', ' ', 'l', 'i', 'v', 'e', ' ', 'w', 'i', 't', 'h', ' ', 'm', 'e', '.'], ['i', ' ', 'l', 'i', 'k', 'e', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 't', 'r', 'e', 'a', 'd', 'm', 'i', 'l', 'l', ' ', 'a', 'n', 'd', ' ', 'r', 'o', 'w', 'i', 'n', 'g', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', '.'], ['e', 'a', 't', 'i', 'n', 'g', ' ', 'i', 's', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', ' ', 'i', ' ', 'd', 'o', ' ', 'w', 'h', 'e', 'n', ' ', 'i', \"'\", 'm', ' ', 'b', 'o', 'r', 'e', 'd', '.'], ['i', ' ', 'h', 'a', 'v', 'e', ' ', 's', 'h', 'o', 'r', 't', ' ', 'h', 'a', 'i', 'r', '.'], ['i', ' ', 'g', 'o', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'g', 'y', 'm', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', 'l', 'y', '.']], 'eval_score': 1, 'profile_match': 0}\n",
            "user profile:  two dogs live with me. i like doing the treadmill and rowing machine. eating is something i do when i'm bored. i have short hair. i go to the gym regularly.\n",
            "text:  <PS> two dogs live with me. i like doing the treadmill and rowing machine. eating is something i do when i'm bored. i have short hair. i go to the gym regularly.<EOS>I love iphone! i just bought new iphone!<EOS>Thats good for you, i'm not very into new tech\n",
            "text:  <PS> two dogs live with me. i like doing the treadmill and rowing machine. eating is something i do when i'm bored. i have short hair. i go to the gym regularly. I love iphone! i just bought new iphone! Thats good for you, i'm not very into new tech<EOS>I am a college student and i am a college student<EOS>I am go to gym and live on donations\n",
            "text:  <PS> two dogs live with me. i like doing the treadmill and rowing machine. eating is something i do when i'm bored. i have short hair. i go to the gym regularly. I love iphone! i just bought new iphone! Thats good for you, i'm not very into new tech I am a college student and i am a college student I am go to gym and live on donations<EOS>I am a vegan and i am in the midwest<EOS>So vegan... i have dogs maybe i should told then that they may eat cheap salads insted of meat\n",
            "text:  <PS> two dogs live with me. i like doing the treadmill and rowing machine. eating is something i do when i'm bored. i have short hair. i go to the gym regularly. I love iphone! i just bought new iphone! Thats good for you, i'm not very into new tech I am a college student and i am a college student I am go to gym and live on donations I am a vegan and i am in the midwest So vegan... i have dogs maybe i should told then that they may eat cheap salads insted of meat<EOS>I would not mind having them in the office that would be hard for me<EOS>Dogs or vegan in office?\n",
            "text:  <PS> two dogs live with me. i like doing the treadmill and rowing machine. eating is something i do when i'm bored. i have short hair. i go to the gym regularly. I love iphone! i just bought new iphone! Thats good for you, i'm not very into new tech I am a college student and i am a college student I am go to gym and live on donations I am a vegan and i am in the midwest So vegan... i have dogs maybe i should told then that they may eat cheap salads insted of meat I would not mind having them in the office that would be hard for me Dogs or vegan in office?<EOS>I am a vegetarian so i am vegan<EOS>Strange answer\n"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "    print(dataset['train'][i])\n",
        "    instance = dataset['train'][i]\n",
        "    user_profile = ' '.join([''.join(x) for x in instance['user_profile']])\n",
        "    print('user profile: ', user_profile)\n",
        "\n",
        "    persona_pieces = f\"<PS> {user_profile}\"\n",
        "    num_entries = len([x for x in instance['dialog'] if x['sender_class'] == 'Human'])\n",
        "    previous_utterance_pieces = [persona_pieces]\n",
        "\n",
        "    for entry_idx in range(num_entries):\n",
        "        bot_msg = instance['dialog'][entry_idx*2]['text']\n",
        "        human_msg = instance['dialog'][entry_idx*2+1]['text']\n",
        "        original_context = ' '.join(previous_utterance_pieces)\n",
        "        previous_utterance_pieces += [\n",
        "            bot_msg,\n",
        "            human_msg,\n",
        "        ]\n",
        "\n",
        "        text = original_context + '<EOS>' + bot_msg + '<EOS>' + human_msg\n",
        "        print(\"text: \", text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Empathetic Dialogues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|██████████| 4.51k/4.51k [00:00<00:00, 1.76MB/s]\n",
            "Downloading metadata: 100%|██████████| 1.91k/1.91k [00:00<00:00, 953kB/s]\n",
            "Downloading readme: 100%|██████████| 7.15k/7.15k [00:00<00:00, 3.02MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset empathetic_dialogues/default to /home/dsi/yufli/.cache/huggingface/datasets/empathetic_dialogues/default/0.1.0/09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 28.0M/28.0M [00:01<00:00, 26.6MB/s]\n",
            "                                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset empathetic_dialogues downloaded and prepared to /home/dsi/yufli/.cache/huggingface/datasets/empathetic_dialogues/default/0.1.0/09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 523.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 76673\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 12030\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
            "        num_rows: 10943\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"empathetic_dialogues\")\n",
        "print(dataset)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def group_texts(dataset):\n",
        "    results = {\n",
        "        'conv_id': [], \n",
        "        'prompt': [],\n",
        "        'dialog': [], \n",
        "        'context': [],\n",
        "    }\n",
        "    for i, instance in enumerate(dataset):\n",
        "        if instance['utterance_idx'] == 1:\n",
        "            results['conv_id'].append(instance['conv_id'])\n",
        "            results['dialog'].append([])\n",
        "            results['prompt'].append(instance['prompt'])\n",
        "            results['context'].append(instance['context'])\n",
        "\n",
        "        response = {'text': instance['utterance'], 'speaker_idx': instance['speaker_idx']}\n",
        "        results['dialog'][-1].append(response)\n",
        "\n",
        "    return Dataset.from_dict(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conv_id', 'prompt', 'dialog', 'context'],\n",
              "    num_rows: 2541\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_test_dataset = group_texts(test_dataset)\n",
        "grouped_test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'conv_id': 'hit:0_conv:0',\n",
              " 'prompt': \"I felt guilty when I was driving home one night and a person tried to fly into my lane_comma_ and didn't see me. I honked and they swerved back into their lane_comma_ slammed on their brakes_comma_ and hit the water cones.\",\n",
              " 'dialog': [{'speaker_idx': 0,\n",
              "   'text': 'Yeah about 10 years ago I had a horrifying experience. It was 100% their fault but they hit the water barrels and survived. They had no injuries but they almost ran me off the road.'},\n",
              "  {'speaker_idx': 1, 'text': 'Did you suffer any injuries?'},\n",
              "  {'speaker_idx': 0,\n",
              "   'text': \"No I wasn't hit. It turned out they were drunk. I felt guilty but realized it was his fault.\"},\n",
              "  {'speaker_idx': 1,\n",
              "   'text': \"Why did you feel guilty? People really shouldn't drive drunk.\"},\n",
              "  {'speaker_idx': 0,\n",
              "   'text': \"I don't know I was new to driving and hadn't experienced anything like that. I felt like my horn made him swerve into the water barrels.\"}],\n",
              " 'context': 'guilty'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_test_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wHvWQLxuhgsM",
        "SfVI32fzhmV6"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "29966b4b85dd369f0f60e5cdfe24bb91fae213e2bd4adc2b4429ab38cab7a2d3"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "004738fd62854848a7676e3100693a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37c18f11be549a8834c745a960807f3",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9301ea70fc4a6691af00b5d43321f4",
            "value": "100%"
          }
        },
        "01bdfd23d64049638a2e629d093ee7bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be25e0afa9c405fa3b7089fe6aa6ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d8e725392444308b0c8233f62d9ec64",
            "placeholder": "​",
            "style": "IPY_MODEL_3c2deb172de24560b57a5a548acfe53b",
            "value": " 3/3 [00:00&lt;00:00, 82.94it/s]"
          }
        },
        "213f6810504c44b3ba261a5eaea26b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8ec86383e14aa7ad58304e64546741",
            "placeholder": "​",
            "style": "IPY_MODEL_428f603e389347229be629f3c16b29d9",
            "value": "100%"
          }
        },
        "26a6adaecf3b462186fa09311f32a4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a90ce1d47e243e1a574e923098867f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01bdfd23d64049638a2e629d093ee7bb",
            "placeholder": "​",
            "style": "IPY_MODEL_40b233347a504c5699577a4f94b91db1",
            "value": " 3/3 [00:00&lt;00:00, 89.30it/s]"
          }
        },
        "36d9fd106ff240c8b0fb971e5c8efb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c2deb172de24560b57a5a548acfe53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b233347a504c5699577a4f94b91db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "428f603e389347229be629f3c16b29d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d8e725392444308b0c8233f62d9ec64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7170283bda7045e283e036ccaeffdcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_004738fd62854848a7676e3100693a1b",
              "IPY_MODEL_df68b3c7f37d4821bff89b0e1cd082bb",
              "IPY_MODEL_1be25e0afa9c405fa3b7089fe6aa6ba7"
            ],
            "layout": "IPY_MODEL_7c8a70a52cb64d8eb765ee4cd5df30ec"
          }
        },
        "7c8a70a52cb64d8eb765ee4cd5df30ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84315fa7f06c46278ea553e836ce60e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_213f6810504c44b3ba261a5eaea26b82",
              "IPY_MODEL_f985e336fb6b46c094cadb43bdbc0e4a",
              "IPY_MODEL_2a90ce1d47e243e1a574e923098867f3"
            ],
            "layout": "IPY_MODEL_26a6adaecf3b462186fa09311f32a4d1"
          }
        },
        "91ecbb6978664e289607aac1db2e4166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d8ec86383e14aa7ad58304e64546741": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40f1faa4ea44197be87ef6f6bc63c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37c18f11be549a8834c745a960807f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df68b3c7f37d4821bff89b0e1cd082bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5f96869232484282b47280a9f0639c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d9fd106ff240c8b0fb971e5c8efb3b",
            "value": 3
          }
        },
        "ed5f96869232484282b47280a9f0639c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f985e336fb6b46c094cadb43bdbc0e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40f1faa4ea44197be87ef6f6bc63c4e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ecbb6978664e289607aac1db2e4166",
            "value": 3
          }
        },
        "fe9301ea70fc4a6691af00b5d43321f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
