{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqs6Cy4veuWR"
      },
      "outputs": [],
      "source": [
        "! pip install transformers==4.24.0\n",
        "! pip install datasets \n",
        "! pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUd3jJg_egjk",
        "outputId": "a3588f7e-728a-46a4-9feb-2f16f0ead794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import language_tool_python\n",
        "from argparse import Namespace\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import (\n",
        "    AutoConfig, \n",
        "    AutoTokenizer, \n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForMaskedLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "import os\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "import inspect\n",
        "\n",
        "from transformers.generation_utils import BeamSearchScorer\n",
        "from transformers import LogitsProcessorList, StoppingCriteriaList\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXWXvwRJjpBA"
      },
      "source": [
        "## Blended_Skill_Talk Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "7170283bda7045e283e036ccaeffdcf2",
            "004738fd62854848a7676e3100693a1b",
            "df68b3c7f37d4821bff89b0e1cd082bb",
            "1be25e0afa9c405fa3b7089fe6aa6ba7",
            "7c8a70a52cb64d8eb765ee4cd5df30ec",
            "d37c18f11be549a8834c745a960807f3",
            "fe9301ea70fc4a6691af00b5d43321f4",
            "ed5f96869232484282b47280a9f0639c",
            "36d9fd106ff240c8b0fb971e5c8efb3b",
            "6d8e725392444308b0c8233f62d9ec64",
            "3c2deb172de24560b57a5a548acfe53b"
          ]
        },
        "id": "qIXJ_oTvGRcB",
        "outputId": "829e202a-f7cc-4849-c194-50ae9afa91e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset blended_skill_talk (/root/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/8544e13cbbf2fb9b34157f2e2f28c1539e4f36bf0ef2bd96edd138b4000c5ca1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7170283bda7045e283e036ccaeffdcf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 4819\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 1009\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['personas', 'additional_context', 'previous_utterance', 'context', 'free_messages', 'guided_messages', 'suggestions', 'guided_chosen_suggestions', 'label_candidates'],\n",
            "        num_rows: 980\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "train_dataset = bst_dataset['train']\n",
        "eval_dataset = bst_dataset['validation']\n",
        "test_dataset = bst_dataset['test']\n",
        "print(bst_dataset)\n",
        "# print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOTNHI1jq5W1",
        "outputId": "88cba86e-ddf9-4eea-ee9b-9a2cdafabdb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#pairs of training dialogues: 27018, validation dialogues: 5651, test dialogues: 5482\n"
          ]
        }
      ],
      "source": [
        "# Get statistics of pair of dialogues \n",
        "train_num, eval_num, test_num = 0, 0, 0\n",
        "for i, instance in enumerate(train_dataset):\n",
        "    train_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(eval_dataset):\n",
        "    eval_num += len(instance['free_messages'])\n",
        "\n",
        "for i, instance in enumerate(test_dataset):\n",
        "    test_num += len(instance['free_messages'])\n",
        "\n",
        "print(\"#pairs of training dialogues: {}, validation dialogues: {}, test dialogues: {}\".format(\n",
        "    train_num, eval_num, test_num,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xnYGvfspczn",
        "outputId": "f17f5947-5fb2-4902-96e1-021efe8dfc0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U--and why is that?\n",
            "G--I think it's because in my head, I think everyone is judging me. I just start to sweat and I get sick in my stomach.\n",
            "\n",
            "U--interesting but I know how you feel especially the whole people telling that it in your head \n",
            "G--I don't really have people telling me in my head, more like behind my back\n",
            "\n",
            "U--Dang that's though. But I also understand that. I have people some who talks behind my back because of certain things that I believe in \n",
            "G--Me too! What do you believe in? I believe in dragons... Just finished watching Game of Thrones. Man, those things are dope\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show examples\n",
        "for i, instance in enumerate(test_dataset.select(range(1))):\n",
        "    for (x, y) in zip(instance['free_messages'], instance['guided_messages']):\n",
        "        print(\"U--{}\\nG--{}\".format(x, y))\n",
        "        print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "M8sPchPi4zW3"
      },
      "outputs": [],
      "source": [
        "data_args = Namespace(\n",
        "    # model_name_or_path=\"facebook/bart-base\",\n",
        "    model_name_or_path=\"/content/drive/My Drive/results/\",\n",
        "    max_source_length=256,\n",
        "    max_target_length=256,\n",
        "    pad_to_max_length=False,\n",
        "    ignore_pad_token_for_loss=True,\n",
        "    max_train_samples=None,\n",
        "    preprocessing_num_workers=None,\n",
        "    overwrite_cache=True,\n",
        "    output_dir='.',\n",
        ")\n",
        "\n",
        "max_target_length = data_args.max_target_length\n",
        "padding = \"max_length\" if data_args.pad_to_max_length else False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_5bCeARMzxEl"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(data_args.model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(data_args.model_name_or_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(data_args.model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6fiKaj4sXO1",
        "outputId": "2098092e-a18d-404d-e187-7190ef7e2fb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHvWQLxuhgsM"
      },
      "source": [
        "### Tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykvNCj60y0eT"
      },
      "outputs": [],
      "source": [
        "def preprocess_bst(examples):\n",
        "    inputs = [sent for ex in examples[\"free_messages\"] for sent in ex]\n",
        "    targets = [sent for ex in examples[\"guided_messages\"] for sent in ex]\n",
        "    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNv_GKSVy0a9"
      },
      "outputs": [],
      "source": [
        "column_names = train_dataset.column_names\n",
        "\n",
        "if data_args.max_train_samples is not None:\n",
        "    train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")\n",
        "\n",
        "eval_dataset = eval_dataset.map(\n",
        "    preprocess_bst,\n",
        "    batched=True,\n",
        "    num_proc=data_args.preprocessing_num_workers,\n",
        "    remove_columns=column_names,\n",
        "    load_from_cache_file=not data_args.overwrite_cache,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfVI32fzhmV6"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64cF7tqxy0KF"
      },
      "outputs": [],
      "source": [
        "# Data collator\n",
        "label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
        "if data_args.pad_to_max_length:\n",
        "    data_collator = default_data_collator\n",
        "else:\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
        "    )\n",
        "\n",
        "# Metric\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if data_args.ignore_pad_token_for_loss:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset if training_args.do_train else None,\n",
        "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLh15B-ny0HZ"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "checkpoint = None\n",
        "\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "metrics = train_result.metrics\n",
        "max_train_samples = (\n",
        "    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        ")\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9obPrvznFjL8"
      },
      "source": [
        "### Dialogue API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C2MrymsRFUDi"
      },
      "outputs": [],
      "source": [
        "def dialogue(\n",
        "        module,\n",
        "        inputs: Optional[torch.Tensor] = None,\n",
        "        max_length: Optional[int] = None,\n",
        "        min_length: Optional[int] = None,\n",
        "        do_sample: Optional[bool] = None,\n",
        "        early_stopping: Optional[bool] = None,\n",
        "        num_beams: Optional[int] = None,\n",
        "        temperature: Optional[float] = None,\n",
        "        top_k: Optional[int] = None,\n",
        "        top_p: Optional[float] = None,\n",
        "        repetition_penalty: Optional[float] = None,\n",
        "        bad_words_ids: Optional[Iterable[int]] = None,\n",
        "        bos_token_id: Optional[int] = None,\n",
        "        pad_token_id: Optional[int] = None,\n",
        "        eos_token_id: Optional[int] = None,\n",
        "        length_penalty: Optional[float] = None,\n",
        "        no_repeat_ngram_size: Optional[int] = None,\n",
        "        encoder_no_repeat_ngram_size: Optional[int] = None,\n",
        "        num_return_sequences: Optional[int] = None,\n",
        "        max_time: Optional[float] = None,\n",
        "        max_new_tokens: Optional[int] = None,\n",
        "        decoder_start_token_id: Optional[int] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        num_beam_groups: Optional[int] = None,\n",
        "        diversity_penalty: Optional[float] = None,\n",
        "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
        "        logits_processor: Optional[LogitsProcessorList] = LogitsProcessorList(),\n",
        "        stopping_criteria: Optional[StoppingCriteriaList] = StoppingCriteriaList(),\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        output_scores: Optional[bool] = None,\n",
        "        return_dict_in_generate: Optional[bool] = None,\n",
        "        forced_bos_token_id: Optional[int] = None,\n",
        "        forced_eos_token_id: Optional[int] = None,\n",
        "        remove_invalid_values: Optional[bool] = None,\n",
        "        synced_gpus: Optional[bool] = None,\n",
        "        **model_kwargs):\n",
        "    # 1. Set generation parameters if not already defined\n",
        "    bos_token_id = bos_token_id if bos_token_id is not None else module.config.bos_token_id\n",
        "    num_beams = num_beams if num_beams is not None else module.config.num_beams\n",
        "    length_penalty = length_penalty if length_penalty is not None else module.config.length_penalty\n",
        "    early_stopping = early_stopping if early_stopping is not None else module.config.early_stopping\n",
        "    num_beam_groups = num_beam_groups if num_beam_groups is not None else module.config.num_beam_groups\n",
        "    do_sample = do_sample if do_sample is not None else module.config.do_sample\n",
        "    num_return_sequences = (\n",
        "        num_return_sequences if num_return_sequences is not None else module.config.num_return_sequences\n",
        "    )\n",
        "\n",
        "    pad_token_id = pad_token_id if pad_token_id is not None else module.config.pad_token_id\n",
        "    eos_token_id = eos_token_id if eos_token_id is not None else module.config.eos_token_id\n",
        "\n",
        "    # output_scores = output_scores if output_scores is not None else module.config.output_scores\n",
        "    output_scores = True\n",
        "    output_attentions = output_attentions if output_attentions is not None else module.config.output_attentions\n",
        "    output_hidden_states = (\n",
        "        output_hidden_states if output_hidden_states is not None else module.config.output_hidden_states\n",
        "    )\n",
        "    # return_dict_in_generate = (\n",
        "    #     return_dict_in_generate if return_dict_in_generate is not None else module.config.return_dict_in_generate\n",
        "    # )\n",
        "    return_dict_in_generate = True\n",
        "\n",
        "    if pad_token_id is None and eos_token_id is not None:\n",
        "        # special case if pad_token_id is not defined\n",
        "        # logger.warning(f\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\")\n",
        "        pad_token_id = eos_token_id\n",
        "\n",
        "    # 2. Define model inputs\n",
        "    # inputs_tensor has to be defined\n",
        "    # model_input_name is defined if model-specific keyword input is passed\n",
        "    # otherwise model_input_name is None\n",
        "    # all model-specific keyword inputs are removed from `model_kwargs`\n",
        "    inputs_tensor, model_input_name, model_kwargs = module._prepare_model_inputs(inputs, bos_token_id, model_kwargs)\n",
        "    batch_size = inputs_tensor.shape[0]\n",
        "\n",
        "    # 3. Define other model kwargs\n",
        "    model_kwargs[\"output_attentions\"] = output_attentions\n",
        "    model_kwargs[\"output_hidden_states\"] = output_hidden_states\n",
        "    model_kwargs[\"use_cache\"] = use_cache\n",
        "\n",
        "    accepts_attention_mask = \"attention_mask\" in set(inspect.signature(module.forward).parameters.keys())\n",
        "    requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
        "\n",
        "    if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask and accepts_attention_mask:\n",
        "        model_kwargs[\"attention_mask\"] = module._prepare_attention_mask_for_generation(\n",
        "            inputs_tensor, pad_token_id, eos_token_id\n",
        "        )\n",
        "\n",
        "    if module.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
        "        # if model is encoder decoder encoder_outputs are created\n",
        "        # and added to `model_kwargs`\n",
        "        model_kwargs = module._prepare_encoder_decoder_kwargs_for_generation(\n",
        "            inputs_tensor, model_kwargs, model_input_name\n",
        "        )\n",
        "\n",
        "    # 4. Prepare `input_ids` which will be used for auto-regressive generation\n",
        "    if module.config.is_encoder_decoder:\n",
        "        input_ids = module._prepare_decoder_input_ids_for_generation(\n",
        "            batch_size,\n",
        "            decoder_start_token_id=decoder_start_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            model_kwargs=model_kwargs,\n",
        "        )\n",
        "    else:\n",
        "        # if decoder-only then inputs_tensor has to be `input_ids`\n",
        "        input_ids = inputs_tensor\n",
        "\n",
        "    # 5. Prepare `max_length` depending on other stopping criteria\n",
        "    # if `max_new_tokens` is passed, but not `max_length` -> set `max_length = max_new_tokens`\n",
        "    if max_length is None and max_new_tokens is not None:\n",
        "        max_length = max_new_tokens + input_ids.shape[-1]\n",
        "    elif max_length is not None and max_new_tokens is not None:\n",
        "        pass\n",
        "        # Both are set, this is odd, raise a warning\n",
        "        # warnings.warn(\n",
        "        #     \"Both `max_length` and `max_new_tokens` have been set \"\n",
        "        #     f\"but they serve the same purpose. `max_length` {max_length} \"\n",
        "        #     f\"will take priority over `max_new_tokens` {max_new_tokens}.\",\n",
        "        #     UserWarning,\n",
        "        # )\n",
        "    # default to config if still None\n",
        "    max_length = max_length if max_length is not None else module.config.max_length\n",
        "\n",
        "    if input_ids.shape[-1] >= max_length:\n",
        "        input_ids_string = \"decoder_input_ids\" if module.config.is_encoder_decoder else \"input_ids\"\n",
        "        # logger.warning(\n",
        "        #     f\"Input length of {input_ids_string} is {input_ids.shape[-1]}, but ``max_length`` is set to {max_length}. \"\n",
        "        #     \"This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\"\n",
        "        # )\n",
        "\n",
        "    # 6. determine generation mode\n",
        "    is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False\n",
        "    is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True\n",
        "    is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False\n",
        "    is_beam_sample_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is True\n",
        "    is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1)\n",
        "\n",
        "    if num_beam_groups > num_beams:\n",
        "        raise ValueError(\"`num_beam_groups` has to be smaller or equal to `num_beams`\")\n",
        "    if is_group_beam_gen_mode and do_sample is True:\n",
        "        raise ValueError(\n",
        "            \"Diverse beam search cannot be used in sampling mode. Make sure that `do_sample` is set to `False`.\"\n",
        "        )\n",
        "\n",
        "    # # 7. prepare distribution pre_processing samplers\n",
        "    # logits_processor = module._get_logits_processor(\n",
        "    #     repetition_penalty=repetition_penalty,\n",
        "    #     no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "    #     encoder_no_repeat_ngram_size=encoder_no_repeat_ngram_size,\n",
        "    #     encoder_input_ids=inputs_tensor,\n",
        "    #     bad_words_ids=bad_words_ids,\n",
        "    #     min_length=min_length,\n",
        "    #     max_length=max_length,\n",
        "    #     eos_token_id=eos_token_id,\n",
        "    #     forced_bos_token_id=forced_bos_token_id,\n",
        "    #     forced_eos_token_id=forced_eos_token_id,\n",
        "    #     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
        "    #     num_beams=num_beams,\n",
        "    #     num_beam_groups=num_beam_groups,\n",
        "    #     diversity_penalty=diversity_penalty,\n",
        "    #     remove_invalid_values=remove_invalid_values,\n",
        "    #     logits_processor=logits_processor,\n",
        "    # )\n",
        "\n",
        "    # 8. prepare stopping criteria\n",
        "    stopping_criteria = module._get_stopping_criteria(\n",
        "        max_length=max_length, max_time=max_time, stopping_criteria=stopping_criteria\n",
        "    )\n",
        "\n",
        "    # 9. go into different generation modes\n",
        "    if is_greedy_gen_mode:\n",
        "        if num_return_sequences > 1:\n",
        "            raise ValueError(\n",
        "                f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
        "            )\n",
        "\n",
        "        # 10. run greedy search\n",
        "        return module.greedy_search(\n",
        "            input_ids,\n",
        "            logits_processor=logits_processor,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "    elif is_sample_gen_mode:\n",
        "        # 10. prepare logits warper\n",
        "        logits_warper = module._get_logits_warper(\n",
        "            top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
        "        )\n",
        "\n",
        "        # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n",
        "        input_ids, model_kwargs = module._expand_inputs_for_generation(\n",
        "            input_ids,\n",
        "            expand_size=num_return_sequences,\n",
        "            is_encoder_decoder=module.config.is_encoder_decoder,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "        # 12. run sample\n",
        "        return module.sample(\n",
        "            input_ids,\n",
        "            logits_processor=logits_processor,\n",
        "            logits_warper=logits_warper,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "    elif is_beam_gen_mode:\n",
        "        if num_return_sequences > num_beams:\n",
        "            raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
        "\n",
        "        if stopping_criteria.max_length is None:\n",
        "            raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
        "\n",
        "        # 10. prepare beam search scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size,\n",
        "            num_beams=num_beams,\n",
        "            device=module.device,\n",
        "            length_penalty=length_penalty,\n",
        "            do_early_stopping=early_stopping,\n",
        "            num_beam_hyps_to_keep=num_return_sequences,\n",
        "        )\n",
        "        # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
        "        input_ids, model_kwargs = module._expand_inputs_for_generation(\n",
        "            input_ids, expand_size=num_beams, is_encoder_decoder=module.config.is_encoder_decoder, **model_kwargs\n",
        "        )\n",
        "        # 12. run beam search\n",
        "        return module.beam_search(\n",
        "            input_ids,\n",
        "            beam_scorer,\n",
        "            logits_processor=logits_processor,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "    elif is_beam_sample_gen_mode:\n",
        "        # 10. prepare logits warper\n",
        "        logits_warper = module._get_logits_warper(\n",
        "            top_k=top_k, top_p=top_p, temperature=temperature, num_beams=num_beams\n",
        "        )\n",
        "\n",
        "        if stopping_criteria.max_length is None:\n",
        "            raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
        "        # 11. prepare beam search scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size * num_return_sequences,\n",
        "            num_beams=num_beams,\n",
        "            device=module.device,\n",
        "            length_penalty=length_penalty,\n",
        "            do_early_stopping=early_stopping,\n",
        "        )\n",
        "\n",
        "        # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
        "        input_ids, model_kwargs = module._expand_inputs_for_generation(\n",
        "            input_ids,\n",
        "            expand_size=num_beams * num_return_sequences,\n",
        "            is_encoder_decoder=module.config.is_encoder_decoder,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "        # 13. run beam sample\n",
        "        return module.beam_sample(\n",
        "            input_ids,\n",
        "            beam_scorer,\n",
        "            logits_processor=logits_processor,\n",
        "            logits_warper=logits_warper,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )\n",
        "\n",
        "    elif is_group_beam_gen_mode:\n",
        "        if num_return_sequences > num_beams:\n",
        "            raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n",
        "\n",
        "        if num_beams % num_beam_groups != 0:\n",
        "            raise ValueError(\"`num_beams` should be divisible by `num_beam_groups` for group beam search.\")\n",
        "\n",
        "        if stopping_criteria.max_length is None:\n",
        "            raise ValueError(\"`max_length` needs to be a stopping_criteria for now.\")\n",
        "\n",
        "        # 10. prepare beam search scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size,\n",
        "            num_beams=num_beams,\n",
        "            max_length=stopping_criteria.max_length,\n",
        "            device=module.device,\n",
        "            length_penalty=length_penalty,\n",
        "            do_early_stopping=early_stopping,\n",
        "            num_beam_hyps_to_keep=num_return_sequences,\n",
        "            num_beam_groups=num_beam_groups,\n",
        "        )\n",
        "        # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
        "        input_ids, model_kwargs = module._expand_inputs_for_generation(\n",
        "            input_ids, expand_size=num_beams, is_encoder_decoder=module.config.is_encoder_decoder, **model_kwargs\n",
        "        )\n",
        "        # 12. run beam search\n",
        "        return module.group_beam_search(\n",
        "            input_ids,\n",
        "            beam_scorer,\n",
        "            logits_processor=logits_processor,\n",
        "            stopping_criteria=stopping_criteria,\n",
        "            pad_token_id=pad_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            output_scores=output_scores,\n",
        "            return_dict_in_generate=return_dict_in_generate,\n",
        "            synced_gpus=synced_gpus,\n",
        "            **model_kwargs,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kyn42UoVJ5"
      },
      "source": [
        "### Attack methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0GNLfbkioZwO"
      },
      "outputs": [],
      "source": [
        "class BaseAttacker:\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "      \n",
        "        self.device = device\n",
        "        self.model = model.to(self.device)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.embedding = self.model.get_input_embeddings().weight\n",
        "        self.specical_token = self.tokenizer.all_special_tokens\n",
        "        self.specical_id = self.tokenizer.all_special_ids\n",
        "        self.eos_token_id = self.model.config.eos_token_id\n",
        "        self.pad_token_id = self.model.config.pad_token_id\n",
        "        self.num_beams = self.model.config.num_beams\n",
        "        self.num_beam_groups = self.model.config.num_beam_groups\n",
        "        self.max_len = max_len\n",
        "        self.max_per = max_per\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "\n",
        "    @classmethod\n",
        "    def _get_hparam(cls, namespace: Namespace, key: str, default=None):\n",
        "        if hasattr(namespace, key):\n",
        "            return getattr(namespace, key)\n",
        "        print('Using default argument for \"{}\"'.format(key))\n",
        "\n",
        "        return default\n",
        "\n",
        "    def run_attack(self, x):\n",
        "        pass\n",
        "\n",
        "    def compute_loss(self, x):\n",
        "        pass\n",
        "\n",
        "    def compute_seq_len(self, seq):\n",
        "        if seq[0].eq(self.pad_token_id):\n",
        "            return int(len(seq) - sum(seq.eq(self.pad_token_id)))\n",
        "        else:\n",
        "            return int(len(seq) - sum(seq.eq(self.pad_token_id))) - 1\n",
        "\n",
        "    def get_prediction(self, sentence):\n",
        "        def remove_pad(s):\n",
        "            for i, tk in enumerate(s):\n",
        "                if tk == self.eos_token_id and i != 0:\n",
        "                    return s[:i + 1]\n",
        "            return s\n",
        "\n",
        "        input_ids = self.tokenizer(sentence, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "        \n",
        "        # ['sequences', 'sequences_scores', 'scores', 'beam_indices'] if num_beams != 1\n",
        "        # ['sequences', 'scores'] if num_beams = 1\n",
        "        outputs = dialogue(\n",
        "            self.model, \n",
        "            input_ids,\n",
        "            early_stopping=False, \n",
        "            num_beams=self.num_beams,\n",
        "            num_beam_groups=self.num_beam_groups, \n",
        "            use_cache=True,\n",
        "            max_length=self.max_len,\n",
        "        )\n",
        "        \n",
        "        seqs = outputs['sequences']\n",
        "        seqs = [remove_pad(seq) for seq in seqs]\n",
        "        out_scores = outputs['scores']\n",
        "        pred_len = [self.compute_seq_len(seq) for seq in seqs]\n",
        "        return pred_len, seqs, out_scores\n",
        "\n",
        "    def get_trans_string_len(self, text):\n",
        "        pred_len, seqs, _ = self.get_prediction(text)\n",
        "        return seqs[0], pred_len[0]\n",
        "\n",
        "    def get_trans_len(self, text):\n",
        "        pred_len, _, _ = self.get_prediction(text)\n",
        "        return pred_len\n",
        "\n",
        "    def get_trans_strings(self, text):\n",
        "        pred_len, seqs, _ = self.get_prediction(text)\n",
        "        out_res = [self.tokenizer.decode(seq, skip_special_tokens=True) for seq in seqs]\n",
        "        return out_res, pred_len\n",
        "    \n",
        "    def compute_score(self, text):\n",
        "        batch_size = len(text)\n",
        "        index_list = [i * self.num_beams for i in range(batch_size + 1)]\n",
        "        pred_len, seqs, out_scores = self.get_prediction(text)\n",
        "\n",
        "\n",
        "        scores = [[] for _ in range(batch_size)]\n",
        "        for out_s in out_scores:\n",
        "            for i in range(batch_size):\n",
        "                current_index = index_list[i]\n",
        "                scores[i].append(out_s[current_index: current_index + 1])\n",
        "        scores = [torch.cat(s) for s in scores]\n",
        "        scores = [s[:pred_len[i]] for i, s in enumerate(scores)]\n",
        "        return scores, seqs, pred_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RzfF1r-CoZtg"
      },
      "outputs": [],
      "source": [
        "class SlowAttacker(BaseAttacker):\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "        super(SlowAttacker, self).__init__(device, tokenizer, model, max_len, max_per)\n",
        "\n",
        "    def leave_eos_loss(self, scores, pred_len):\n",
        "        loss = []\n",
        "        for i, s in enumerate(scores):\n",
        "            s[:, self.pad_token_id] = 1e-12 # T X V\n",
        "            eos_p = self.softmax(s)[:pred_len[i], self.eos_token_id]\n",
        "            loss.append(self.bce_loss(eos_p, torch.zeros_like(eos_p)))\n",
        "        return loss\n",
        "\n",
        "    def leave_eos_target_loss(self, scores, seqs, pred_len):\n",
        "        loss = []\n",
        "        for i, s in enumerate(scores): # s: T X V\n",
        "            # if self.pad_token_id != self.eos_token_id:\n",
        "            s[:, self.pad_token_id] = 1e-12\n",
        "            softmax_v = self.softmax(s)\n",
        "            eos_p = softmax_v[:pred_len[i], self.eos_token_id]\n",
        "            target_p = torch.stack([softmax_v[idx, s] for idx, s in enumerate(seqs[i][1:])])\n",
        "            target_p = target_p[:pred_len[i]]\n",
        "            pred = eos_p + target_p\n",
        "            pred[-1] = pred[-1] / 2\n",
        "            loss.append(self.bce_loss(pred, torch.zeros_like(pred)))\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def select_best(self, new_strings, batch_size=30):\n",
        "        \"\"\"\n",
        "        Select generated strings which induce longest output sentences.\n",
        "        \"\"\"\n",
        "        pred_len = []\n",
        "        # seqs = []\n",
        "        batch_num = len(new_strings) // batch_size\n",
        "        if batch_size * batch_num != len(new_strings):\n",
        "            batch_num += 1\n",
        "\n",
        "        for i in range(batch_num):\n",
        "            st, ed = i * batch_size, min(i * batch_size + batch_size, len(new_strings))\n",
        "            input_ids = self.tokenizer(new_strings[st:ed], return_tensors=\"pt\", padding=True).input_ids\n",
        "            input_ids = input_ids.to(self.device)\n",
        "            outputs = self.model.generate(\n",
        "                input_ids, \n",
        "                num_beams=self.num_beams, \n",
        "                max_length=self.max_len,\n",
        "                return_dict_in_generate=True,\n",
        "            )\n",
        "            lengths = [self.compute_seq_len(seq) for seq in outputs['sequences']]\n",
        "            # pdb.set_trace()\n",
        "            pred_len.extend(lengths)\n",
        "            \n",
        "        # pred_len = np.array([self.compute_seq_len(torch.tensor(seq)) for seq in seqs])\n",
        "        pred_len = np.array(pred_len)\n",
        "        # pdb.set_trace()\n",
        "\n",
        "        assert len(new_strings) == len(pred_len)\n",
        "        return new_strings[pred_len.argmax()], max(pred_len)\n",
        "\n",
        "    def prepare_attack(self, text):\n",
        "        ori_len = self.get_trans_len(text)[0] # original sentence length\n",
        "        best_adv_text, best_len = deepcopy(text), ori_len\n",
        "        current_adv_text, current_len = deepcopy(text), ori_len  # current_adv_text: List[str]\n",
        "        return ori_len, (best_adv_text, best_len), (current_adv_text, current_len)\n",
        "\n",
        "    def compute_loss(self, text):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def mutation(self, current_adv_text, grad, modified_pos):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def run_attack(self, text):\n",
        "        \"\"\"\n",
        "        (1) Using gradient ascent to generate adversarial sentences -- mutation();\n",
        "        (2) Select the best samples which induce longest output sentences -- select_best();\n",
        "        (3) Save the adversarial samples -- adv_his.\n",
        "        \"\"\"\n",
        "        assert len(text) != 1\n",
        "        # torch.autograd.set_detect_anomaly(True)\n",
        "        ori_len, (best_adv_text, best_len), (current_adv_text, current_len) = self.prepare_attack(text)\n",
        "        # adv_his = [(deepcopy(current_adv_text), deepcopy(current_len), 0.0)]\n",
        "        adv_his = []\n",
        "        modify_pos = []\n",
        "        pbar = tqdm(range(self.max_per))\n",
        "        t1 = time.time()\n",
        "\n",
        "        for it in pbar:\n",
        "            loss_list = self.compute_loss([current_adv_text])\n",
        "            loss = sum(loss_list)\n",
        "            self.model.zero_grad()\n",
        "            loss.backward()\n",
        "            grad = self.embedding.grad\n",
        "            new_strings = self.mutation(current_adv_text, grad, modify_pos)\n",
        "\n",
        "            if new_strings:\n",
        "                current_adv_text, current_len = self.select_best(new_strings)\n",
        "                log_str = \"%d, %d, %.2f\" % (it, len(new_strings), best_len / ori_len)\n",
        "                pbar.set_description(log_str)\n",
        "\n",
        "                if current_len > best_len:\n",
        "                    best_adv_text = deepcopy(current_adv_text)\n",
        "                    best_len = current_len\n",
        "                t2 = time.time()\n",
        "                adv_his.append((best_adv_text, int(best_len), t2 - t1))\n",
        "\n",
        "        if adv_his:\n",
        "            return True, adv_his\n",
        "        else:\n",
        "            return False, [(deepcopy(current_adv_text), deepcopy(current_len), 0.0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZkV1HKfloZmK"
      },
      "outputs": [],
      "source": [
        "class WordAttacker(SlowAttacker):\n",
        "    def __init__(self, \n",
        "                 device,\n",
        "                 tokenizer,\n",
        "                 model,\n",
        "                 max_len=64,\n",
        "                 max_per=3):\n",
        "        super(WordAttacker, self).__init__(device, tokenizer, model, max_len, max_per)\n",
        "\n",
        "    def compute_loss(self, text):\n",
        "        scores, seqs, pred_len = self.compute_score(text) # [T X V], [T], [1]\n",
        "        loss_list = self.leave_eos_target_loss(scores, seqs, pred_len)\n",
        "        # loss_list = self.leave_eos_loss(scores, pred_len)\n",
        "        return loss_list\n",
        "    \n",
        "\n",
        "    def token_replace_mutation(self, current_adv_text, grad, modified_pos):\n",
        "        new_strings = []\n",
        "        current_ids = self.tokenizer(current_adv_text, return_tensors=\"pt\", padding=True).input_ids[0]\n",
        "        base_ids = current_ids.clone()\n",
        "        for pos in modified_pos:\n",
        "            t = current_ids[0][pos]\n",
        "            grad_t = grad[t]\n",
        "            score = (self.embedding - self.embedding[t]).mm(grad_t.reshape([-1, 1])).reshape([-1])\n",
        "            index = score.argsort()\n",
        "            for tgt_t in index:\n",
        "                if tgt_t not in self.specical_token:\n",
        "                    base_ids[pos] = tgt_t\n",
        "                    break\n",
        "\n",
        "        for pos, t in enumerate(current_ids):\n",
        "            if t not in self.specical_id:\n",
        "                cnt, grad_t = 0, grad[t]\n",
        "                score = (self.embedding - self.embedding[t]).mm(grad_t.reshape([-1, 1])).reshape([-1])\n",
        "                index = score.argsort()\n",
        "                for tgt_t in index:\n",
        "                    if tgt_t not in self.specical_token:\n",
        "                        new_base_ids = base_ids.clone()\n",
        "                        new_base_ids[pos] = tgt_t\n",
        "                        candidate_s = self.tokenizer.decode(new_base_ids, skip_special_tokens=True)\n",
        "                        new_strings.append(candidate_s)\n",
        "                        cnt += 1\n",
        "                        if cnt >= 50:\n",
        "                            break\n",
        "\n",
        "        return new_strings\n",
        "\n",
        "\n",
        "    def mutation(self, current_adv_text, grad, modify_pos):\n",
        "        new_strings = self.token_replace_mutation(current_adv_text, grad, modify_pos)\n",
        "        return new_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GnXnWV4o4e2"
      },
      "source": [
        "### Inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o-edv1Ybo3-D"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(preds, labels, metric, tokenizer):\n",
        "    if not isinstance(preds, list):\n",
        "        preds = [preds]\n",
        "    if not isinstance(labels, list):\n",
        "        labels = [labels]\n",
        "    preds, labels = postprocess_text(preds, labels)\n",
        "    result = metric.compute(predictions=preds, references=labels)\n",
        "    return result['score']\n",
        "\n",
        "\n",
        "def inference(sentence, label, model, tokenizer, metric, device):\n",
        "    input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "    t1 = time.time()\n",
        "    outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    t2 = time.time()\n",
        "    prediction_len = len(output.split())\n",
        "    eval_score = compute_metrics(output, label, metric, tokenizer)\n",
        "\n",
        "    print(\"\")\n",
        "    success, adv_his = attacker.run_attack(sentence)\n",
        "    print(\"\\nU--{}\".format(sentence))\n",
        "    print(\"G--{}\".format(output))\n",
        "    print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(\n",
        "        prediction_len, t2-t1, eval_score,\n",
        "    ))\n",
        "\n",
        "    if success:\n",
        "        print(\"U'--{}\".format(adv_his[-1][0]))\n",
        "    else:\n",
        "        print(\"Attack failed!\")\n",
        "\n",
        "    input_ids = tokenizer(adv_his[-1][0], return_tensors=\"pt\").input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "    t1 = time.time()\n",
        "    outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    t2 = time.time()\n",
        "    prediction_len = len(output.split())\n",
        "    print(\"G'--{}\".format(output))\n",
        "    eval_score = compute_metrics(output, label, metric, tokenizer)\n",
        "    print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(\n",
        "        prediction_len, t2-t1, eval_score,\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGhuwycqpe6V"
      },
      "source": [
        "### Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAVngC49o36c"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "attacker = WordAttacker(\n",
        "        device=device,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        max_len=64,\n",
        "        max_per=1,\n",
        "    )\n",
        "\n",
        "metric = load_metric(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OF2USA2kXUS",
        "outputId": "50058530-ce29-4dd5-8f67-a2cbbfb86fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 500, 1.00: 100%|| 1/1 [00:12<00:00, 12.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--Can't believe the kid grew up so quick.\n",
            "G--I can't believe it. He's only a few months old.\n",
            "(length: 10, latency: 0.189, BLEU: 3.386)\n",
            "U'--Can Directions believe the kid grew up so quick.\n",
            "G'--I can't believe it. My dad was an Army brat and he always said he wanted to be a pilot.\n",
            "(length: 20, latency: 0.277, BLEU: 1.727)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 350, 1.00: 100%|| 1/1 [00:11<00:00, 11.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--How would I start rock climbing?\n",
            "G--If you know how to climb, you can google it.\n",
            "(length: 10, latency: 0.211, BLEU: 12.046)\n",
            "U'--How would I start rock waterfall?\n",
            "G'--There are many ways to start a rock waterfall. There are many different types of water based on rock factors such as salinity, temperate, etc.\n",
            "(length: 25, latency: 0.352, BLEU: 1.820)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0, 350, 1.00: 100%|| 1/1 [00:09<00:00,  9.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "U--How often do you use computers?\n",
            "G--Almost every week ends.\n",
            "(length: 4, latency: 0.113, BLEU: 8.627)\n",
            "U'--How sort do you use computers?\n",
            "G'--I have a couple of computers. One is a gaming rig and the other is a storage unit.\n",
            "(length: 18, latency: 0.235, BLEU: 2.708)\n"
          ]
        }
      ],
      "source": [
        "# Demo 1\n",
        "input_text = \"Can't believe the kid grew up so quick.\"\n",
        "output_text = \"Yeah, kids grow up so quickly.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)\n",
        "\n",
        "# Demo 2\n",
        "input_text = \"How would I start rock climbing?\"\n",
        "output_text = \"You can google it. But I suggest you to find a local climbing gym and take a class.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)\n",
        "\n",
        "# Demo 3\n",
        "input_text = \"How often do you use computers?\"\n",
        "output_text = \"Almost every week. I use them for work and personal use.\"\n",
        "inference(input_text, output_text, model, tokenizer, metric, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YEeglOfRpzba"
      },
      "outputs": [],
      "source": [
        "# Demo on BST test set\n",
        "import random\n",
        "\n",
        "def test_demo(device, model, tokenizer, attacker, max_num_samples=100, max_per=3):\n",
        "    random.seed(2019)\n",
        "    bst_dataset = load_dataset(\"blended_skill_talk\")\n",
        "    test_dataset = bst_dataset['test']\n",
        "    ids = random.sample(range(len(test_dataset)), max_num_samples)\n",
        "\n",
        "    sampled_test_dataset = test_dataset.select(ids)\n",
        "\n",
        "    metric = load_metric(\"sacrebleu\")\n",
        "    ori_lens, adv_lens = [], []\n",
        "    ori_bleus, adv_bleus = [], []\n",
        "    ori_time, adv_time = [], []\n",
        "    att_success = 0\n",
        "    total_pairs = 0\n",
        "\n",
        "    for i, instance in tqdm(enumerate(sampled_test_dataset)):\n",
        "        if total_pairs >= max_num_samples:\n",
        "            break\n",
        "\n",
        "        for (sentence, label) in zip(instance['free_messages'], instance['guided_messages']):\n",
        "\n",
        "            input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "            input_ids = input_ids.to(device)\n",
        "            t1 = time.time()\n",
        "            \n",
        "            outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "            output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            t2 = time.time()\n",
        "            pred_len = np.count_nonzero(outputs[0].cpu() != tokenizer.pad_token_id)\n",
        "            eval_scores = compute_metrics(output, label, metric, tokenizer)\n",
        "            \n",
        "            ori_lens.append(pred_len)\n",
        "            ori_bleus.append(eval_scores)\n",
        "            ori_time.append(t2-t1)\n",
        "            \n",
        "            # Attack\n",
        "            print(\"\")\n",
        "            success, adv_his = attacker.run_attack(sentence)\n",
        "            print('\\n')\n",
        "            print(\"U--{}\".format(sentence))\n",
        "            print(\"G--{}\".format(output))\n",
        "            print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(pred_len, t2-t1, eval_scores))\n",
        "\n",
        "            if success:\n",
        "                # print(\"Attack Succeed!\")\n",
        "                print(\"U'--{}\".format(adv_his[-1][0]))\n",
        "            else:\n",
        "                print(\"Attack failed!\")\n",
        "\n",
        "            input_ids = tokenizer(adv_his[-1][0], return_tensors=\"pt\").input_ids\n",
        "            input_ids = input_ids.to(device)\n",
        "            t1 = time.time()\n",
        "            outputs = model.generate(input_ids, max_length=64, do_sample=False)\n",
        "            output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            t2 = time.time()\n",
        "            adv_pred_len = np.count_nonzero(outputs[0].cpu() != tokenizer.pad_token_id)\n",
        "            print(\"G'--{}\".format(output))\n",
        "            eval_scores = compute_metrics(output, label, metric, tokenizer)\n",
        "            print(\"(length: {}, latency: {:.3f}, BLEU: {:.3f})\".format(adv_pred_len, t2-t1, eval_scores))\n",
        "\n",
        "            adv_lens.append(adv_pred_len)\n",
        "            adv_bleus.append(eval_scores)\n",
        "            adv_time.append(t2-t1)\n",
        "\n",
        "            att_success += (adv_pred_len > pred_len)\n",
        "            total_pairs += 1\n",
        "\n",
        "            if total_pairs >= max_num_samples:\n",
        "                break\n",
        "\n",
        "\n",
        "    # Summarize eval results\n",
        "    ori_len = np.mean(ori_lens)\n",
        "    adv_len = np.mean(adv_lens)\n",
        "    ori_bleu = np.mean(ori_bleus)\n",
        "    adv_bleu = np.mean(adv_bleus)\n",
        "    ori_t = np.mean(ori_time)\n",
        "    adv_t = np.mean(adv_time)\n",
        "    print(\"Original output length: {:.3f}, latency: {:.3f}, BLEU: {:.3f}\".format(ori_len, ori_t, ori_bleu))\n",
        "    print(\"Adversarial output length: {:.3f}, latency: {:.3f}, BLEU: {:.3f}\".format(adv_len, adv_t, adv_bleu))\n",
        "    print(\"Attack success rate: {:.2f}%\".format(100*att_success/total_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "84315fa7f06c46278ea553e836ce60e9",
            "213f6810504c44b3ba261a5eaea26b82",
            "f985e336fb6b46c094cadb43bdbc0e4a",
            "2a90ce1d47e243e1a574e923098867f3",
            "26a6adaecf3b462186fa09311f32a4d1",
            "9d8ec86383e14aa7ad58304e64546741",
            "428f603e389347229be629f3c16b29d9",
            "a40f1faa4ea44197be87ef6f6bc63c4e",
            "91ecbb6978664e289607aac1db2e4166",
            "01bdfd23d64049638a2e629d093ee7bb",
            "40b233347a504c5699577a4f94b91db1"
          ]
        },
        "id": "wQkZmzYStHZL",
        "outputId": "2eb827f3-7f0f-4dee-9315-9d681e941ff2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset blended_skill_talk (/root/.cache/huggingface/datasets/blended_skill_talk/default/1.0.0/8544e13cbbf2fb9b34157f2e2f28c1539e4f36bf0ef2bd96edd138b4000c5ca1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84315fa7f06c46278ea553e836ce60e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 550, 1.00:   0%|          | 0/1 [00:15<?, ?it/s]\u001b[A\n",
            "0, 550, 1.00: 100%|| 1/1 [00:15<00:00, 15.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--I have two daughters. Do you have any children?\n",
            "G--Yes, I have a son and daughter. How about you?\n",
            "(length: 16, latency: 0.161, BLEU: 30.266)\n",
            "U'--I have twoifix. Do you have any children?\n",
            "G'--No, I don't. But I wish someday after getting married I will have my own. Do you like kids?\n",
            "(length: 27, latency: 0.275, BLEU: 1.958)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 600, 1.00:   0%|          | 0/1 [00:14<?, ?it/s]\u001b[A\n",
            "0, 600, 1.00: 100%|| 1/1 [00:14<00:00, 14.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--Congratulations! That's great. Do you have any hobbies?\n",
            "G--Yes, I love to read. What about you?\n",
            "(length: 14, latency: 0.162, BLEU: 3.629)\n",
            "U'--Congratulations! That's great. Do you have any averages?\n",
            "G'--I don't have any averages, but I do know that if I work hard enough I will be good at something.\n",
            "(length: 27, latency: 0.287, BLEU: 2.167)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 950, 1.00:   0%|          | 0/1 [00:34<?, ?it/s]\u001b[A\n",
            "0, 950, 1.00: 100%|| 1/1 [00:34<00:00, 34.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--Keeping active will make you feel younger and healthy too. Will you teach your grandchild soccer?\n",
            "G--Yes, I will. It's a fun sport to play. How old is he?\n",
            "(length: 21, latency: 0.207, BLEU: 3.152)\n",
            "U'--Keeping activeELS make you feel younger and healthy too. Will you teach your grandchild soccer?\n",
            "G'--I think it would be a good way to keep them active and healthy. I'm not sure yet, but I hope to one day.\n",
            "(length: 31, latency: 0.323, BLEU: 1.891)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 1050, 1.00:   0%|          | 0/1 [00:28<?, ?it/s]\u001b[A\n",
            "0, 1050, 1.00: 100%|| 1/1 [00:28<00:00, 28.62s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--Wonderful. I am sure they will appreciate that! I like to photograph nature in my free time.\n",
            "G--I love taking pictures of nature in my free time.\n",
            "(length: 14, latency: 0.152, BLEU: 1.815)\n",
            "U'--Wonderful. I am sure they will appreciate that! I took to photograph nature in my free time.\n",
            "G'--It's nice to have a hobby, especially when you have free time. What kinds of things do you do?\n",
            "(length: 26, latency: 0.282, BLEU: 2.443)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "0, 1300, 1.00:   0%|          | 0/1 [00:45<?, ?it/s]\u001b[A\n",
            "0, 1300, 1.00: 100%|| 1/1 [00:45<00:00, 45.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "U--That's okay! Yes, I mostly photograph nature, but I like to photograph other things as well, like people and landmarks.\n",
            "G--That's interesting. I like to photograph nature in general. People and landmarks are fascinating to me.\n",
            "(length: 23, latency: 0.233, BLEU: 2.167)\n",
            "U'--That's okay! Yes, I mostly photograph nature, but I like pip photograph other things as well, like people and landmarks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [02:21, 141.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "G'--That's interesting. I like to photograph nature in general, but I also like being able to photograph other things, like people and landmarks.\n",
            "(length: 31, latency: 0.335, BLEU: 1.650)\n",
            "Original output length: 17.600, latency: 0.183, BLEU: 8.206\n",
            "Adversarial output length: 28.400, latency: 0.301, BLEU: 2.022\n",
            "Attack success rate: 100.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "max_num_samples = 5\n",
        "max_per = 1\n",
        "test_demo(device, model, tokenizer, attacker, max_num_samples, max_per)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q3MYRHwtOah"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wHvWQLxuhgsM",
        "SfVI32fzhmV6"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "004738fd62854848a7676e3100693a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37c18f11be549a8834c745a960807f3",
            "placeholder": "",
            "style": "IPY_MODEL_fe9301ea70fc4a6691af00b5d43321f4",
            "value": "100%"
          }
        },
        "01bdfd23d64049638a2e629d093ee7bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be25e0afa9c405fa3b7089fe6aa6ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d8e725392444308b0c8233f62d9ec64",
            "placeholder": "",
            "style": "IPY_MODEL_3c2deb172de24560b57a5a548acfe53b",
            "value": " 3/3 [00:00&lt;00:00, 82.94it/s]"
          }
        },
        "213f6810504c44b3ba261a5eaea26b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8ec86383e14aa7ad58304e64546741",
            "placeholder": "",
            "style": "IPY_MODEL_428f603e389347229be629f3c16b29d9",
            "value": "100%"
          }
        },
        "26a6adaecf3b462186fa09311f32a4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a90ce1d47e243e1a574e923098867f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01bdfd23d64049638a2e629d093ee7bb",
            "placeholder": "",
            "style": "IPY_MODEL_40b233347a504c5699577a4f94b91db1",
            "value": " 3/3 [00:00&lt;00:00, 89.30it/s]"
          }
        },
        "36d9fd106ff240c8b0fb971e5c8efb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c2deb172de24560b57a5a548acfe53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b233347a504c5699577a4f94b91db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "428f603e389347229be629f3c16b29d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d8e725392444308b0c8233f62d9ec64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7170283bda7045e283e036ccaeffdcf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_004738fd62854848a7676e3100693a1b",
              "IPY_MODEL_df68b3c7f37d4821bff89b0e1cd082bb",
              "IPY_MODEL_1be25e0afa9c405fa3b7089fe6aa6ba7"
            ],
            "layout": "IPY_MODEL_7c8a70a52cb64d8eb765ee4cd5df30ec"
          }
        },
        "7c8a70a52cb64d8eb765ee4cd5df30ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84315fa7f06c46278ea553e836ce60e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_213f6810504c44b3ba261a5eaea26b82",
              "IPY_MODEL_f985e336fb6b46c094cadb43bdbc0e4a",
              "IPY_MODEL_2a90ce1d47e243e1a574e923098867f3"
            ],
            "layout": "IPY_MODEL_26a6adaecf3b462186fa09311f32a4d1"
          }
        },
        "91ecbb6978664e289607aac1db2e4166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d8ec86383e14aa7ad58304e64546741": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40f1faa4ea44197be87ef6f6bc63c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37c18f11be549a8834c745a960807f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df68b3c7f37d4821bff89b0e1cd082bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5f96869232484282b47280a9f0639c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d9fd106ff240c8b0fb971e5c8efb3b",
            "value": 3
          }
        },
        "ed5f96869232484282b47280a9f0639c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f985e336fb6b46c094cadb43bdbc0e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40f1faa4ea44197be87ef6f6bc63c4e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ecbb6978664e289607aac1db2e4166",
            "value": 3
          }
        },
        "fe9301ea70fc4a6691af00b5d43321f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
